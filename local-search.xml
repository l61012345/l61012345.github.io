<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>课后练习-MNIST 手写训练集</title>
    <link href="/2021/02/18/Machine%20Learning-NAU/4.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-MNIST/"/>
    <url>/2021/02/18/Machine%20Learning-NAU/4.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-MNIST/</url>
    
    <content type="html"><![CDATA[<h1 id="课后练习3"><a href="#课后练习3" class="headerlink" title="课后练习3"></a>课后练习3</h1><h2 id="Tasks"><a href="#Tasks" class="headerlink" title="Tasks"></a>Tasks</h2><ol><li>Familiarize yourself with the MNIST dataset: MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges. [<a href="http://yann.lecun.com/exdb/mnist/]">http://yann.lecun.com/exdb/mnist/]</a>  </li><li>Familiarize yourself with sklearn package: scikit-learn: machine learning in Python — scikitlearn 0.24.1 documentation [scikit-learn.org]  </li></ol><h2 id="Programming-exercise"><a href="#Programming-exercise" class="headerlink" title="Programming exercise"></a>Programming exercise</h2><h3 id="Q1-Use-the-fetch-openml-function-found-in-sklearn-datasets-to-load-the-mnist-784-dataset-into-python-This-will-load-X-and-y-variables-for-you"><a href="#Q1-Use-the-fetch-openml-function-found-in-sklearn-datasets-to-load-the-mnist-784-dataset-into-python-This-will-load-X-and-y-variables-for-you" class="headerlink" title="Q1. Use the fetch_openml function found in sklearn.datasets to load the mnist_784 dataset into python. This will load X and y variables for you."></a>Q1. Use the fetch_openml function found in sklearn.datasets to load the mnist_784 dataset into python. This will load X and y variables for you.</h3><ul><li>Print the dimensions of the variables returned by the function.</li><li>Write a python script to find how many distinct values are present in y?</li><li>Select one sample from X for each distinct y value.</li><li>Resize each sample to represent the 28x28 pixel image.</li><li>Display all the selected images in one diagram using subplots in matplotlib. The following<br>code gives you an example of how to do this,<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python">fig = plt.figure()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>):<br>fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, i)<br>plt.imshow(images[i])<br>plt.show()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn,datasets <span class="hljs-keyword">import</span> fetch_openml<br>images,labels = fetch_openml(<span class="hljs-string">&#x27;mnist_784&#x27;</span>,version=<span class="hljs-number">1</span>, return_x_y=true, as_frame=false)<br><span class="hljs-comment"># load 70000 28x28=784 handwriting images</span><br><span class="hljs-comment"># print(images.shape)</span><br><span class="hljs-comment">#&gt;&gt; (7000,784)</span><br></code></pre></td></tr></table></figure>or <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br>digits = datasets.load_digits() <span class="hljs-comment">#load the mnist dataset which already in sklearn</span><br>images = digits.images <span class="hljs-comment">#access  1797 8x8 images in mnist by print(images.shape)</span><br>labels = digits.target <span class="hljs-comment">#access 1797 labes </span><br><span class="hljs-comment"># print(images.shape)</span><br><span class="hljs-comment">#&gt;&gt; (1797,8,8)</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">import</span> matplot <span class="hljs-keyword">as</span> plt<br>digits = datasets.load_digits() <span class="hljs-comment">#load the mnist dataset which already in sklearn</span><br>images = digits.images <span class="hljs-comment">#access  1797 8x8 images in mnist by print(images.shape)</span><br>labels = digits.target <span class="hljs-comment">#access labels</span><br><br>np.unique(labels) <span class="hljs-comment"># summerize the labels</span><br>print(np.unique(labels).shape)<br><br>fig = plt.figure()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>):<br>    fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, i+<span class="hljs-number">1</span>) <span class="hljs-comment"># creat a batch of subplot with 2 rows 5 columns</span><br>    <span class="hljs-comment"># i means the position in the subplot</span><br>    plt.imshow(images[i])<br>plt.show() <span class="hljs-comment"># display the subplot</span><br></code></pre></td></tr></table></figure><h3 id="Q2-Use-sklearn-to-train-a-digit-classifier"><a href="#Q2-Use-sklearn-to-train-a-digit-classifier" class="headerlink" title="Q2. Use sklearn to train a digit classifier."></a>Q2. Use sklearn to train a digit classifier.</h3></li><li>Split the X and y into a training set and testing set of 80-20 split.</li><li>Train a Support Vector Machin (SVM) for classification of the digits using the training set.<br>The following code shows how to train a model using sklearn.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">clf = svm.SVC()<br>clf.fit(x_train, y_train)<br></code></pre></td></tr></table></figure></li><li>Test the model using the test set.</li><li>Experiment with different parameter values for the SVM and see how it performs. Try<br>changing the gamma value to be [0.0001, 0.0005, 0.001, 0.005, 0.01]</li><li>Plot the accuracy value with respect to the change in gamma above.</li><li>Plot the confusion matrix<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">import</span> matplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br><br>digits = datasets.load_digits() <span class="hljs-comment"># load the mnist dataset which already in sklearn</span><br>images = digits.images <span class="hljs-comment"># access  1797 8x8 images in mnist, print(images.shape)</span><br>labels = digits.target <span class="hljs-comment"># access 1797 labels</span><br><br>images = data.reshape((<span class="hljs-built_in">len</span>(data),-<span class="hljs-number">1</span>)) <span class="hljs-comment"># reshaape the 8x8 matrixes into 64x1 vectors</span><br><br>x_train,x_test,y_train,y_test = train_test_split(images,labels, test_size = <span class="hljs-number">0.2</span>, shuffle = false) <span class="hljs-comment"># 20% will be test set</span><br><span class="hljs-comment"># x:images y:labels</span><br><br>clf = svm.SVC() <span class="hljs-comment"># create the svm classifier</span><br>clf.fit(x_train, y_train) <span class="hljs-comment"># fit the data  within vectors</span><br><br>acc = clf.score(x_test, y_test) <span class="hljs-comment"># do the test and retrun the accurancy</span><br>disp = metrics.plot_confusion_matrix(clf,x_test,y_test) <span class="hljs-comment"># add into confusion matrix</span><br>print(acc) <span class="hljs-comment"># print the accurancy</span><br>sklearn.metrics.ConfusionMatrixDisplay(disp) <span class="hljs-comment"># display the confusion matrix</span><br><br>g_ = [<span class="hljs-number">0.0001</span>,<span class="hljs-number">0.0005</span>,<span class="hljs-number">0.001</span>,<span class="hljs-number">0.005</span>,<span class="hljs-number">0.01</span>] <span class="hljs-comment"># list of gamma</span><br>scores = [] <span class="hljs-comment"># list of accurancy</span><br><span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> g_:<br>    clf = svm.SVC(gamma = g_) <span class="hljs-comment"># create the svm classifier,specify the gamma</span><br>    clf.fit(x_train, y_train) <span class="hljs-comment"># fit the data  within vectors</span><br><br>    acc = clf.score(x_test, y_test) <span class="hljs-comment"># do the test and retrun the accurancy</span><br>    scores.append(acc)<br><br><br><br>print(g_) <span class="hljs-comment"># print the accurancy</span><br>print(scores)<br><br>plt.plot(g_, scores)<br>plt.show()  <br></code></pre></td></tr></table></figure></li></ul><p>```</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1. 机器学习简介和Python的基本操作</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/1.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%92%8CPython%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/1.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%92%8CPython%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="机器学习简介和Python的基本操作"><a href="#机器学习简介和Python的基本操作" class="headerlink" title="机器学习简介和Python的基本操作"></a>机器学习简介和Python的基本操作</h1><blockquote><p>新加坡国立大学(NAU)/OTH927/2021/1/23<br>Terence Sim (<a href="mailto:&#116;&#x73;&#105;&#109;&#64;&#99;&#111;&#x6d;&#x70;&#x2e;&#110;&#117;&#115;&#x2e;&#x65;&#100;&#117;&#46;&#x73;&#x67;">&#116;&#x73;&#105;&#109;&#64;&#99;&#111;&#x6d;&#x70;&#x2e;&#110;&#117;&#115;&#x2e;&#x65;&#100;&#117;&#46;&#x73;&#x67;</a>) /Karen Boh/ Sanka Rasnayaka(Assistant)</p></blockquote><h2 id="Before-the-course…"><a href="#Before-the-course…" class="headerlink" title="Before the course…"></a>Before the course…</h2><ul><li>Software and environment: Anaconda and Opencv  </li><li>Ultimate Project: Traffic Sign Recognition   </li><li>There’s a individual Quiz on lecture 5  </li></ul><h2 id="人工智能的产生"><a href="#人工智能的产生" class="headerlink" title="人工智能的产生"></a>人工智能的产生</h2><p>lines of codes programming forces people to find a way to teach the program to do things.<br>example: makeup transfer<br>example: auto ping-pong machine</p><h2 id="计算机视觉简介"><a href="#计算机视觉简介" class="headerlink" title="计算机视觉简介"></a>计算机视觉简介</h2><p>计算机视觉可以大致的被分为三个大类：  </p><ul><li>3D建模(3D Construction)<br>例子：敦煌莫高窟的3D建模（来自武汉大学）  </li><li>图像渲染(Image Rendering)<br>例子：Google Pixel    <pre><code> 其搭载的增强现实算法能够对周围的图像进行实时渲染  </code></pre></li><li>图像检测(Pattern Recongnition)<br> 例子：都灵的图像识别装置   <pre><code>   人们穿戴对应的设备行走，设备能够识别他周围的物品   </code></pre></li></ul><p>计算机视觉可以在各个领域帮助到人们，在医学领域帮助医生识别X光片，在自动驾驶领域，自动驾驶汽车依靠车身上的传感器和相机识别道路上的物体，在体育竞技领域，计算机视觉能够帮助人们更好的训练运动员的运动姿势。世界上第一张人脸检测的图片由Dr.Sung Kah Kay在1996年完成。    </p><p>计算机科学的知识架构：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210123230629.png">  </p><h2 id="Python-的基本操作"><a href="#Python-的基本操作" class="headerlink" title="Python 的基本操作"></a>Python 的基本操作</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>  Python 不需要编译，是机器学习的首选语言之一，有非常多的库能够被调用。<br>  Python支持超大的数字运算<br>  编译环境（IDE）：Anaconda， 适用于大数据环境    </p><blockquote><p>不要使用 Python 2.x   </p></blockquote><ul><li>Python的IDE思路：REPL<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210123231447.png"><br>Reading： 读取来自键盘等的输入<br>Evalueate： 将输入进行Evaluate，其结果通常是一个数值，这个数值最终会被编译器输出（Print）<br>在输出后，这个程序将等待下一次的输入，形成一个循环  <h3 id="赋值和函数定义"><a href="#赋值和函数定义" class="headerlink" title="赋值和函数定义"></a>赋值和函数定义</h3>Python可以不用声明变量的类型，直接对其进行赋值，其变量的赋值类型取决于赋值<br>Python支持同时对多个变量进行赋值：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python">a,b=c,d <br>```  <br><br>定义函数的结构：  <br>``` Python<br>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">functionname</span>(<span class="hljs-params">variable</span>):</span>  <br>      <span class="hljs-keyword">return</span> value<br>    <span class="hljs-comment"># 也可以不需要返回值</span><br></code></pre></td></tr></table></figure> 另一种定义方式：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python">    function= <span class="hljs-keyword">lambda</span> return_variable: options  <br>```  <br>例如：  <br>  ``` Python<br>   S= <span class="hljs-keyword">lambda</span> x: x*x  <br></code></pre></td></tr></table></figure>在Python中，函数的定义和调用有如下特点：</li></ul><ol><li><p>嵌套调用：<br><code>funcationname(funcationname(variable))</code>  </p></li><li><p>在Python中，变量可以传递给函数，<strong>函数也可以传递给变量</strong>。<br>例如：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">function</span> (<span class="hljs-params">n</span>):</span><br>    n*n<br>    <span class="hljs-keyword">return</span> n<br>foo=function(<span class="hljs-number">5</span>)<br><span class="hljs-comment"># 此时foo的类型是一个函数</span><br>foo(<span class="hljs-number">10</span>)<br><span class="hljs-comment"># &gt;&gt; 100</span><br></code></pre></td></tr></table></figure></li><li><p>可以在定义函数的部分嵌套定义其他函数：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">function</span>(<span class="hljs-params">variable1</span>):</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">subfuntion</span>(<span class="hljs-params">variable2</span>):</span><br>     <span class="hljs-keyword">return</span> variable2<br> <span class="hljs-keyword">return</span> variable1<br></code></pre></td></tr></table></figure><h3 id="条件结构-if-else"><a href="#条件结构-if-else" class="headerlink" title="条件结构(if-else)"></a>条件结构(if-else)</h3><p>条件语句的基本结构： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">if</span> conditon:<br>  options<br></code></pre></td></tr></table></figure><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compare</span>(<span class="hljs-params">a,b</span>):</span><br>  <span class="hljs-keyword">if</span> a&gt;b:<br>    <span class="hljs-keyword">return</span> a<br>  <span class="hljs-keyword">return</span> b<br>compare(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-comment">#&gt;&gt; 4</span><br></code></pre></td></tr></table></figure><p>在条件语句中可以同时并存多个条件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">if</span> condition1:<br>  options<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> condition2:<br>  options<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>  condtion3:<br>  options<br><span class="hljs-keyword">else</span>:<br>  options<br></code></pre></td></tr></table></figure><h3 id="循环结构"><a href="#循环结构" class="headerlink" title="循环结构"></a>循环结构</h3></li><li><p>通过函数定义的返回值来进行循环 (recursion)<br>例如：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">factorial</span>(<span class="hljs-params">n</span>):</span><br>  <span class="hljs-keyword">if</span> n==<span class="hljs-number">1</span>:<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>  <span class="hljs-keyword">return</span> n*factorial(n-<span class="hljs-number">1</span>)<br>factorial(<span class="hljs-number">5</span>)<br><span class="hljs-comment">#&gt;&gt; 120</span><br><span class="hljs-comment">#5*4*3*2*1</span><br></code></pre></td></tr></table></figure></li><li><p>通过for循环语句来进行循环(for-range) </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> innnervariable <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span> <span class="hljs-comment">#usually is range(a,b) a to b do n++</span><br>options<br></code></pre></td></tr></table></figure><p>例如：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">factorial</span>(<span class="hljs-params">n</span>):</span><br> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,n+<span class="hljs-number">1</span>)<br>   result=result*n<br> <span class="hljs-keyword">return</span> result<br> factorial(<span class="hljs-number">5</span>) <br> <span class="hljs-comment">#&gt;&gt; 120  </span><br></code></pre></td></tr></table></figure><blockquote><p>要注意 <code>range(a,b)</code> 是不包括b的：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> (<span class="hljs-number">0</span>,<span class="hljs-number">4</span>)<br>   print(a) <br><span class="hljs-comment">#&gt;&gt; 3  </span><br></code></pre></td></tr></table></figure><p>这样的循环结构没有自增加（<code>x++</code>）的存在</p></blockquote></li><li><p>通过while语句进行循环</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">while</span> (condition):<br>  options<br></code></pre></td></tr></table></figure><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gcd</span>(<span class="hljs-params">a,b</span>):</span> <span class="hljs-comment">#最大公约数</span><br>  <span class="hljs-keyword">while</span> (b&gt;<span class="hljs-number">0</span>):<br>    r=a&amp;b<br>    a,b=b,r<br> <span class="hljs-keyword">return</span> a<br></code></pre></td></tr></table></figure><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>在Pyhton中，字符串由双引号””或者单引号’’定义。<br>字符串支持加减法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-string">&#x27;hello&#x27;</span>+<span class="hljs-string">&#x27;world&#x27;</span><br><span class="hljs-comment">#&gt;&gt; &#x27;helloworld&#x27;</span><br></code></pre></td></tr></table></figure><p>也支持乘法（重复多次）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-string">&#x27;hello&#x27;</span>*<span class="hljs-number">3</span><br><span class="hljs-comment">#&gt;&gt; &#x27;hellohellohello&#x27; </span><br></code></pre></td></tr></table></figure></li></ol><ul><li>字符串的传递<br>字符串可以传值给变量（类型是字符串），可以通过[起点：终点：步长]访问字符串中的特定位置的字符。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python">a=<span class="hljs-string">&#x27;helloworld&#x27;</span><br>print(a[<span class="hljs-number">2</span>])<br><span class="hljs-comment">#&gt;&gt;&#x27;l&#x27;</span><br>print(a[<span class="hljs-number">0</span>:<span class="hljs-number">4</span>])<br><span class="hljs-comment">#&gt;&gt;&#x27;hello&#x27;</span><br>&gt; Python 中的序号是从<span class="hljs-number">0</span>开始的<br>print(a[::-<span class="hljs-number">1</span>] )<br><span class="hljs-comment">#&gt;&gt;&#x27;dlrowolleh&#x27;</span><br>b=<span class="hljs-string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span><br>print(a[<span class="hljs-number">1</span>:<span class="hljs-number">15</span>:<span class="hljs-number">2</span>])<br><span class="hljs-comment">#&gt;&gt;&#x27;bdfhjln&#x27;</span><br></code></pre></td></tr></table></figure><code>len()</code>函数将返回字符串的长度 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-built_in">len</span>(hello)<br><span class="hljs-comment">#&gt;&gt; 5</span><br></code></pre></td></tr></table></figure>[]默认的访问顺序是从左到右，负号（-）表示从右到左的访问顺序。   <h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3>列表（list）是一种参数类型，例如：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Python">x=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]<br><span class="hljs-built_in">type</span>(x)<br><span class="hljs-comment">#&gt;&gt; list</span><br></code></pre></td></tr></table></figure>列表用[]来表示，列表也可以嵌套。<br>列表中的元素可以是任何类型。<br>和字符串一样，可以用[]来访问列表中特定的某一个或者多个元素。    </li><li>列表的操作  </li></ul><ol><li>append()<br><code>append()</code>函数将在列表最后一位加上()内的字符串后，输出整个字符串<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python">x=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]<br>x.append(<span class="hljs-number">2</span>)<br>print(x)<br><span class="hljs-comment">#&gt;&gt;[1,2,3,4,2]</span><br></code></pre></td></tr></table></figure></li><li>列表理解（list comprehension）<br>在列表的[]中填入生成列表的方法：<br>例如：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python">x=[a <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">8</span>)]<br><span class="hljs-built_in">print</span> x<br><span class="hljs-comment">#&gt;&gt; [1,2,3,4,5,6,7]</span><br>y=[square(a) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> x]<br><span class="hljs-built_in">print</span> y<br><span class="hljs-comment">#&gt;&gt;[1,4,9,16,25,36,49]</span><br></code></pre></td></tr></table></figure>可以利用列表理解来过滤某些元素:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">iseven</span>(<span class="hljs-params">n</span>):</span><br>    <span class="hljs-keyword">return</span> n%<span class="hljs-number">2</span>==<span class="hljs-number">0</span><br>x=[a <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">8</span>)]<br>y=[square(a) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> x <span class="hljs-keyword">if</span> iseven(a)]<br><span class="hljs-built_in">print</span> y<br><span class="hljs-comment">#&gt;&gt;[4,16,36]  </span><br></code></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3. 图像处理</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/3.%20%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/3.%20%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 60%;    padding-left: 30%;}</style><h1 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h1><h2 id="成像原理与数字化"><a href="#成像原理与数字化" class="headerlink" title="成像原理与数字化"></a>成像原理与数字化</h2><ul><li>小孔成像（Pinhole）<br>小孔成像的基本原理如下图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207142920.png">    </li><li>透镜成像<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207143053.png">  </li><li>CCD/CMOS（电荷耦合）成像<br>在CCD成像当中，通过透镜后的像会呈现在CMOS上，COMS会将呈现数字化，这一过程中有两个重要的步骤：  <ol><li><p><strong>抽样</strong>（Sampling）<br>将图像转化为有限的单位像素，如图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207144301.png">    </p></li><li><p><strong>量化</strong>（Quantization）<br>用整数表示单位像素的值，对于8bit而言，单位像素的明暗程度以0~255的灰度值来表示，0表示黑色，255表示白色。  </p><p>现在，一幅黑白的图像中的每一个单位像素点都用一个整数来表示其黑白的程度，那么整张图片就可以用一个只有整数的矩阵来表示（单通道）。<br>对于彩色的图像，通常以RGB（红色、绿色、蓝色）的三种程度（三通道）来进行量化，因此彩色图片的一个单位像素点以一个三维的向量，通常是$[R,G,B]$来表示。最终三个矩阵表示一幅彩色图像，这个过程叫做张量（Tensor）。  </p><h2 id="点处理-Point-Processing"><a href="#点处理-Point-Processing" class="headerlink" title="点处理(Point Processing)"></a>点处理(Point Processing)</h2><p>图像的点处理是： 设定图像上一个像素值$r(x,y)$,经过处理$s(x,y)=T(r(x,y))$后，得到同一位置的像素$s(x,y)$。<br>注意：</p></li></ol></li></ul><ol><li>$(x,y)$表示坐标。</li><li>不同的图像处理库其坐标系统的原点设置不同，y的取值设定也不同。  </li><li>$T$只能是单调（通常是单调递增）的函数。<h3 id="常见的点处理变换"><a href="#常见的点处理变换" class="headerlink" title="常见的点处理变换"></a>常见的点处理变换</h3></li></ol><ul><li><p>阈值变换(Thresholding)<br>函数图像<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207152536.png"><br>阈值函数可分为两种：软阈值函数（左）和硬阈值函数/二值化函数(Hard thresholding/Binarization，右),它们的作用都是将像素转换成黑白像素。<br>图像中的$m$点称为阈值，高于阈值的像素将会被强化为近黑色/黑色的像素值，低于阈值的像素值将被弱化为近白色/白色的像素值。  </p></li><li><p>像素反转<br>$$s=L-1-r$$<br>$L$表示最大的像素值。<br>作用是将像素值进行翻转，白色变为黑色，黑色变为白色。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207154347.png">  </p></li><li><p>对数变换<br>$$s=clog(1+r)$$<br>对数变换能够扩展低灰度值（突出过曝区域的细节）而压缩高灰度值（突出过暗区域的细节），从而增强图像的清晰度。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207154929.png">   </p></li><li><p>幂变换<br>$$s=cr^γ$$<br>$γ$是幂指数，显示器中的伽玛校正(Gamma Correction)即调整该值使得显示器整体偏亮或偏暗。 当$0&lt;γ&lt;1$时，显示器偏暗，$γ&lt;1$时，显示器偏亮。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207155330.png"><br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207155454.png">   </p></li><li><p>折线(Piecewise Linear Curves)<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207155745.png">   </p></li></ul><h3 id="亮度直方图-Historgram"><a href="#亮度直方图-Historgram" class="headerlink" title="亮度直方图(Historgram)"></a>亮度直方图(Historgram)</h3><p>亮度直方图的横轴是像素值，纵轴是该像素值内的像素点个数，它反映了黑白图像整体的像素分布情况。<br>如果直方图在白色区域内比较集中，图像偏亮，直方图窄黑色区域内比较集中，图像偏暗。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207160828.png"><br>图像在直方图上的最大分布范围（即横轴的宽度）称为对比度(Contrast)，直方图窄的图像对比度低。通常情况下，对比度越高图像越清晰。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207160857.png">    </p><ul><li>直方图均衡(Historgram equalization)<br>直方图均衡是一种用于增强图像对比度的同时均衡图像亮度的方法（即拉宽和拖平直方图）。<br>直方图可以视为反映了每一个像素值在整个图像的占比关系（$\frac{n_w}{n}$，n表示图像像素点总数，$n_w$表示像素值为w的像素点数量），因此整个直方图可以被概率分布函数化：<br>$$p_r(w)=p(r=w)=\frac{n_w}{n},w=0,1,…,255$$<br>应用变换：<br>$$s=T(r)=(L-1)\int_0^rp_r(w)dw$$<br>使得:<br>$$P_s(s)=P_r(r)|\frac{dr}{ds}|=\frac{1}{L-1}$$<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207162034.png"><br>由于w并不是连续的，因此最终的直方图并非是完全扁平的矩形。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207164743.png"></li><li>局部增强(Local enhancement)<br>对局部的一些像素群（例如以某个像素为中心$9 × 9$或$3 × 3$的像素）应用直方图均衡的方法称为局部增强。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207163241.png">   <h2 id="相邻处理（Neighborhood-Processing）"><a href="#相邻处理（Neighborhood-Processing）" class="headerlink" title="相邻处理（Neighborhood Processing）"></a>相邻处理（Neighborhood Processing）</h2>与点处理比较，虽然相邻处理的输入值仍然是一个像素值，但是输出值确实一个围绕输入像素值的像素集。（如下图所示）<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208124611.png"><br>常见的一些相邻处理的方法：  </li><li>均值滤波  </li><li>最值滤波  </li><li>中值滤波（像素值按大小排列，取排序位于中间位置的像素值作为中值滤波后的像素值）<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208124649.png"><br>▲上面的三张图依次是：原图（有一些细小噪点）、求平均、求中位数后的输出。  </li></ul><h3 id="滤波（Filtering）"><a href="#滤波（Filtering）" class="headerlink" title="滤波（Filtering）"></a>滤波（Filtering）</h3><p>滤波是一种应用于相邻处理的常见方法。<br>设以某个像素为中心的像素方阵称为<strong>核矩阵</strong>(Kernel/Mask matrix),以$W$记，核矩阵中的每一个像素值以$w(u,v)$表示。 将核矩阵照射至图像的某一区域$r$，使被照射区域中的像素值与对应的核矩阵中同位置的像素值一一相乘后全部相加，最后用一常数$C$调整，该过程被称为滤波。<br>$$s(x,y)=C∑_{(u,v)∈W}w(u,v)r(x+u,y+v)$$<br>整个过程可以用下图来表示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208130628.png"><br>一次滤波结束后，核矩阵平移$α$个像素单位（称为<strong>步长</strong>(Padding)），照射图像的另一个区域，重复上述过程。    </p><ul><li><p>均值滤波<br>其核矩阵如下：<br>$$\frac{1}{n}×\begin{bmatrix} 1&amp;1&amp;…&amp;1\…&amp;…&amp;…&amp;…\ 1&amp;1&amp;…&amp;1 \ 1&amp;1&amp;…&amp;1\end{bmatrix}$$<br>n表示方阵元素的数量。<br>这样最终的输出结果是取像素点周围领域的平均值作为响应输出，最终的图像会被模糊化。  </p></li><li><p>高斯滤波(Gaussian Filtering)<br>高斯滤波的核矩阵内的元素在三维上符合标准高斯/标准正态分布，且最高点在核矩阵中心处，如下图所示。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208131837.png"><br>其具体的计算公式为：<br>$$G(x,y)=\frac{1}{2πσ^2}e^{-\frac{x^2+y^2}{2σ^2}}$$<br>$σ$是标准高斯分布中的方差，$σ$较小时，图像的峰值窄且高。<br>例如当$σ=1.4$时，其核矩阵可以取：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208132110.png"><br>高斯滤波的作用是<strong>将图像模糊化</strong>，使图像呈现一种毛玻璃的质感。运用高斯滤波处理图像的方法又被称为<strong>高斯模糊</strong>(Gaussian Blurring)。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208132748.png"><br><strong>高斯模糊的重要作用是将图像中的噪点通过模糊化图像的方法移除。</strong></p></li></ul><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p><strong>卷积</strong>(Convolution)是一种常用于图像处理的方法。<br>设核矩阵(又被称为卷积核)的像素分布可以表示为$h$,原图的像素分布表示为$f$，卷积有如下公式：<br>$$g(x)=\int_{-∞}^∞f(τ)h(x-τ)dτ$$    </p><ul><li>卷积核的正则化<br>如果要使得图像的整体亮度在卷积前后不发生改变，卷积核必须被正则化，即卷积核内所有元素的和必须是1。</li></ul><h2 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h2><p>边缘（Edges）是图像中像素值变化急剧(Sharply)的部分，常见的边缘有如下图所示的四大类。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208134126.png"><br>边缘检测是以特殊的卷积核（称为算子（Operator））对图像进行处理。<br>常见的算子有如下几种：  </p><ul><li><p>Roberts算子<br>Roberts算子是两个能够强化图像的边缘部分的核矩阵：<br>$$G_x=\begin{bmatrix}<br>  +1&amp;0\0&amp;-1<br>   \end{bmatrix}$$<br>$$G_y=\begin{bmatrix}<br>  0&amp;+1\-1&amp;0<br>   \end{bmatrix}$$<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208133900.png">    </p></li><li><p>索伯算子(Sobel’s Operator)<br>Sobel算子的两个卷积核形式：<br>$$\begin{bmatrix} -1&amp;0&amp;+1\-2&amp;0&amp;+2\-1&amp;0&amp;+1 \end{bmatrix}$$<br>$$\begin{bmatrix}+1&amp;+2&amp;+1\0&amp;0&amp;0\-1&amp;-2&amp;-1\end{bmatrix}$$<br>两个卷积核的特征是卷积核正中的纵列或行列为0，用于检测图像的纵向/横向边缘。<br>PIL或者是OpenCV中有对应的库可以执行Sobel边缘检测。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208140605.png">    </p></li><li><p>拉普拉斯算子(Laplacian operator)<br>函数$F(x,y)$的梯度可由梯度公式得到：<br>$$G(x,y)=\frac{∂F(x,y)}{∂x}cos(σ)+\frac{∂F(x,y)}{∂y}sin(σ)$$<br>定义拉普拉斯算子（一阶）$▿f|(x_0,y_0)=(f_x(x_0,y_0),f_y(x_0,y_0))$，其二阶形式：<br>$$▿^2f(x,y)=\frac{∂^2f(x,y)}{∂x^2}+\frac{∂^2f(x,y)}{∂y^2}$$<br>在x方向上可以近似由差分表示：<br>$$\frac{∂^2f(x,y)}{∂x^2}=f(x+1,y)+f(x-1,y)-2f(x,y)$$<br>在y方向上同理，最终得到二阶拉普拉斯算子的表达式：<br>$$▿^2f(x,y)=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)$$<br>得到拉普拉斯算子的卷积核形式：<br>$$\begin{bmatrix}<br>  0&amp;1&amp;0\1&amp;-4&amp;1\0&amp;1&amp;0<br>\end{bmatrix}$$  </p></li><li><p>拉普拉斯-高斯算子（LoG operator）<br>由表达式：<br>$$▽^2g(x,y)=-\frac{1}{2πσ^4}(2-\frac{x^2+y^2}{σ^2})e^{-\frac{x^2+y^2}{2σ^2}}$$<br>所得到的算子的卷积核形式：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208141000.png">   </p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>课后练习-机器学习简介和Python的基本操作</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/1.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/1.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="课后练习1"><a href="#课后练习1" class="headerlink" title="课后练习1"></a>课后练习1</h1><p>Solve the questions below by writing a Python function or script.   </p><h2 id="Q1-Add-up-the-numbers-from-100-to-200-and-output-their-sum-using-while-and-for-loops"><a href="#Q1-Add-up-the-numbers-from-100-to-200-and-output-their-sum-using-while-and-for-loops" class="headerlink" title="Q1. Add up the numbers from 100 to 200 and output their sum, using while and for loops."></a>Q1. Add up the numbers from 100 to 200 and output their sum, using while and for loops.</h2><p>for loop:   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python">total = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>,<span class="hljs-number">201</span>): <span class="hljs-comment">#注意range不包括右边项</span><br>    total+=i<br>print(<span class="hljs-string">&quot;for loop sum:&quot;</span>,total)    <br></code></pre></td></tr></table></figure><p>while loop:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Python">total=<span class="hljs-number">0</span><br>counter = <span class="hljs-number">100</span><br><span class="hljs-keyword">while</span> counter &lt; <span class="hljs-number">201</span>:<br>    total= total+counter <span class="hljs-comment">#++不能在python当中使用</span><br>    counter+=<span class="hljs-number">1</span> <span class="hljs-comment">#while 循环里面没有自增加</span><br>print(<span class="hljs-string">&#x27;for loop sum:&#x27;</span>,total)<br></code></pre></td></tr></table></figure><h2 id="Q2-Read-a-string-from-console-and-output-its-length-swap-its-cases（转换大小写）-convert-it-to-lower-case-and-upper-case-and-reverse-it-Hint-try-string-slice-with-step-1"><a href="#Q2-Read-a-string-from-console-and-output-its-length-swap-its-cases（转换大小写）-convert-it-to-lower-case-and-upper-case-and-reverse-it-Hint-try-string-slice-with-step-1" class="headerlink" title="Q2. Read a string from console and output its length, swap its cases（转换大小写）, convert it to lower case and upper case, and reverse it. (Hint: try string slice with step -1)"></a>Q2. Read a string from console and output its length, swap its cases（转换大小写）, convert it to lower case and upper case, and reverse it. (Hint: try string slice with step -1)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs Python">s= <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;enter the string:&quot;</span>)<br>print(<span class="hljs-string">&quot;the length of the string:&quot;</span>,<span class="hljs-built_in">len</span>(s))<br><br>swap=<span class="hljs-built_in">str</span>.swapcase(s) <span class="hljs-comment">#str.swapcase() 转换大小写</span><br><br>print(<span class="hljs-string">&quot;swapcase is :&quot;</span>,swap)<br>print(<span class="hljs-string">&quot;lowercase:&quot;</span>,<span class="hljs-built_in">str</span>.lower(s)) <span class="hljs-comment">#str.lower/upper()</span><br>print(<span class="hljs-string">&quot;uppercase:&quot;</span>,<span class="hljs-built_in">str</span>.upper(s)) <br><br>print(<span class="hljs-string">&quot;reverse order:&quot;</span>,s[::-<span class="hljs-number">1</span>]) <span class="hljs-comment">#[起点:终点：步长]</span><br></code></pre></td></tr></table></figure><h2 id="Q3-Read-a-string-from-console-Split-the-string-on-space-delimiter-”-”-and-join-using-a-hyphen-”-”"><a href="#Q3-Read-a-string-from-console-Split-the-string-on-space-delimiter-”-”-and-join-using-a-hyphen-”-”" class="headerlink" title="Q3. Read a string from console. Split the string on space delimiter (” ”) and join using a hyphen (”-”)."></a>Q3. Read a string from console. Split the string on space delimiter (” ”) and join using a hyphen (”-”).</h2><p>(Example: input the string, ”this-is a string” and output as ”this is-a-string”)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python">s=<span class="hljs-string">&quot;hello world&quot;</span><br>print(<span class="hljs-string">&quot;replace:&quot;</span>,<span class="hljs-built_in">str</span>.replace(s,<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;-&quot;</span>)) <span class="hljs-comment">#str.replace(string,&quot;&quot;,&quot;&quot;)交换前后元素</span><br></code></pre></td></tr></table></figure><h2 id="Q4-Learn-the-Python-list-operations-and-follow-the-commands-below"><a href="#Q4-Learn-the-Python-list-operations-and-follow-the-commands-below" class="headerlink" title="Q4. Learn the Python list operations and follow the commands below:"></a>Q4. Learn the Python list operations and follow the commands below:</h2><ul><li>Initialize an empty list L.  </li><li>Add 12, 8, 9 to the list.  </li><li>Insert 9 to the head of the list;  </li><li>Double the list. (e.g. change L = [1, 2, 3] to L = [1, 2, 3, 1, 2, 3])  </li><li>Remove all 8 in the list.  </li><li>Reverse the list.  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs Python">L=[]<br>print(L)<br><br>L.append(<span class="hljs-number">12</span>) <span class="hljs-comment">#List.append() 列表在末尾添加</span><br>L.append(<span class="hljs-number">8</span>)<br>L.append(<span class="hljs-number">9</span>)<br>print(L)<br><br>L.insert(<span class="hljs-number">0</span>,<span class="hljs-number">9</span>) <span class="hljs-comment">#list.insert(position,element) 在列表的指定位置添加一个元素</span><br>L=[<span class="hljs-number">9</span>]+L <span class="hljs-comment">#另一种方式</span><br><br>L=L+L<br>print(L)<br>L=L*<span class="hljs-number">2</span> <span class="hljs-comment">#另一种方法</span><br>L=L.extend(L) <span class="hljs-comment">#list.extend(list) 在列表的末尾添加一个列表</span><br>print(L)   <br><br>number_eights=L.count(<span class="hljs-number">8</span>) <br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,number_eight):<br>    L.remove(<span class="hljs-number">8</span>)  <span class="hljs-comment">#list.remove(element) 在列表中移除第一个【element】元素</span><br><span class="hljs-comment">#另一种解决办法</span><br><span class="hljs-keyword">while</span> <span class="hljs-number">8</span> <span class="hljs-keyword">in</span> L:<br>     L.remove(<span class="hljs-number">8</span>)<br><br>L.reverse() <span class="hljs-comment">#list.reverse() 列表内倒序</span><br>print(L)<br></code></pre></td></tr></table></figure><h1 id="Q5-Learn-Python-matrix-operations-by-completing-the-following-tasks"><a href="#Q5-Learn-Python-matrix-operations-by-completing-the-following-tasks" class="headerlink" title="Q5. Learn Python matrix operations by completing the following tasks:"></a>Q5. Learn Python matrix operations by completing the following tasks:</h1></li><li>Create a 3x2 matrix named A, with all ones.</li><li>Create a 3x2 matrix named B, where $𝐵 =\begin{bmatrix} 1&amp;2\ 3&amp;4 \ 5&amp;6 \end{bmatrix}$  </li><li> Print A and B.</li><li> Transpose A to be a 2x3 matrix.</li><li> Multiply matrix A with matrix B and store the output in matrix C.</li><li> Print the dimensions of C.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <span class="hljs-comment">#import packagename 加载库  import packagename as nickname 加载并替换库的名字</span><br>A=np.ones(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>) <span class="hljs-comment">#numpy.ones(row,line)</span><br>print(A)  <br><br>B=np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]]) <span class="hljs-comment">#numpy.array() 创建矩阵，用法同左边</span><br>print(B)<br><br>print(A,B)<br><br>A=A.reshape((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)) <span class="hljs-comment">#numpy.reshape((row,line)) 重新改写矩阵的大小</span><br>A=np.transpose(A) <span class="hljs-comment">#numpy.transpose(matrix) 返回转置矩阵</span><br>print(A)<br><br>C=A @ B <span class="hljs-comment">#@ 矩阵叉乘</span><br>print(C.shape) <span class="hljs-comment">#matrix.shape 矩阵的大小</span><br></code></pre></td></tr></table></figure><h1 id="Q6-Use-𝑀-for-the-following-tasks"><a href="#Q6-Use-𝑀-for-the-following-tasks" class="headerlink" title="Q6. Use 𝑀 for the following tasks,"></a>Q6. Use 𝑀 for the following tasks,</h1>$$𝑀 =\begin{bmatrix}−2&amp;−4&amp;2 \ −2&amp;1&amp;2 \ 4&amp;2&amp;5 \end{bmatrix}$$  </li><li> Calculate the eigenvalues and eigenvectors for M. (hint: use numpy.linalg.eig)    </li><li> Use matplotlib to plot the eigenvalues in a graph.    </li><li> Save the eigenvalues into a file named “eig.npy” (hint: use numpy.save).   </li><li> Load the saved file into a new variable called load_eig and print the values.   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> plot <span class="hljs-keyword">as</span> plt<br>M=[[-<span class="hljs-number">1</span>,-<span class="hljs-number">4</span>,<span class="hljs-number">2</span>],[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>]]<br>eignval,eignvect=np.linalg,eig(M) <span class="hljs-comment">#numpy.linalg,eig(Matrix) 返回两个值，第一个是特征值，第二个是特征向量</span><br>print(eignval)<br><br>plt.plot(eignval)<br><br>np.save(<span class="hljs-string">&quot;eig&quot;</span>,eigval) <span class="hljs-comment">#numpy.save(&quot;filename&quot;,value) 将值在当前目录下以“filename.npy”储存</span><br><br>load.eig=np.load(<span class="hljs-string">&quot;eigval.npy&quot;</span>) <span class="hljs-comment">#numpy.load(&quot;path&quot;) 返回加载的文件</span><br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2. 数学方法</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/2.%20%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/2.%20%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="数学方法"><a href="#数学方法" class="headerlink" title="数学方法"></a>数学方法</h1><h2 id="矩阵的运算"><a href="#矩阵的运算" class="headerlink" title="矩阵的运算"></a>矩阵的运算</h2><h3 id="矩阵的乘法"><a href="#矩阵的乘法" class="headerlink" title="矩阵的乘法"></a>矩阵的乘法</h3><p>矩阵的乘法规则：前一矩阵的行乘后一矩阵的纵列若A是一个$m \times n$的矩阵，B是一个$a \times b$的矩阵，那么矩阵乘法$A \times B$的结果将会是一个$n \times a$的矩阵</p><blockquote><p>要注意$A × B=0 ⇏A=0 <del>or</del> B=0$<br>$AB \not ={} BA$,但是$(AB)C=A(BC)$</p></blockquote><p>矩阵与向量的乘法可以改写,用如下例子来做表示：<br>$A=\begin{bmatrix}<br>1&amp;2&amp;1\-1&amp;0&amp;2<br>\end{bmatrix} B=\begin{bmatrix}1\-1\1\end{bmatrix}$<br>有$A\times B=1×\begin{bmatrix}1\-1\end{bmatrix}+(-1)×\begin{bmatrix}2\0\end{bmatrix}+1\times \begin{bmatrix}1\2\end{bmatrix}$<br>称B是A的一组线性组合</p><h3 id="转置、对称"><a href="#转置、对称" class="headerlink" title="转置、对称"></a>转置、对称</h3><p>$A^T$表示A的转置，即A行列交换后的矩阵。<br>有$(AB)^{T}=B^{T}A^{T}$<br>方阵：A为一个正方形矩阵:$m×m$<br>单位矩阵：\对角线上的元素为1，其余元素为0的方阵，有$AI=IA=A$<br>若$A^T=A$,称A是一个对称矩阵(Symmetric matrix)，若$A^T=-A$，称A是一个交错矩阵(Skew-symmectric matrix)</p><h3 id="矩阵的逆"><a href="#矩阵的逆" class="headerlink" title="矩阵的逆"></a>矩阵的逆</h3><p>有矩阵$A^{-1}A=I$,称矩阵$A^{-1}$为矩阵A的逆。奇异矩阵不可逆。运算规律：</p><ol><li>$(AB)^{-1}=B^{-1}A^{-1}$</li><li>$(A^T)^{-1}=(A^{-1})^{T}$</li><li>$(A^{-1})^{-1}=A$</li></ol><h3 id="矩阵方程的解法"><a href="#矩阵方程的解法" class="headerlink" title="矩阵方程的解法"></a>矩阵方程的解法</h3><p>对于任何一个线性方程组可以改写成：</p><p>$$<br>Ax=b</p><p>$$</p><p>A是系数矩阵，x是参数向量，b是方程组右边组成的常数向量<br>解方程只需要求出$x=Ab$</p><ul><li>线性无关<br>如果A的列向量$a_1,a_2,a_3…a_n$的线性组合为0：</li></ul><p>$$<br>\sum λ_ia_i=0</p><p>$$</p><p>称这些向量是线性无关的。<br>A中线性无关列向量的最大数目表示A的秩(Rank)<br>$Nul(A)$表示线性齐次方程$Ax=0$的解集，它的维度称为零度(Nullity)<br>如果 $RanK(A)+Nullity(A)=columns<del>of</del>A$，可以判断A是可逆的。</p><h3 id="正交-Orthogonal-Perpenticular"><a href="#正交-Orthogonal-Perpenticular" class="headerlink" title="正交(Orthogonal/Perpenticular)"></a>正交(Orthogonal/Perpenticular)</h3><p>两个向量$x,y$,如果$x^Ty=0$，称这两个向量是正交的。<br>如果一个向量集$b_1,b_2,b_3…b_n$中的任意两个元素 $b_i^Tb_j=\begin{dcases}<br>1, i=j \ 0, i \not =j<br>\end{dcases}$<br>称这个向量集是标准正交集(Orthonormal)<br>若矩阵Q，$Q^TQ=I$称Q是正交的，它所有的列向量都是正交的</p><h3 id="行列式计算"><a href="#行列式计算" class="headerlink" title="行列式计算"></a>行列式计算</h3><p>det(A)或者|A|记作A的行列式,在Python中可以用numpy库中的函数进行运算。计算性质：</p><ol><li>$det(AB)=det(A)det(B)$</li><li>$det(A^{-1})=\frac{1}{det(A)}$<br>所以det(A)=0时，A不可逆</li><li>$det(A^T)=det(A)$</li><li>$det(kA)=k^ndet(A)$,A_{n \times n}</li><li>$det(A)=Πλ_i$</li></ol><h3 id="特征值-特征向量"><a href="#特征值-特征向量" class="headerlink" title="特征值/特征向量"></a>特征值/特征向量</h3><p>若有$Ax=λx$,λ称为A的特征值(Eigenvalue)，x称为A的特征向量(Eigenvector)</p><blockquote><p>特征值可能是一个复数，矩阵的特征向量/特征值可能有几个是相同的<br>$kλ$和$kx$仍然是A的特征值和特征向量，所以默认解出的特征向量的模长(Norm)为1。</p></blockquote><ul><li><p>求解特征值/特征向量<br>通过$det(A-λI)=0$，求解$λ$,再代回$Ax-λx=0$求解x</p></li><li><p>谱分解(Spectral Theorem)<br>A所有的特征值和特征向量可以写成一个矩阵方程：</p><p>$$<br>A \begin{bmatrix}<br>  x_1&amp;x_2&amp;…&amp;x_n<br>\end{bmatrix}=\begin{bmatrix}<br>  x_1&amp;x_2&amp;…&amp;x_n<br>\end{bmatrix}\begin{bmatrix}<br>  λ_1 &amp;&amp;&amp;&amp;\ &amp;λ_2\&amp;&amp;λ_3\&amp;&amp;&amp;…<br>\end{bmatrix}$$</p><p>$\begin{bmatrix}<br>λ_1 &amp;&amp;&amp;&amp;\ &amp;λ_2\&amp;&amp;λ_3\&amp;&amp;&amp;…<br>\end{bmatrix}$ 称为A的对角矩阵(Diagonal matrix)$Λ$<br>记作$AE=EΛ$<br>如果E是可逆矩阵，$A=EΛE^{-1}$<br>如果A是对称矩阵，有$E^{-1}=E^T$,$A=EΛE^T$<br>在机器学习中，A常常是对称的，而且所有的特征值都是实数</p></li></ul><h3 id="迹-Trace"><a href="#迹-Trace" class="headerlink" title="迹(Trace)"></a>迹(Trace)</h3><p>矩阵A的\对角线元素的总和称为A的迹：</p><p>$$<br>tr(A)=\sum a_{ii}</p><p>$$</p><p>它在数值上也等于所有特征值的和：</p><p>$$<br>tr(A)=\sum λ_{i}</p><p>$$</p><p>计算性质：</p><ol><li>$tr(AB)=tr(BA)$</li><li>$tr(A+B)=tr(A)+tr(B)$</li></ol><h3 id="伪逆矩阵-Pseudo-inverse"><a href="#伪逆矩阵-Pseudo-inverse" class="headerlink" title="伪逆矩阵(Pseudo-inverse)"></a>伪逆矩阵(Pseudo-inverse)</h3><p>当A不可逆时，要解决$Ax=b$,转写为$x=A^{-1}b$的形式求解x看似不可能，因此构造矩阵$A^{+}$,使得$x=A^{+}b,Ax-b$的模长最小，$A^+$称为A的伪逆矩阵。</p><p>$$<br>A^+=(A^TA)^{-1}A^T</p><p>$$</p><p>$$<br>A^+A=(A^TA)^{-1}A^TA=I</p><p>$$</p><p>$$<br>AA^+=A (A^TA)^{-1}A^T\not=I</p><p>$$</p><p>在Python中<code>pinv(A)</code>可以实现求解伪逆矩阵</p><h3 id="矩阵的导数"><a href="#矩阵的导数" class="headerlink" title="矩阵的导数"></a>矩阵的导数</h3><p>矩阵的导数满足如下性质：</p><ol><li>$\frac{d}{dx}Ax=A^T$</li><li>$\frac{dx}{dx}=I$</li><li>$\frac{y^Tx}{dx}=\frac{dx^Ty}{dx}=y$</li><li>$\frac{d(x^TAx)}{dx}=\begin{dcases}<br>(A+A^T)x,A<del>is</del>square\2Ax,A<del>is</del>symmetrix<br>\end{dcases}$</li><li>$\frac{d(u^T(x)~v(x))}{dx}=[\frac{du^T}{dx}]v+[\frac{dv^T}{dx}]u$</li><li>$\frac{d~tr(A)}{dA}=I$</li><li>$\frac{det(A)}{dA}=det(A)(A^{-1})^T$</li></ol><ul><li><p>伪逆矩阵证明当A为奇异矩阵时，求解$Ax=b$:定义误差(error)$e=Ax-b$，要使得$|e|$尽可能小：设$y=|e|^2$，$y=e^Te$$~~~=(Ax-b)^T(Ax-b)$$~~~=(Ax)^T(Ax)-(Ax)^Tb-b^T(Ax)+b^Tb$$~~~=x^TA^TAx-2b^T(Ax)+b^Tb$对y求导：$\frac{dy}{dx}=2A^TAx-2A^Tb+0$</p><blockquote><p>第一项，$A^TA$是一个对称矩阵，可以应用#4.<br>第二项，应用#3<br>第三项，$b^Tb$是一个常数</p></blockquote><p>令$\frac{dy}{dx}=2A^TAx-2A^Tb=0$：</p><p>$$<br>A^TAx=A^Tb</p><p>$$</p></li></ul><p>$$<br>(A^TA)^{-1}A^Tb=A^+b</p><p>$$</p><p>在Python中，<code>linalg.solve(A,B)</code>能够求解$x=A^+b$     </p><h2 id="概率论的基础概念"><a href="#概率论的基础概念" class="headerlink" title="概率论的基础概念"></a>概率论的基础概念</h2><ul><li><p>随机变量<br>随机实验是一种能够产生随机结果的可重复性实验，样本空间$S$表示随机实验中所有可能出现的结果构成的集合，事件(Event)是样本空间S的子集。<br>随机变量(Random variables)是将S对应到实数集的一种函数，概率分布函数(MPF)表示X在样本空间中所发生的概率与X之间的关系。概率密度函数(PDF)表示样本空间中概率分布的稠密程度。    </p></li><li><p>常见的随机分布<br>*具体翻阅概率论笔记，此处不再赘述。<br>两点分布(Bernouli)<br>几何分布(Geometric)<br>二项分布(Binomial)<br>泊松分布(Poisson)<br>均匀分布(Uniform)<br>指数分布(Exponential)<br>双指数分布(Laplace/Double Exponential): $F(x)=\frac{λ}{2}e^{-λ|x|},λ&gt;0$<br>正态分布(Gaussian/Normal)  </p></li><li><p>其他分布函数概念<br>联合概率密度/联合分布函数(Joint PDF)<br>边缘分布函数(Marginal PDF)<br>条件概率函数(Conditional PDF)   </p></li></ul><h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h2><p>  全概率公式的逆公式，表示已知B事件发生的概率下，在A中某一个划分下发生的概率：<br>  $$P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}=\frac{P(A_i)P(B|A_i)}{ΣP(A_i)P(B|A_i)}$$   </p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>课后练习-图像处理</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/3.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/3.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 40%;    padding-left: 10%;}</style><h1 id="课后练习2"><a href="#课后练习2" class="headerlink" title="课后练习2"></a>课后练习2</h1><p>Required libs:Numpy PIL Scipy Matplotlib cv2</p><h2 id="Q1-Write-a-python-script-to-open-the-“lena-png”-file-using-opencv"><a href="#Q1-Write-a-python-script-to-open-the-“lena-png”-file-using-opencv" class="headerlink" title="Q1. Write a python script to open the “lena.png” file using opencv."></a>Q1. Write a python script to open the “lena.png” file using opencv.</h2><ul><li>Display the opened image in a new window named “Display Lena”</li><li>Save the image to a new file named “lena_resaved.png”<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/lena.png">  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br>img = cv.imread(<span class="hljs-string">&quot;lena.png&quot;</span>) <span class="hljs-comment"># cv2.imread(&#x27;path&#x27;)  read the img</span><br>cv.imshow(<span class="hljs-string">&quot;Display Lena&quot;</span>,img) <span class="hljs-comment">#cv2.imshow(windowname,path)</span><br>cv.waitkey(<span class="hljs-number">0</span>) <span class="hljs-comment">#to let the window display until clicking/pressing</span><br>cv.imwrite(<span class="hljs-string">&quot;lena_resaved.png&quot;</span>,img) <span class="hljs-comment">#cv2.imwrite(filename,path,params)</span><br></code></pre></td></tr></table></figure><h2 id="Q2-Use-PIL-and-Matplotlib-libraries-for-Q2"><a href="#Q2-Use-PIL-and-Matplotlib-libraries-for-Q2" class="headerlink" title="Q2. Use PIL and Matplotlib libraries for Q2."></a>Q2. Use PIL and Matplotlib libraries for Q2.</h2>Use “lena.png” to perform following operations and save the images:  </li><li>Crop a section from the image whose vertices are (100,100), (100,400), (400,100), (400,400).<br>(hint: convert the cv2 image into PIL Image)  </li><li>Rotate the cropped image by 45 degrees counter-clockwise.</li><li>Perform histogram equalization on lena.png. (hint: use ImageOps.equalize from PIL)</li><li>Use matplotlib to plot the histogram figure for both original image and processed image.<br>(hint: use histogram() function in PIL)  </li><li>Perform Max Filtering, Min Filtering, and Median Filter on lena.png. (hint: PIL.ImageFilter)  </li><li>Perform Gaussian Blur with sigma equal to 3 and 5. (hint: PIL.ImageFilter)<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/bee.png"> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageOps <br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageFilter <span class="hljs-keyword">as</span> <span class="hljs-built_in">filter</span><br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt  <br><br>pil_img=Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;lena.png&quot;</span>) <span class="hljs-comment">#open img in pil </span><br><span class="hljs-comment">#(in cv2 lib, img is opened as array)</span><br><span class="hljs-comment"># load cv img: Image.fromarray()</span><br>pil_img.show() <span class="hljs-comment"># show the img</span><br>img_crop = pil_img.crop((<span class="hljs-number">100</span>,<span class="hljs-number">100</span>,<span class="hljs-number">300</span>,<span class="hljs-number">300</span>)) <span class="hljs-comment">#crop((start point,hight,width))</span><br>img_crop.show() <span class="hljs-comment">#show the img</span><br><br>img_rota = img_crop.rotate(<span class="hljs-number">45</span>) <span class="hljs-comment">#rotate(degree)</span><br>img_rota.show()<br><br>img_eql=ImageOps.equalize(pil_img) <br><span class="hljs-comment">#ImageOps.equalize(path) histogram equalize the imge</span><br><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">256</span>),img_eql.histogram()) <br><span class="hljs-comment">#pyplot(aix,img) plot someting   </span><br><span class="hljs-comment">#img.histogram()  return the histogram</span><br>plt.show()<br>plt.show(rang(<span class="hljs-number">0</span>,<span class="hljs-number">256</span>),pil_img.histogram())<br>plt.show()  <br><br>img_max = pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.maxfilter()) <br><span class="hljs-comment">#filter.(Imagefilter.parm()) add filters</span><br>img_max.show()<br><br>img_min=pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.minfilter())<br>img_min.show()<br><br>img_mid=pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.medianfilter())<br>img_mid.show()<br><br><br>img_gus3=pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.gaussianblur(radius=<span class="hljs-number">3</span>))<br>img_gus3.show()<br><br>img_gus10=pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.gaussianblur(radius=<span class="hljs-number">10</span>))<br>img_gus10.show()<br><br></code></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210210191614.png"></li></ul><h2 id="Q3-Colour-space-conversion-Use-Python-OpenCV-functions-to-perform-following-operations-on"><a href="#Q3-Colour-space-conversion-Use-Python-OpenCV-functions-to-perform-following-operations-on" class="headerlink" title="Q3. Colour space conversion. Use Python OpenCV functions to perform following operations on"></a>Q3. Colour space conversion. Use Python OpenCV functions to perform following operations on</h2><p>“bee.png” and save the images at each step.</p><ul><li>Read the image.</li><li>Convert the image to HSV(<strong>Hu Satuation Value:包含了三个通道：单色(H)，饱和度(S)，灰度(V)</strong>) color space.</li><li>Perform histogram equalization on V channel by cv2.equalizeHist().</li><li>Convert the result image to BGR color space.</li><li>Show the image by cv2.imshow() and save the image.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageOps <br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageFilter <span class="hljs-keyword">as</span> <span class="hljs-built_in">filter</span><br><br>bee_img = cv2.imread(<span class="hljs-string">&quot;bee.png&quot;</span>)<br>bee_hsv = cv2.cvtColor(bee_img,cv2.COLOR_BGR2HSV)<br>bee_hsv.imshow()<br><br>bee_hsv[:,:,<span class="hljs-number">2</span>]= cv2.equalizeHist(bee_hsv[:,:,<span class="hljs-number">2</span>])<br><span class="hljs-comment"># 2 presents the channel 2: V</span><br>bee_rgb = cv2.cvtColor(bee_hsv,cv2.COLOR_HSV2BGR)<br>cv2.imshow(<span class="hljs-string">&quot;norm&quot;</span>,bee_rgb)<br><br>bee_img[:,:,<span class="hljs-number">2</span>]= cv2.equalizeHist(bee_rgb[:,:,<span class="hljs-number">2</span>])<br>bee_img = cv2.cvtColor(bee_hsv,cv2.COLOR_HSV2BGR)<br>cv2.imshow(<span class="hljs-string">&quot;rgb&quot;</span>,bee_img)<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4. 分类器</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/4.%20%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/4.%20%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 60%;    padding-left: 30%;}</style><h1 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h1><h2 id="分类器概述"><a href="#分类器概述" class="headerlink" title="分类器概述"></a>分类器概述</h2><p>设$S={ω_1,ω_2,..,ω_c}$是表示所有特征标签ω的集合，x表示数据集空间$R^n$中的特征向量，定义：<strong>分类器</strong>（Classifier）是一种能够使$R^n→S$的函数$f$。分类器能够将特征标签（labels）指定到特征向量。  </p><ul><li>图像识别的基本流程<br>输入图像—&gt;预处理-&gt;获取特征-&gt;分类<h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2></li></ul><p><strong>特征</strong>(Features)是不同类别的数据具有的用于识别其自身的属性。在机器学习中，要想对数据集进行识别和分类就必须要提取数据集的特征。<br>特征的提取并不是越多越好，不相关的特征（称为噪声（Noise features））会降低识别的准确度；具有高相关性的特征（比如：长发和女性）会让模型出现过拟合（Generalization）和模型冗余之类的其他问题。  </p><ul><li><p>决策边界<br><strong>决策边界</strong>（Decision boundary）是二元分类中能够依据特征的分布来分出两类的边界。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208145142.png">   </p></li><li><p>过拟合问题（Generalization）<br>如果一个模型虽然可以穿过所有的数据点，但是其图像波动很大，其同样也不能描述数据的分布，（其数据的分布是无法被泛化处理），称为过拟合，或者说这个算法具有高方差的特性。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208150529.png"></p></li></ul><h2 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h2><p><strong>贝叶斯分类器</strong>(Bayes’ classifier)被理论证明是目前最好的分类器。贝叶斯分类器依赖于模式识别(Pattern recognition)<br>想象如下的情形：我们已经测量了用于识别男女性别的特征，现在要根据这些特征来对一个未知的人判断其性别。如果你对其不做任何的测量，那么你是否还能进行分类？<br>₋答案是：如果我们知道男女性别的比例，比如男性/总的人群:$P{ω<em>1}=58.8%$。将频率视为概率，那么这个概率被称为<strong>先验概率</strong>（Prior probability），由于是男性的概率大于是女性的概率，因此我们将这个识别目标<strong>总是判断为概率最高的标签</strong>——即男性，那么我们判断其为男性的正确率为58.8%。<br>如何去优化这个正确率？——对识别目标进行观测：<br>假设已经观测到目标的特征$X$，对标签集$Ω$,计算所有的如果具有特性$x$,识别目标为标签$ω<em>i$的概率，并从中找到最大的条件概率，目标特征的标签$ω^*$即为最大的条件概率。用数学公式表达为：<br>$$ω^*=arg</em>{ω_i}~maxP(ω_i|x)$$<br>由计算得出的概率$P(ω_i|x)$称为<strong>后验概率</strong>(Posteriori)。<br>由贝叶斯公式：<br>$$P(A|B)=\frac{P(B|A)×P(A)}{P(B)}$$<br>那么后验概率可以转化为：<br>$$P(ω_i|x)=\frac{P(x|ω_i)·P(ω_i)}{P(x)}$$<br>带入分类器公式：<br>$$ω^*=arg</em>{ω<em>i}~max\frac{P(x|ω_i)·P(ω_i)}{P(x)}$$<br>由于$P(x)$是一个常数，那么最大值函数可以被简化为求$P(x|ω_i)·P(ω_i)$的最大值：<br>$$ω^*=arg</em>{ω_i}~maxP(x|ω_i)·P(ω_i)$$<br>这就是贝叶斯分类器公式。    </p><ul><li><p>特殊情况<br>如果先验概率是均等的：<br>$$P(ω<em>i)=P(ω_1)=…=P(ω_n)=C$$<br>那么分类器公式还能被简化为：<br>$$ω^*=arg</em>{ω_i}~maxP(x|ω_i)$$<br>称为最大可能公式（Maximunm Likelihood）。   </p><p>如果分类器中只有两个标签$ω_1,ω_2$:<br>那么设定：<br>$$g(x)=P(ω_1|x)-P(ω_2|x)$$<br>如果$g(x)&gt;0$则判断为$ω_1$,反之判断为另一类。$g(x)$称为判别函数（Discriminant function）。   </p></li><li><p>代价/损失（Cost）<br>设对于标签集${ω<em>1,ω_2,…,ω_c}∈C$,$λ</em>{ij}$是分类器判断为$ω<em>i$但实际上的标签是$ω<em>j$所作出的<strong>代价</strong>（Cost）。<br>规定在$λ$中，当$i=j$时，$λ</em>{ij}=0$。<br>那么二元的贝叶斯分类器的代价函数为：<br>$$\frac{P(x|ω_1)}{P(x|ω_2)}&gt;\frac{λ</em>{12}-λ<em>{22}}{λ</em>{21}-λ<em>{11}}·\frac{P(ω<em>2)}{P(ω_1)}…….ω_1$$<br>$$\frac{P(x|ω_1)}{P(x|ω_2)}&lt;\frac{λ</em>{12}-λ</em>{22}}{λ<em>{21}-λ</em>{11}}·\frac{P(ω<em>2)}{P(ω_1)}…….ω_2$$<br>如果$λ</em>{12}=λ<em>{21}=1$且$λ</em>{11}=λ_{22}=0$,称$P(x|ω_i)P(ω_i)$为MAP方程。</p></li></ul><blockquote><p>贝叶斯分类器被证明是理论上误差最小的分类器。     </p></blockquote><p>运用贝叶斯分类器需要知道在有特征$x$的条件下是分类标签$ω_i$的概率——$P(ω_i|x)$，称为可能性（Likelyhood）。在实际运用当中，一般是从数据集中估计这个概率（采用抽样检测等方法），这个估计出的概率通常是不准确的。<br>这个抽样检测的原则是：<strong>如果要创建一个D维（D是特征向量X的维度，即特征的数量）的直方图，一般而言至少需要$10^D$的训练样本。</strong></p><h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><p>解决贝叶斯分类器需要的训练样本数量大的问题的其中一个办法是假设所有的特征之间是独立的，根据概率论，有：<br>$$P(XY)=P(X)P(Y)$$<br>假设特征向量$x=[x_1,x_2,…,x_D]^T$，有：<br>$$ω^*=arg_{ω<em>j}~maxP(x|ω_j)P(ω_j)$$<br>$$ω^*=arg</em>{ω_j}~maxP(ω<em>j)Π</em>{i=1}^DP(x_i|ω_j)$$  </p><blockquote><p>在实际中，由于$P(x_i|ω<em>j)∈[0,1]$，因此$Π</em>{i=1}^DP(x_i|ω_j)$的乘积可能会下溢（非常趋近0）。因此对两边取log函数将乘法项目转为加法项防止下溢。   </p></blockquote><p>$$ω^*=arg_{ω_j}<del>max</del>log[P(ω<em>j)]+∑</em>{i=1}^Dlog[P(x_i|ω_j)]$$<br>这种分类器公式称为朴素贝叶斯分类器（Naive Bayes）   </p><h2 id="K邻近算法"><a href="#K邻近算法" class="headerlink" title="K邻近算法"></a>K邻近算法</h2><p>K邻近算法（K-Nearest Neighbor,KNN）是一种不依赖概率而直接求得决策边界的办法。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210215095611.png"><br>如上图，设想现在样本空间内有两类样本，新加入一个x到样本空间内，设定$k=5$,计算x到样本空间内所有点的距离，最终取5个距离x最近的样本点，这五个样本点中哪一种类别的样本点多x就是哪一种类别。<br>通常情况下，标签数和K是都是奇数。<br>有数学证明在训练样本足够多的条件下， KNN的错误概率相比于贝叶斯更小。<br>$$P(error_{KNN})⪙P(error_{Bayes})$$</p><ul><li>距离度量（Distance Metrics）<br>设定距离度量函数$D(x,y)$,具有非负性、唯一性、和三角矢量性。<br>$$D_p(x,y)=(∑<em>{i=1}^n|x_i-y_i|^p)^{1/p}$$<br>x,y为向量。p称为范数（Norm）。<br>为了避免x，y的数值过于悬殊，人为地添加权重$w_i$，有：<br>$$D_p(x,y)=(∑</em>{i=1}^nw_i|x_i-y_i|^p)^{1/p}$$   </li></ul><h2 id="其他分类器"><a href="#其他分类器" class="headerlink" title="其他分类器"></a>其他分类器</h2><ul><li>神经网络<br>神经网络是目前最热门的分类器方法，它模拟了神经元的传递过程，即输入信号——处理信号——接收信号。 </li><li>向量机（Support vector machine,SVM）<br>向量机的目的是为了找到一个线性的决策边界。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210215102440.png">  </li></ul><h2 id="性能评估"><a href="#性能评估" class="headerlink" title="性能评估"></a>性能评估</h2><p>现在要对一个分类器的效果进行评估，方法是用另一组数据集去测试分类器的性能。在实际运用中，通常把训练集划分为两部分：训练集和测试集。 测试集不会被训练。 将测试集放入分类器后，分类器得出的标签和测试集中的标签进行对比。   </p><h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p>有如下将测试结果可视化的方法，称为<strong>混淆矩阵</strong>(Confusion matrix)方法：<br>将横轴作为实际的标签，纵轴作为预测的标签，每一格表示“实际为标签i/但是预测为标签j”的频率，做出矩阵，如下图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210217114634.png"><br>对角线上频率的总和即为训练集的正确率。<br>混淆矩阵能够容易的表现出分类器错误的分类情况。  </p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p><strong>交叉验证</strong>（K-fold cross validation）能够最大程度的避免测试集发生的“偶然正确（称为福禄克测试，Fluke test）”，具体的做法是：<br>将数据集平均分为k份，取其中一份为测试集，剩下的为训练集。重复上述步骤直到每一份都被做过训练集。 最终分类器的准确率为所有测试的准确率的平均值。    </p><h3 id="错误类型与ROC曲线"><a href="#错误类型与ROC曲线" class="headerlink" title="错误类型与ROC曲线"></a>错误类型与ROC曲线</h3><ul><li><p>FRR<br>False Reject Rate， 表示目标正确却识别为错误的概率。   </p></li><li><p>FAR<br>False Accept Rate， 表示目标错误却识别为正确的概率。</p></li><li><p>FTE<br>Failure to Enroll Rate, 无法识别的概率。</p></li></ul><p>理想条件下，FRR和FAR都应该等于0。不断地改变分类器的阈值，将横轴为FAR,纵轴为1-FRR，作出<strong>ROC曲线</strong>(受试者工作特征曲线, Receiver operating characteristic curve)。这条曲线始终在y=x以上。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210217121830.png"><br>理想条件下，ROC曲线应该是一个L形状，即FAR=FRR=0。<br>ROC曲线围成的下夹面积，即AUC表示了系统的强壮性，AUC越大越好。<br>EER(Equal error rate)，也就是FPR=FNR的值，由于FNR=1-TPR，可以画一条从（0,1）到（1,0）的直线，找到直线与ROC曲线的交点。 交点越靠近(1,1)越好。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.4. 多元分类</title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.4%20%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.4%20%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>一对多分类的实质是对一对一分类的拓展。<br>基本方法是建立一个多输出的神经网络，因此<strong>在多元分类中，最终的输出结果将是一个n维的向量</strong>，输出层的每一个输出单元用于判断是否是某一类（例如：是否是行人，是否是自行车），当判断为结果是某一类时，在理想情况下，这个网络会在这一输出单元输出1，其他的输出单元输出0，最终输出的结果是如：$h_Θ(x)≈\left[\begin{smallmatrix} 1 \\ 0 \\ 0 \\ 0 \end{smallmatrix}\right]$之类的向量。<br>以前我们在训练集中用一个整数y来表示分类的标签，在多元分类中，我们使用如上所示的向量来表示分类的标签。<br>现在假设函数的模型应该为：<br>$$h_Θ(x^{(i)})≈y^{(i)}$$<br>等式的左右两边输出的结果都是n维的向量。</p>]]></content>
    
    
    <categories>
      
      <category>4. 神经网络简介</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.3. 神经网络与逻辑函数</title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.3%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.3%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 50%;<br>    padding-left: 20%;<br>}</style></p><h1 id="神经网络与逻辑函数"><a href="#神经网络与逻辑函数" class="headerlink" title="神经网络与逻辑函数"></a>神经网络与逻辑函数</h1><p>思考下面一个例子：<br>如果$x_1,x_2$都为二进制数，如图有四个样本分布在样本空间内，<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101151549.png" alt=""><br>那么假设函数可以写成：</p><script type="math/tex; mode=display">y=x_1 XNOR x_2</script><p>那么神经网络是否可以生成这样的函数呢？ </p><h2 id="简单逻辑函数的实现——AND-OR-NOT"><a href="#简单逻辑函数的实现——AND-OR-NOT" class="headerlink" title="简单逻辑函数的实现——AND,OR,NOT"></a>简单逻辑函数的实现——AND,OR,NOT</h2><p>为了解决这个问题，我们从AND函数入手：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101151722.png" alt=""><br>如图，输入层有两个特征$x_1$和$x_2$，他们是二进制数。目标函数为$y=x_1 AND x_2$.  </p><p><strong>观察激活函数$y=g(x)$,我们发现当$x=4$时，$y=0.99$,当$x=-4$时，$y=0.01$</strong><br>由上述激活函数的性质，为了计算这样的神经网络，首先先增加一个偏置单元 【+1】，对每个单元赋予权重：-30，20,20<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101152043.png" alt=""><br>列出真值表：  </p><script type="math/tex; mode=display">\begin{array}{lcr}x_1 & x_2 & h_θ(x) \\\    0 & 0 & g(-30)≈0 \\\   0 & 1 & g(-10)≈0  \\\  1 & 0 & g(-10)≈0  \\\  1 & 1 & g(10)≈1 \\\  \end{array}</script><p>观察最后一列，我们发现最后一列的输出事实上很接近与AND函数的结果，那么可以认为$h_θ(x) ≈ x_1 AND x_2$</p><p>那么同理，下图的神经网络最终可以生成一个类似于OR函数的假设函数。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101152850.png" alt="">  </p><p>下图的神经网络最终可以生成一个类似于NOT函数的假设函数。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101153145.png" alt=""><br>可以发现NOT是通过给对应的单元施加一个较大的负数来实现的。</p><h2 id="复杂逻辑函数的实现"><a href="#复杂逻辑函数的实现" class="headerlink" title="复杂逻辑函数的实现"></a>复杂逻辑函数的实现</h2><p>下面我们来试试生成如下的函数：  </p><script type="math/tex; mode=display">h_θ(x)=(NOT x_1)AND(NOT x_2)</script><p>分析：<br>要想使$h_θ(x)=1$,那么当且仅当$x_1=x_2=0$时成立，最终的神经网络如下图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101154246.png" alt="">   </p><p>现在我们可以应付本节一开头的问题了——如何使神经网络生成$y=x_1 XNOR x_2$？<br>分析： </p><script type="math/tex; mode=display">x_1XNORx_2=NOT(X_1 XOR X_2)=(x_1 AND x_2)OR((NOT x_1) AND (NOT x_2))</script><p>以逻辑表达式形式书写：</p><script type="math/tex; mode=display">h_θ(x)=(x_1.x_2)+\overline{x_1}.\overline{x_2}</script><p>将它分层：<br>第一层： 获取$x_1$和$x_2$<br>第二层：计算  $a_1^{(2)}=x_1.x_2$  和  $a_2^{(2)}=\overline{x_1}.\overline{x_2}$.<br>第三层：计算  $a_1^{(2)}+a_2^{(2)}$.<br>通过这一节的前半部分，我们已经知道了每一层所需要的函数的神经网络构建方法，最终的神经网络将是：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101155720.png" alt="">   </p><blockquote><p>实例： 可视化Minst手写字符数据集识别</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>4. 神经网络简介</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.2. 神经网络的基本模型</title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 60%;<br>    padding-left: 20%;<br>}</style></p><h1 id="神经网络的基本模型"><a href="#神经网络的基本模型" class="headerlink" title="神经网络的基本模型"></a>神经网络的基本模型</h1><h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><ul><li>假设： 大脑对于不同功能（听觉，视觉，触觉的处理）的实现是依赖于同样的学习方法  </li><li>依据： 神经重接实验  </li><li>神经元模型<br>神经网络模拟了大脑中的神经元或者是神经网络。先来看大脑中的神经元构成，我们会发现神经元有很多的输入通道（树突），同时通过轴突给其他的神经元传递信号。  将神经元简单抽象：一个计算单元，它从输入端接收一定数目的信息，并作一些处理，并将结果传递给其他的神经元。</li></ul><p>在计算机中，我们构建一个逻辑单元，它从输入端接收数据集X，并作处理来生成一个激活函数$h_θ (x)=\frac{1}{1+e^{-θ^T X}}$<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201229183932.png" alt=""><br>在这个模型之上，输入端会额外增加一个$x_0=1$，称为偏置单元。<br>在神经网络中，$Θ$称为模型的权重，$g(z)=\frac{1}{1+e^{-z}}$称为激活函数。  </p><p>神经网络是一组神经元连接在一起的集合，如图所示<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201229184740.png" alt=""><br>第一层称为输入层，我们在这一层输入全部的特征，最后一层称为输出层，这一层的神经元输出假设的最终结果，中间的层称为隐藏层，隐藏层可能不止有一层。<br>统一地，$a_i^{(j)}$将表示第j层的第i个激活项（激活指计算并输出结果），同时，第j层到第j+1层之间的映射由参数矩阵$Θ^{(j)}$确定，那么上图就可以用公式表示为：</p><script type="math/tex; mode=display">a_1^{(2)}=g(Θ_{10}^{(1)}x_0+Θ_{11}^{(1)}x_1+Θ_{12}^{(1)}x_2+Θ_{13}^{(1)}x_3)</script><script type="math/tex; mode=display">a_2^{(2)}=g(Θ_{20}^{(1)}x_0+Θ_{21}^{(1)}x_1+Θ_{22}^{(1)}x_2+Θ_{23}^{(1)}x_3)</script><script type="math/tex; mode=display">a_3^{(2)}=g(Θ_{30}^{(1)}x_0+Θ_{31}^{(1)}x_1+Θ_{32}^{(1)}x_2+Θ_{33}^{(1)}x_3)</script><script type="math/tex; mode=display">h_{Θ}(x)=g(Θ_{10}^{(2)}a_0^{(2)}+Θ_{11}^{(2)}a_1^{(2)}+Θ_{12}^{(2)}a_2^{(2)}+Θ_{13}^{(2)}a_3^{(2)})</script><p>如果一个网络在第j层有$s<em>j$个单元，且在第j+1层有$s_j+1$个单元，那么矩阵$Θ^{(j)}$的维度为$s</em>{j+1} \times (s_j+1)$</p><h2 id="神经网络的向量化-前向传输-Forward-propagation"><a href="#神经网络的向量化-前向传输-Forward-propagation" class="headerlink" title="神经网络的向量化:前向传输(Forward propagation)"></a>神经网络的向量化:前向传输(Forward propagation)</h2><p>对如上的等式，现在将$g()$中的线性加权组合以$z^{(2)}_1,z^{(2)}_2,z^{(2)}_3$表示，那么就有：</p><script type="math/tex; mode=display">a_1^{(2)}=g(z_1^{(2)})</script><script type="math/tex; mode=display">a_2^{(2)}=g(z_2^{(2)})</script><script type="math/tex; mode=display">a_3^{(2)}=g(z_3^{(2)})</script><p>现在就能够定义三个向量使得上述等式转化为向量乘法：<br>$x= \left[\begin{smallmatrix} x_0 \\ x_1 \\ x_2 \\ x_3 \end{smallmatrix}\right]$, $z^{(2)}=\left[\begin{smallmatrix} z_1^{(2)}\\ z_2^{(2)}\\ z_3^{(2)}\end{smallmatrix}\right]=Θ^{(1)}x$，$a^{(2)}=\left[\begin{smallmatrix} a_1^{(2)}\\ a_2^{(2)}\\ a_3^{(2)}\end{smallmatrix}\right]$  </p><p>那么上述等式最终就可以转化成：</p><script type="math/tex; mode=display">z^{(2)}=Θ^{(1)}x</script><script type="math/tex; mode=display">a^{(2)}=g(z^{(2)})</script><p>对于隐藏层的偏置单元，增加一项$a_0^{(2)}=1$.<br>最后计算$z^{(3)}=Θ^{(2)}a^{(2)}$,那么最终得到的假设模型将会是：    </p><script type="math/tex; mode=display">h_{Θ}(x)=a^{(3)}=g(z^{(3)})</script><p>单看layer2 和 layer3，事实上，这两层做的就是逻辑回归，但输入进逻辑回归的特征不再是原始的特征x，而是通过原始特征生成的特征$a$。<br>而$a$与$x$之间的关系通过θ来定义。 因此可以通过改变$θ$来改变输入层和隐藏层之间的关系。</p><blockquote><p>下一章将说明如何调整$θ$的值来优化假设函数。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>4. 神经网络简介</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.1. 神经网络的背景</title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%83%8C%E6%99%AF/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%83%8C%E6%99%AF/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 60%;    padding-left: 20%;}</style><h1 id="神经网络的背景知识"><a href="#神经网络的背景知识" class="headerlink" title="神经网络的背景知识"></a>神经网络的背景知识</h1><h2 id="激活函数算法的局限性"><a href="#激活函数算法的局限性" class="headerlink" title="激活函数算法的局限性"></a>激活函数算法的局限性</h2><p>假设一个数据集拥有非常多的原始特征和数据量，执行激活函数算法，那么次方项、交叉项会非常的多，计算量非常的大，最终的拟合结果也不好。<br>计算机视觉中的例子：<br>计算机读取到的是图片所对应的像素强度的矩阵。 </p><blockquote><p>对于灰度图像来说，像素强度就是每一个像素的灰度值。<br>对于RGB彩色图像来说，图片上的一个像素以三个值（R,G,B）/三维向量 来进行表示  </p></blockquote><p>如果现在设计一个分类器，使得计算机能够区分一个图片的主体是否为汽车。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/472E9216086782CF8029F2818CA1027A.png"><br>以图中的pixel1 和 pixel2的位置为例，我们可以把所有数据集中pixel1和pixel2的像素强度投射到坐标轴上，如图使用一个非线性假设来对图像进行分类。<br>如果对于一个50*50像素的图片数据集，那么训练集中将包含至少2500个原始特征（7500 RGB），这时候用激活函数算法计算量会非常的大。  </p>]]></content>
    
    
    <categories>
      
      <category>4. 神经网络简介</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.3. 正则化</title>
    <link href="/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.3.%20%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.3.%20%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h2><p>对于模型，如果一个模型对于数据的偏差很大，不能能够很好的拟合数据的分布，称为欠拟合，或者说这个算法具有高偏差的特性。 如果一个模型虽然可以穿过所有的数据点，但是其图像波动很大，其同样也不能描述数据的分布，（其数据的分布是无法被泛化处理），称为过拟合，或者说这个算法具有高方差的特性。 在这种情况下，模型的参数过于多（有可能代价函数正好为0），以至于可能没有足够多的数据去约束它来获得一个假设函数。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224205854.png"><br>过拟合现象往往会发生在<strong>参数过多，而训练样本过少的情况</strong>。减小过拟合现象的思路有两种： </p><ol><li>尽可能的去掉那些影响因素很小的变量，这种方法虽然解决了过拟合问题，但是损失了精度。  </li><li><strong>正则化</strong>（Regularization）  </li></ol><h2 id="代价函数的正则化"><a href="#代价函数的正则化" class="headerlink" title="代价函数的正则化"></a>代价函数的正则化</h2><p>对于代价函数：<br>$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2$$<br>增加两个惩罚项$1000\theta^2_3$和$1000\theta^2_4$，代价函数变为：<br>$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+1000\theta^2_3+1000\theta^2_4$$<br>如果要最小化这个函数，那么$\theta_3$与$\theta_4$就要尽可能的接近0，那么最后拟合的结果（假设函数）：$\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$，仍然是一个类似的二次函数.<br>正则化的基本思想是<strong>如果所有的参数足够小，那么假设模型就更简单。</strong>  </p><blockquote><p>事实上，如果参数足够小，得到的函数就会越平滑，越简单，越不容易出现过拟合的问题  </p></blockquote><p>在实际上，对于大量的特征和大量的参数，比如$x_1..x_{100}$和$\theta_0…\theta_{100}$，我们无法确定哪些参数是高阶项的参数，这个时候采用的方法就是对代价函数进行修改，使得所有的参数都尽可能的小。<br>修改后的代价函数方程：<br>$$ J_{\theta}=\frac{1}{2m}[\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+λ\Sigma_{j=1}^{m}\theta_j^2]$$<br>其中$λ\Sigma_{j=1}^{m}\theta_j^2$称为<strong>正则化项</strong>，它的目的是为了<strong>缩小每一项的参数</strong>。</p><blockquote><p>$\theta_0$是否正则化对结果影响不大<br>λ的作用是对“+”号的前后（前：更好的拟合训练集，后：假设函数足够简单）两项进行取舍平衡，称为正则化系数  </p></blockquote><p>如果λ被设置的太大，那么所有参数的惩罚力度被加大，这些参数最后的结构都将全部接近于0，那么最后的假设函数将会变成$h_\theta(x)=θ_0$,最终导致欠拟合。  </p><h2 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h2><p><strong>正则化的梯度下降算法</strong><br>在线性回归中，我们使用修改后的梯度下降算法：<br>Repeat {<br>$$θ_0:=θ<em>0-\alpha\frac{1}{m}\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(j)} \tag{1}$$</p><blockquote><p>$θ<em>0$  不需要正则化<br>$$θ_j:=θ_j-\alpha[\frac{1}{m}\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}θ<em>j] \tag{2}$$<br>$$j=1,2,3,…,n$$<br>}<br>事实上  $\frac{1}{m}\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(j)}+\frac{λ}{m}θ_j=\frac{∂J(θ)}{∂θ<em>j}$<br>$\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}=\frac{∂J(θ)}{∂θ_0}$  </p></blockquote><p>如果将（2）中的$\theta_j$统一，那么就可以得到（3）：<br>$$θ_j:=θ<em>j（1-α\frac{λ}{m}）-\frac{α}{m}\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)} \tag{3}$$</p><p>由于$1-α\frac{λ}{m}&lt;1$,且只比1小一点点，也就是说，梯度下降算法每次更新的时候$θ_j$在一开始都会比原来小一点点，再进行原来的梯度下降更新  </p><p><strong>正则化的正规方程算法</strong><br>在之前的讲义中，探讨过设计两个矩阵：<br>$X=\begin{bmatrix} (x^{(1)})^T \ …\ (x^{(m)})^T \end{bmatrix}$ 代表有m个数据的数据集 和 $y=\begin{bmatrix} y^{(1)} \ …\ y^{(m)} \end{bmatrix}$ 代表训练集当中的所有的标签<br>通过：<br>$$θ=(X^TX)^{-1}X^Ty$$<br>（相当于对$J(θ)$中的每一个θ求偏导数，并且使其等于0）<br>可以求出最适合的θ<br>现在改变在正规方程中加入一项：<br>$$θ=(X^TX+λ<br>\begin{bmatrix}<br>0 &amp; 0 &amp; 0 &amp; …&amp;0 \   0 &amp; 1 &amp; 0&amp; …&amp;0 \ 0 &amp; 0 &amp; 1&amp; …&amp;0 \… &amp; … &amp; …&amp; …&amp;…\0 &amp; 0 &amp; 0&amp; …&amp;1<br>\end{bmatrix})^{-1}X^Ty$$<br>来达到同样的效果  </p><blockquote><p>$\begin{bmatrix}<br>0 &amp; 0 &amp; 0 &amp; …&amp;0 \   0 &amp; 1 &amp; 0&amp; …&amp;0 \ 0 &amp; 0 &amp; 1&amp; …&amp;0 \… &amp; … &amp; …&amp; …&amp;…\0 &amp; 0 &amp; 0&amp; …&amp;1<br>\end{bmatrix}$是一个(n+1)的方阵  </p></blockquote><p>如果矩阵X不可逆$（m&lt;=n）$,那么$(X^TX)^{-1}$也同样不可逆,但是经过数学证明，无论如何$(X^TX+λ<br>\begin{bmatrix}<br>0 &amp; 0 &amp; 0 &amp; …&amp;0 \   0 &amp; 1 &amp; 0&amp; …&amp;0 \ 0 &amp; 0 &amp; 1&amp; …&amp;0 \… &amp; … &amp; …&amp; …&amp;…\0 &amp; 0 &amp; 0&amp; …&amp;1<br>\end{bmatrix})^{-1}$ 都是可逆的。</p>]]></content>
    
    
    <categories>
      
      <category>3. 逻辑回归与正则化</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.2. 分类算法的优化</title>
    <link href="/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.2.%20%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E5%8C%96/"/>
    <url>/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.2.%20%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="分类算法的优化"><a href="#分类算法的优化" class="headerlink" title="分类算法的优化"></a>分类算法的优化</h1><h2 id="其他代价函数优化算法"><a href="#其他代价函数优化算法" class="headerlink" title="其他代价函数优化算法"></a>其他代价函数优化算法</h2><p>对于代价函数，它可以被拆分成求$J(θ)$和求$\frac{∂J(θ)}{∂θ_j }$  两个基础的部分  </p><p>事实上，除了基础的梯度下降算法能够求到最小值之外，还有如下的集中基本方法：</p><ol><li>共轭梯度算法（Conjugate Gradient）</li><li>BFGS</li><li>L-BFGS</li></ol><p>这些算法有一些共同的特点：</p><ol><li>这些算法利用线搜索算法（一种智能内循环）不需要手动选择学习率 α。</li><li>收敛的速度高于梯度下降算法</li><li>复杂度高于梯度下降算法</li></ol><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>对于同一个数据集x，需要分类的目标不只有两种，意味着离散取值y的值不只有0，1两个值。<br>基本思想是<strong>将一个n元分类问题转化为n个二分类问题</strong>。<br>比如对y=1，2，3:<br>可以先将2，3 设置为负类，使用之前的逻辑分类方法就能够将1与（2，3）分开，重复三次，能得到三个逻辑斯蒂函数：$h_θ^i (x)i=1,2,3$。<br>由于$h_θ^i (x)=P(y=i│x; θ)$， 因此$h_θ^i (x)$表示将1设置为正类别，分类器中是1的概率。<br>最后给定函数$max(h_θ^i (x))$， 表示选择出三个当中概率最高的部分，作为判断的结果。</p>]]></content>
    
    
    <categories>
      
      <category>3. 逻辑回归与正则化</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.1. 激活函数</title>
    <link href="/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.1.%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <url>/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.1.%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 60%;    padding-left: 20%;}</style><h1 id="逻辑回归、激活函数及其代价函数"><a href="#逻辑回归、激活函数及其代价函数" class="headerlink" title="逻辑回归、激活函数及其代价函数"></a>逻辑回归、激活函数及其代价函数</h1><h2 id="线性回归的可行性"><a href="#线性回归的可行性" class="headerlink" title="线性回归的可行性"></a>线性回归的可行性</h2><p>对分类算法，其输出结果y只有两种结果{0，1}，分别表示负类和正类，代表没有目标和有目标。<br>在这种情况下，如果用传统的方法以线性拟合$（h_θ (x)=θ^T X）$，对于得到的函数应当对y设置阈值a，高于a为一类，低于a为一类。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224223513.png"><br>对于分类方法，这种拟合的方式极易受到分散的数据集的影响而导致损失函数的变化，以至于对于特定的损失函数，其阈值的设定十分困难。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224223523.png"><br>除此之外，$h_θ (x)$（在分类算法中称为分类器）的输出值很可能非常大或者非常小，并不与{0，1}完全相符</p><h2 id="假设表示"><a href="#假设表示" class="headerlink" title="假设表示"></a>假设表示</h2><p>基于上述情况，要使分类器的输出在[0,1]之间，可以采用假设表示的方法。<br>设$h_θ (x)=g(θ^T x)$，其中$g(z)=\frac{1}{(1+e^{−z} )}$, 称为<strong>逻辑斯蒂函数</strong>（Sigmoid function，又称为<strong>激活函数</strong>，生物学上的S型曲线），有：<br>$$h_θ (x)=\frac{1}{(1+e^{−θ^T X} )}$$<br>其两条渐近线分别为h(x)=0和h(x)=1</p><p>在分类条件下，最终的输出结果是：<br>$$h_θ (x)=P(y=1│x,θ)$$<br>其代表在给定x的条件下 其y=1的概率<br>有:<br>$$P(y=1│x,θ)+P(y=0│x,θ)=1$$</p><h2 id="决策边界（-Decision-boundary）"><a href="#决策边界（-Decision-boundary）" class="headerlink" title="决策边界（ Decision boundary）"></a>决策边界（ Decision boundary）</h2><p>对假设函数设定阈值$h(x)=0.5$，<br>当$h(x)≥0.5$ 时，输出结果y=1.<br>根据假设函数的性质，当 $x≥$0时，$h(x)≥0.5$<br>由于之前用$θ^T x$替换x，则当$θ^T x≥0$时，$h(x)≥0.5，y=1$<br>解出 $θ^T x≥0$，其答案将会是一个在每一个$x_i$轴上都有的不等式函数。<br>这个不等式函数将整个空间分成了y=1 和 y=0的两个部分，称之为决策边界。  </p><h2 id="激活函数的代价函数"><a href="#激活函数的代价函数" class="headerlink" title="激活函数的代价函数"></a>激活函数的代价函数</h2><p>在线性回归中的代价函数：<br>$$J(θ)=\frac{1}{m}∑_{i=1}^m \frac{1}{2} (h_θ (x^{(i)} )−y^{(i)} )^2 $$</p><p>令$Cost（hθ (x)，y）=\frac{1}{2}(h_θ (x^{(i)} )−y^{(i)} )^2$， Cost是一个非凹函数，有许多的局部最小值，不利于使用梯度下降法。对于分类算法，设置其代价函数为：</p><p>对其化简：<br>$$Cost（h_θ (x),y）=−ylog(h_θ (x))−((1−y)log⁡(1−h_θ (x)))$$<br>检验：<br>当 $y=1$时，$−log⁡(h_θ (x))$<br>当 $y=0$时，$−log⁡(1−h_θ (x))$  </p><p>那么代价函数可以写成：<br>$$J(θ)=-\frac{1}{m}[∑_{i=1}^m y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))]$$</p><p>对于代价函数，采用梯度下降算法求θ的最小值：<br>$$θ<em>j≔θ_j−α\frac{∂J(θ)}{∂θ_j}$$<br>代入梯度：<br>$$θ_j≔θ_j−α∑</em>{i=1}^m(h_θ (x^{(i)} )−y^{(i)} ) x_j^i $$</p>]]></content>
    
    
    <categories>
      
      <category>3. 逻辑回归与正则化</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2.4. 向量化</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.4.%20%E5%90%91%E9%87%8F%E5%8C%96/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.4.%20%E5%90%91%E9%87%8F%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h1><p>对于求和的算法，有时可以转换为矩阵的乘法来进行计算<br>E.g.<br>$$H(θ)(x)=∑_{j=0}^nθ_j x_j   $$<br>如果直接求和，求和的过程会非常冗长<br>而设计两个向量$θ$ $x$<br>则有线性回归假设函数的向量形式：<br>$$h(x)=θ^T x$$<br>对更新函数：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224232957.png"></p>]]></content>
    
    
    <categories>
      
      <category>2. Octave语言初步</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2.3. 控制和定义语句</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.3.%20%E6%8E%A7%E5%88%B6%E5%92%8C%E5%AE%9A%E4%B9%89%E8%AF%AD%E5%8F%A5/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.3.%20%E6%8E%A7%E5%88%B6%E5%92%8C%E5%AE%9A%E4%B9%89%E8%AF%AD%E5%8F%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="控制和定义语句"><a href="#控制和定义语句" class="headerlink" title="控制和定义语句"></a>控制和定义语句</h1><p>for i=1:10,<br>Indices=a : b 从a到b的索引<br>Break Continue 与C语言相同<br>While, end 结构体 同C语言  </p><p>选择结构：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> condition,    <br>command   <br>end   <br></code></pre></td></tr></table></figure><p>分支选择结构： </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> condition,   <br>command;   <br>elseif condition,   <br>Command;  <br></code></pre></td></tr></table></figure><p>循环结构：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">while</span> condition，  <br>Conmand;  <br> end  <br></code></pre></td></tr></table></figure><p>定义函数：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp">function y=function(x)  <br>command with Y,x;  <br></code></pre></td></tr></table></figure><p>保存为function.m文件  </p><p>返回多个值的函数：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp">function [y1,y2]=function(x)  <br>Command with y1,y2,x;  <br></code></pre></td></tr></table></figure><p>加载函数：<br>定位到m文件目录下  </p><p><code>addpath（&#39;path&#39;）</code>: 加入Octave路径  </p>]]></content>
    
    
    <categories>
      
      <category>2. Octave语言初步</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2.2. 数据计算和绘制</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.2.%20%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%92%8C%E7%BB%98%E5%88%B6%E5%9B%BE%E5%83%8F/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.2.%20%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%92%8C%E7%BB%98%E5%88%B6%E5%9B%BE%E5%83%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="数据计算和绘制"><a href="#数据计算和绘制" class="headerlink" title="数据计算和绘制"></a>数据计算和绘制</h1><h2 id="对元素的操作"><a href="#对元素的操作" class="headerlink" title="对元素的操作"></a>对元素的操作</h2><p><code>A.\*B</code> A矩阵和B矩阵的每一个元素对应相乘<br><code>.</code> 对每一个元素进行运算操作<br><code>abs(A)</code> 对A每一个元素取绝对值<br><code>v+1</code> 对向量v里面的每一个元素+1<br><code>A’</code> 矩阵A的转置<br><code>pinv(A)</code>对A求逆矩阵，不可逆时即为伪逆矩阵<br><code>max(A)</code> A中最大的元素的值<br><code>max(A,[], DI)</code> A中DI维度下元素最大的值（1 列 2 行）<br><code>ind()</code> 某个元素的位置<br><code>magic(n)</code> 返回n*n的幻方<br><code>find(condition)</code>查找对应条件的元素，并返回一个向量<br><code>sum(A)</code>A所有元素的和<br><code>prod(A)</code>A所有元素的乘积<br><code>ceil(A)</code> 对A向上取整<br><code>floor(A)</code>对A每个元素向下取整<br><code>flipud(A)</code>对A上下翻转  </p><h2 id="绘制图像"><a href="#绘制图像" class="headerlink" title="绘制图像"></a>绘制图像</h2><p><code>Plot(x,y,&#39;r&#39;)</code> 绘制关于x，y的图像 r表示y的函数是红色的（默认为蓝色）<br><code>hold on</code> 保存octave内存中的旧函数图像<br><code>xlabel(&#39;&#39;)</code>添加横轴标签<br><code>ylabel（‘’）</code>添加纵轴标签<br><code>legend(&#39;&#39;,&#39;&#39;)</code> 图例<br><code>title（‘’）</code>添加标题<br><code>print -dpng &#39;xx.png&#39;</code> 在当前路径下以png保存当前图像<br><code>close</code> 关闭当前图像<br><code>figure(1)</code>; 标记图像（多开图像窗口）<br><code>subplot(1,2,1)</code> 把图像分成1x2的网格 从第一个格图开始画图<br><code>axis([0.5 1 -1 1])</code>横轴0.5<del>1 纵轴-1</del>1<br><code>clf</code> 清除一幅图像<br><code>imagesc(A)</code>可视化矩阵<br><code>colorbar</code> 添加颜色条<br><code>colormap gray</code>  生成黑白图像<br><code>,</code>依次执行每一个命令  </p>]]></content>
    
    
    <categories>
      
      <category>2. Octave语言初步</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2.1. 基本命令</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.1.%20%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.1.%20%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h1 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h1><h2 id="基本运算"><a href="#基本运算" class="headerlink" title="基本运算"></a>基本运算</h2><p>代数运算：+ - * / sqrt（）<br>布尔运算：且：&amp;&amp;  或：||  非：！<br>赋值：=  </p><h2 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h2><p><code>disp()</code> 显示（）内的命令到屏幕<br><code>sprintf()</code>用法同c语言中的printf<br><code>format lone</code> 显示变量的更多小数位数<br><code>formate short</code> 显示变量的更少小数位数（4位）<br><code>help fuction</code> 显示function 函数的帮助文档  </p><h2 id="矩阵的快速操作"><a href="#矩阵的快速操作" class="headerlink" title="矩阵的快速操作"></a>矩阵的快速操作</h2><p><code>[a b; c d;]</code> 2x2矩阵<br>[]矩阵符号<br>；换行<br>快速建立步长相等的行向量：  起始参数：步长（默认为1）：终止参数<br><code>ones(a,b)</code> 快速生成axb的矩阵，且所有元素为1<br><code>zeros(a,b)</code>快速生成axb的矩阵，且所有元素为0<br><code>rand(a,b)</code>快速生成axb的矩阵，且所有元素的值为在（0，1）内的随机数<br><code>randn(a,b)</code>快速生成axb的矩阵，且所有元素的值为服从正态分布的随机数<br><code>hist()</code> 快速绘制变量的直方图<br><code>eye(a)</code> 快速生成axa的单位矩阵<br><code>size(row,column)</code>返回矩阵的大小，并将大小存入一个1x2的矩阵中<br><code>length(A)</code>返回向量A的最大维度的值<br><code>rref(A)</code> 求解矩阵A的阶梯型</p><h2 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h2><p><code>pwd</code> 返回Octave当前指向的路径<br><code>cd &#39;path&#39;</code> 使Octave指向path路径<br><code>ls</code> 返回Octave当前指向的路径下所有的文件名称  </p><p><code>Load (&#39;file.dat&#39;)</code>  加载file.dat文件<br>*file.dat 是一个编写好的仅有数据（用固定格式aaa bbb ccc）的文件<br><code>Who</code> 返回Octave当前内存中所有的变量<br><code>Whos</code> 返回Octave当前内存中所有的变量和对应的维度、数据类型、数据大小<br><code>Clear varible</code> 清除varible变量<br><code>Clear</code> 清除内存中所有的变量<br><code>Varible1=varible2(a:b)</code>将varible2中的a到b位数据赋给varible1<br><code>Save file.mat varible</code> 将varible存入file.mat中<br><code>Save file.txt varible ascii</code> 将varible存入file.txt中 编码为ascii  </p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p><code>varible(a,b)</code>定位到varible中的（a，b）变量<br><code>C=[A B]</code> 生成[A B]矩阵（B在A右边）<br><code>C=[A;B]</code> 生成[A B]矩阵（B在A下边）  </p>]]></content>
    
    
    <categories>
      
      <category>2. Octave语言初步</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.5. 多项式拟合和正规方程</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.5.%20%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.5.%20%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="多项式拟合和正规方程"><a href="#多项式拟合和正规方程" class="headerlink" title="多项式拟合和正规方程"></a>多项式拟合和正规方程</h1><h2 id="特征点的创建和合并"><a href="#特征点的创建和合并" class="headerlink" title="特征点的创建和合并"></a>特征点的创建和合并</h2><p>对于一个特定的问题，可以产生不同的特征点，<strong>通过对问题参数的重新定义和对原有特征点的数学处理合并拆分，能够得到更加优秀的特征点。</strong></p><h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>对于更多更加常见的数学模型，其拟合往往是非线性关系的，这时候就需要考虑引用多项式来进行拟合，如：$h(x)=θ_0+θ_1 x+θ_2 x^2+θ_3 x^3$  </p><h2 id="正规方程算法"><a href="#正规方程算法" class="headerlink" title="正规方程算法"></a>正规方程算法</h2><p>在微积分中，对于函数$f(x,y)$，其局部最值往往是在$f_x=0$ 且$f_y=0$处取得。<br>因此，对于代价函数$J(θ)$，求$J(θ)$对每一个$θ_i$的偏导数，令它们都为0，即：<br>$$\frac{∂J(θ)}{∂θ_i}=0<del>for</del>i=0,1,2,…,n$$<br>称为<strong>正规方程</strong>(Regular expression)。正规方程提供了一种直接求出最小值的方法，而不需要依赖迭代进行一步一步地运算。 </p><h3 id="正规方程的矩阵形式"><a href="#正规方程的矩阵形式" class="headerlink" title="正规方程的矩阵形式"></a>正规方程的矩阵形式</h3><p>对于数据集${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$,  其中每一个$x^{(i)}$都是一个向量：$x^{(i)}=\begin{bmatrix}x_0^{(i)}\x_1^{(i)}\…\x_n^{(i)}\end{bmatrix}$<br>构建<strong>设计矩阵</strong>（Design matrix）$X=\begin{bmatrix}(x^{(1)})^T\(x^{(2})^T\…\(x^{(m)})^T\end{bmatrix}$和值向量$y=\begin{bmatrix}  y^{(1)}\y^{(2)}\…\y^{(m)}  \end{bmatrix}$<br>将代价函数转化为矩阵方程的形式，再对其求导，令其等于0，得到代价函数取得最小值时的$θ$：<br>$$θ=(X^TX)^{-1}X^Ty$$<br>对比梯度下降算法：<br>正规方程算法不需要学习率和迭代，但<strong>对大规模数量（万数量级以上）的特征点（n），工作效率十分低下</strong>。对于一些如分类算法等等更加复杂的算法，正规方程法并不适用于求它们在极值处的θ值。  </p><h3 id="正规方程的不可逆性"><a href="#正规方程的不可逆性" class="headerlink" title="正规方程的不可逆性"></a>正规方程的不可逆性</h3><p>在使用正规方程时，要注意的问题是，<strong>如果设计矩阵X不可逆（为奇异矩阵），正规方程会无法使用。</strong>  </p><p>设计矩阵为奇异矩阵的常见情况：</p><ol><li>x-I 不满足线性关系  </li><li>正在运行的学习算法中，特征点的数量大于样本点的数量（使得$m≤n$）  </li></ol><p>当设计矩阵X不可逆时，应当尝试删除一些特征点，或者考虑正规化（Regularation）。<br>但是总体而言，<strong>矩阵X不可逆的情况是极少数的。</strong></p>]]></content>
    
    
    <categories>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.4. 调试方法</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.4.%20%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.4.%20%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="调试方法"><a href="#调试方法" class="headerlink" title="调试方法"></a>调试方法</h1><h2 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h2><p>对于某些不具有比较性的样本特征$x_i$ （比如对其他的x来说$x_i$ 相当大或者相当小），梯度下降的过程可能会非常漫长，并且可能来回波动才能最后收敛到全局的最小值。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144432.png"><br>在这样的情况下，可以对$x_i$ 进行缩放（如 $x_i≔αx_i$  或者 $x_i=x_i/α$），使得$x_i$ 与其他的$x$具有可比性，以增加梯度下降的效率。<br><strong>通常将$x$缩放至⟦-1,1⟧</strong>的区间内。（只表示一个大致的范围，这不是绝对的。）</p><h2 id="均值归一"><a href="#均值归一" class="headerlink" title="均值归一"></a>均值归一</h2><p>将$x_i$  替换为$x_i−μ_i$ 使得特征值具有为0的平均值（对$x_0$ 不适用）<br>$$x_i:=(x_i−μ_i)/s_i$$<br>定义$μ_i$  为训练集$X$ 的平均值，$s_i=|x_imax−x_imin |$, 表示$x_i$ 的取值范围（近似值），或者直接设置为$s_i$ 的标准差。</p><h2 id="学习率-Learning-rate"><a href="#学习率-Learning-rate" class="headerlink" title="学习率(Learning rate)"></a>学习率(Learning rate)</h2><p>梯度下降调试的方法：</p><ol><li><p>绘制$minJ(θ)-batch$的图像<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144740.png"><br>原则：每一个batch之后 θ 的值都应该减小，这样的图像能够通过直观地表现变化率来表现梯度下降是否收敛（变化率为0）。  </p></li><li><p>自动收敛测试<br>如果$J(θ)$在某一次迭代之后的下降值小于某个值$ε$后，就能够判断算法已经达到了收敛。<br>$ε$的值比较难取，所以通常采取1.中的方法进行观测。</p></li></ol><p>常见的α过大的$minJ(θ)-batch$的图像：<br>α过大,导致代价函数无法收敛<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144816.png">   </p><p>α过小，导致代价函数收敛速度过慢</p>]]></content>
    
    
    <categories>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.3. 多变量预测</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.3.%20%E5%A4%9A%E5%8F%98%E9%87%8F%E9%A2%84%E6%B5%8B/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.3.%20%E5%A4%9A%E5%8F%98%E9%87%8F%E9%A2%84%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 30%;    padding-left: 20%;}</style><h1 id="多变量预测"><a href="#多变量预测" class="headerlink" title="多变量预测"></a>多变量预测</h1><h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><p>对于多个特征量(Features)，规定符号表示：<br>$n$ 特征的总数量<br>$x^{(i)}$  第i个训练样本的输入特征向量，$i$表示的是一个索引(Index)<br>$x_j^i$  第i个训练样本中特征向量的第j个值  </p><p>此时的假设函数不再是单纯的 $h_θ (x)=θ_0+θ_1 x$ 的形式。<br>对于多个特征量，此时的假设函数为：<br>$$<br>h_θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}<br>$$<br>对这个样本进行简化：<br>定义$x_0^i=1$, 定义参数向量：$x=\left[\begin{smallmatrix} x_0 \\ x_1 \\ … \\ x_n \end{smallmatrix}\right]n$，系数向量：$θ=\left[\begin{smallmatrix}θ_0 \\ θ_1 \\ … \\ θ_n \end{smallmatrix}\right]$<br>有：<br>$$<br>h_θ (x)=θ^T x<br>$$<br>这就是假设函数的向量形式。   </p><h2 id="梯度下降算法在多元线性回归中的应用"><a href="#梯度下降算法在多元线性回归中的应用" class="headerlink" title="梯度下降算法在多元线性回归中的应用"></a>梯度下降算法在多元线性回归中的应用</h2><p>对于假设函数：<br>$$<br>\begin{aligned}<br>h_θ(x) \\<br>&amp; =  θ^T x \\<br>&amp; =θ<em>0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)} \\<br>\end{aligned}<br>$$<br>和损失函数：<br>$$<br>J(θ_0,θ_1,…,θ_n)=\frac{1}{2m} ∑</em>{i=1}^m(h_θ (x^{(i)} )−y^{(i)} )^2<br>$$<br>此时的梯度下降算法：<br>Repeat{<br>$$<br>θ_j≔θ<em>j−α\frac{∂J(θ)}{∂θ_j}<br>$$<br>}<br>对$\frac{∂J(θ)}{∂θ_j}$进行等价变形：<br>Repeat{<br>$$<br>θ_j≔θ_j−α\frac{1}{m}∑</em>{i=1}^m(h_θ (x^{(i)} )−y^{(i)})  x_j^i<br>$$<br>}</p>]]></content>
    
    
    <categories>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.2. 梯度下降算法</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.2.%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.2.%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 40%;    padding-left: 20%;}</style><h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><p>在开始之前，为了方便解释，首先规定几个符号所代表的意义：<br>$m$ 训练集中训练样本的数量<br>$X$  输入变量<br>$Y$  输出变量<br>$(x,y)$ 训练样本<br>$(x^i,y^i)$第i个训练样本（i表示一个索引）  </p><h2 id="监督学习算法的流程"><a href="#监督学习算法的流程" class="headerlink" title="监督学习算法的流程"></a>监督学习算法的流程</h2><p>提供训练集&gt;学习算法得到$h$（假设函数：用于描绘x与y的关系）&gt;预测y 的值  </p><h2 id="代价-损失函数（Cost-function）"><a href="#代价-损失函数（Cost-function）" class="headerlink" title="代价/损失函数（Cost function）"></a>代价/损失函数（Cost function）</h2><p><strong>假设函数(Hypothesis function)**——$h$是用来表示某一个数据集可能存在的线性/非线性关系的函数。对于线性拟合，其假设函数为：<br>$$h_θ(x)=θ_1x+θ_0$$<br>这其中的$θ$是假设函数当中的参数。<br>也可以简化为：<br>$$h_θ(x)=θ_1x$$<br>**代价函数</strong>，在统计学上称为均方根误差函数。当假设函数中的系数$θ$取不同的值时，$\frac{1}{2m}$倍假设函数预测值$h_θ(x^{(i)})$和真实值$y^{(i)}$的差的平方的和之间的函数关系表示为代价函数$J$。<br>$$<br>\begin{aligned}<br>J(θ_0,θ<em>1)= \\<br>&amp; \frac{1}{2m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2 \\<br>\end{aligned}<br>$$ </p><blockquote><p>在这里取1/2的原因是便于消除求导之后产生的2倍,同时也可以进一步缩小$θ$  </p></blockquote><blockquote><p>该函数的自变量是$θ_1$和$θ_0$，因此该函数是三维的函数（如图所示）。<br> <img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131130651.png">  </p></blockquote><p><strong>代价函数在几何上表示为数据集空间内的各点到假设函数的距离的平方的平均值的一半。</strong><br>要想使得数据能够被假设函数很好地拟合，那么代价函数要尽量地小。<strong>当代价函数取到它的最小值即</strong>$J(θ<em>1)</em>{min}$<strong>时，此时的填入假设函数的</strong>$θ$<strong>对数据的拟合程度是最好的</strong><br>对于线性的代价函数，假设函数对数据集的拟合程度越高，对应的$(θ_0,θ_1)$越接近代价函数图像等高线的中心。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131132852.png">   </p><h2 id="梯度下降算法（Gradient-Descent）"><a href="#梯度下降算法（Gradient-Descent）" class="headerlink" title="梯度下降算法（Gradient Descent）"></a>梯度下降算法（Gradient Descent）</h2><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>在微积分中，函数$f(x,y)$在$(x_0,y_0)$处是函数值增加最快的方向是<strong>梯度（Gradient）</strong>的方向，<strong>梯度的反方向是函数值减小最快的方向。</strong><br>梯度的计算公式：<br>$$▿f|_{(x_0,y_0)}=(f_x(x_0,y_0),f_y(x_0,y_0))$$</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>梯度下降算法是一种求解代价函数最小值的方法，它可以用在<strong>多维任意的假设函数</strong>当中。<br>简而言之，梯度下降算法求得$J(θ<em>1)</em>{min}$的主要思路是：   </p><ol><li>给定$θ_0$和$θ_1$的初始值，通常令$θ_0=0$，$θ_1=0$。</li><li>不断改变$θ_0$和$θ_1$的值使得$J(θ_0,θ_1)$的值逐渐变小，直到找到$J(θ_0,θ_1)$的最小值或者局部最小值。<br>如果从一个初始值出发，寻找附近的最小值，重复该过程，得到上图，最后得到的值为局部最优解。  <blockquote><p>将梯度下降算法类比为爬山，从一个点开始，不断寻找“下山”的路线，最后找到一个“下山”的出口。——吴恩达  </p></blockquote></li></ol><p>当改变初始值时，会找到另一条“下山”的路径，找到第二个局部最优解（局部最小值）。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131134419.png"><br>对于线性回归的代价函数而言，只存在一个局部最小值。（见代价函数的图像）  </p><h3 id="表示"><a href="#表示" class="headerlink" title="表示"></a>表示</h3><p>梯度下降算法可以表示为：<br>Repeat untill convergence{<br>$$<br>θ_j:=θ_j-α\frac{∂J(θ_0,θ_1)}{∂θ_j},j=0<del>and</del>j=1<br>$$<br>}<br>解释：    </p><ol><li>:=  表示赋值运算符</li><li>α称为<strong>学习率</strong>，用来控制下降的<strong>步长</strong>（Padding），即更新的幅度：  <ul><li>α太小，同步更新的速率会非常的慢     </li><li>α过大，同步更新时可能会越过最小值点   </li></ul></li><li>$\frac{∂J(θ<em>0,θ<em>1)}{∂θ_j}$是代价函数的梯度：<br>$$<br>\frac{∂J(θ_0,θ_1)}{∂θ_0}=\frac{1}{m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})<br>$$<br>$$<br>\frac{∂J(θ_0,θ_1)}{∂θ_1}=\frac{1}{m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}<br>$$<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131135144.png"><br>△在代价函数中（以简化的代价函数为例），无论初始值在最小值点的左侧还是右侧，通过同步更新都能够使该点被“移动（Update）”到最小值，在最小值点，由于导数值为0，最终同步更新停止在了$θ_j=θ_j$，如前面所说，$θ_j$即为极小值点。  </li></ol><h3 id="同步更新"><a href="#同步更新" class="headerlink" title="同步更新"></a>同步更新</h3><p><strong>同步更新</strong>（Simulaneous update）是实现梯度下降算法的最有效方式。<br>$$<br>temp0:θ_0:=θ_0-α\frac{∂J(θ_0,θ_1)}{∂θ_0}<br>$$<br>$$<br>temp1:θ_1:=θ_1-α\frac{∂J(θ_0,θ_1)}{∂θ_1}<br>$$<br>$$<br>θ_0:=temp0<br>$$<br>$$<br>θ_1:=temp1<br>$$<br>这个更新方程能够同时更新$θ_0$和$θ_1$：$θ_0$更新为$θ_0-J’(θ)$，对$θ_1$同理。<br>更新的方法是计算赋值号右边带入$θ_1$和$θ_2$的值进行计算，得到的两个值分别储存在temp0和temp1中，从上到下进行赋值。  </p><p>对于简化的代价函数：<br>$$θ_1：=θ_1-αJ’(θ_1)$$<br>$$\frac{dJ(θ_1,θ_0)}{dθ_j} =d(\frac{1}{2m}Σ(h_θ(x_i)-y_i))^2)$$   </p><p>将梯度代回代价函数中就得到了<strong>Batch梯度下降法</strong>的基本形式：<br>Repeat untill convergence{<br>$$<br>θ_0:=θ<em>0-α\frac{1}{m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})<br>$$<br>$$<br>θ_1:=θ<em>1-α\frac{1}{m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$<br>}    </p>]]></content>
    
    
    <categories>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.1. 什么是机器学习</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.1.%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.1.%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h1><h2 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><blockquote><p>A computer  program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.    ——Tom Mitchelle  </p></blockquote><p>简言之，机器学习通过完成任务（T）得到经验（E），进而提升性能（P）。<br>例如：一个自我对弈的跳棋学习机器：E ：自我对弈的棋局 T：下跳棋 P：与新对手玩跳棋时的获胜概率  </p><h2 id="机器学习的主要算法类型"><a href="#机器学习的主要算法类型" class="headerlink" title="机器学习的主要算法类型"></a>机器学习的主要算法类型</h2><ul><li><strong>监督学习</strong>（Supervised）<br>人教会计算机完成任务。<br>根据统计数据做直线或曲线拟合/分离数据，来预测结果。<br>其中包括了两大问题：  <ul><li><strong>回归</strong>（Regression）<br>给算法做一个数据集，包含正确答案，（比如房价-年），用线性/非线性回归方程拟合数据,预测数据。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124639.png"></li><li><strong>分类问题</strong>（<strong>逻辑回归</strong>问题）（Classification/Logical regression）<br>用实数对出现的可能状况分类<br>（比如：1和0表示患乳腺癌/不患乳腺癌 ；1表示患乳腺癌A，2表示患乳腺癌B，0表示不患乳腺癌），在多维坐标系中（每一个维度表示不同的属性），然后用线性或非线性的函数将不同类的数据分开。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124726.png"></li></ul></li><li><strong>无监督学习</strong>（Unsupervised）<br>计算机自己学习，经典的算法分为两大类：    <ul><li><strong>聚类算法</strong><br>对并不明确分类的数据集，计算机根据数据特征自动将数据分为几个簇<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124803.png"></li><li><strong>鸡尾酒会算法</strong>（Cocktail party）<br>略，这里只对鸡尾酒会问题和解决方法作一个概述：<br>鸡尾酒会问题是在计算机语音识别 领域的一个问题。<br>当前语音识别技术已经可以以较高精度识别一个人所讲的话，但是当说话的人数为两人或者多人时，语音识别率就会极大的降低，这一难题被称为鸡尾酒会问题。<br>对于的给定混合信号，分离出鸡尾酒会中 同时说话的每个人的独立信号。<br>鸡尾酒问题的解决方法是把两个收音器分别放在两个人的附近，每个收音器且与两个人的距离是不等距的，如此来分离两个人的声音。</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>5.2. 神经网络的优化</title>
    <link href="/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/"/>
    <url>/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="算法优化"><a href="#算法优化" class="headerlink" title="算法优化"></a>算法优化</h1><h2 id="参数展开"><a href="#参数展开" class="headerlink" title="参数展开"></a>参数展开</h2><p>参数展开是一种将矩阵展开为向量的方法，常用于很多高级优化中。<br>例如如下的高级优化：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp">function[jVal,gradient]=costFunction(theta)<br>...<br>optTheta=fminunc(@costFunction,initialTheta,options)<br></code></pre></td></tr></table></figure><p>fminuc是一种高级的优化算法。这些高级优化算法的输入值的形式都是参数向量。<br>在神经网络中，很多参数并非是向量的形式，而是完整的矩阵，比如第l层的参数矩阵$Θ^{(l)}$和梯度矩阵$D^{(l)}$(见5.1.)，这时就需要应用参数展开将这些矩阵展开为向量，方法是在Octave中应用<code>[;]</code>表达将所有的元素从矩阵中取出，展开成一个长向量，并应用<code>reshape</code>语法重新合成矩阵。<br>比如如下的10层神经网络：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210204151039.png">   </p><h2 id="梯度检测"><a href="#梯度检测" class="headerlink" title="梯度检测"></a>梯度检测</h2><p>反向传播算法的实现过程非常的繁琐，因此在与其他算法一同工作的时候可能会产生一些bug，这些bug可能本身不会影响程序的运行，但是最终输出的模型准确度可能会非常低。 因此需要引入梯度检测(Gredient Check)来解决反向传播算法或类梯度下降算法中出现的这类问题。  </p><ul><li><p>从数值上近似梯度<br>要想求出代价函数$J(\Theta)$在某一点$\theta$的梯度（在二维内反映为该点的斜率），可以在$\theta$的两边取$\theta \plusmn \epsilon, \epsilon \rightarrow 0$,$\theta$处的梯度可以近似的表示为（实数形式）：<br>$$\frac{dJ(θ )}{dθ }≈  \frac{J(θ +ε )-J(θ -ε )}{2ε }$$<br>称为双侧差分（Two-side difference）。<br>当$\theta$是$\Theta^{(i)}$的展开时，可以用双侧差分来估计所有的偏导数项：<br>$$\frac{∂ J(θ)}{∂ θ_k}≈ \frac{J(θ _k+ε ,θ_1,…,θ_n )-J(θ _k-ε ,\theta_1,…,θ_n  )}{2ε }$$<br>在Octave中用如下的代码实现：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Cpp"><span class="hljs-keyword">for</span> i=<span class="hljs-number">1</span>:n,<br>  thetaPlus=theta;<br>  thetaPlus=thetaPlus(i)+epsilon;<br>  thetaMinus=theta;<br>  thetaMinus(i)=thetaMinus(i)-epsilon;<br>  gradApprox(i)=(J(thetaPlus)-J(thetaMinus))/(<span class="hljs-number">2</span>*epsilon);<br>end;<br>Check gradApprox≈DVec<br></code></pre></td></tr></table></figure></li><li><p>总结流程</p><ol><li>利用反向传播算法算出$D^{(i)}$的展开向量DVec</li><li>利用双侧差分计算gradApprox</li><li>DVec和gradApprox作比较    </li><li>关闭双侧差分，利用反向传播进行训练（以提高训练时的算法效率）</li></ol><h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>在最开始执行高级优化或者是神经网络的梯度下降时，应当对$Θ$设置一些初始值，即初始化$Θ$。<br>在逻辑回归中，将参数全部初始化为0的做法会导致神经网络中一个单元出发的所有的参数都相等，导致神经网络中所有的隐藏单元都在计算相同的特征。正确的做法是对$Θ$随机地设定一些值来初始化它，具体的做法是：<br>设置某个区间$[-ϵ,ϵ]$，使得所有$θ$都在这个区间内随机取到，用Octave代码实现：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp">theta1 = rand(<span class="hljs-number">10</span>,<span class="hljs-number">11</span>)*(<span class="hljs-number">2</span>*init_epsilon)-init_epsilon; <br><span class="hljs-meta">#rand()的作用是随机生成一个mxn的矩阵，矩阵里面所有的元素值都介于0,1之间。</span><br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>5. 神经网络拟合</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>5.1. 神经网络的代价函数</title>
    <link href="/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0/"/>
    <url>/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络的代价函数"><a href="#神经网络的代价函数" class="headerlink" title="神经网络的代价函数"></a>神经网络的代价函数</h1><h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p>接下来的讲义主要考虑两种分类问题：第一种是二元分类，如之前的讲义所述，y的取值只能是0或者1，输出层只有一个输出单元，假设函数的输出值是一个实数；第二种是多元分类，y的取值是一个k维的向量，输出层有k个输出单元。</p><h2 id="神经网络的代价函数形式"><a href="#神经网络的代价函数形式" class="headerlink" title="神经网络的代价函数形式"></a>神经网络的代价函数形式</h2><p>假设一个神经网络训练集有m个训练样本：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$<br>$L$表示神经网络的总层数，$s_l$表示$l$层中神经元的数量（不包括偏置神经元）。<br>在神经网络中使用的代价函数是在逻辑回归中使用的正则化代价函数：<br>$$J(θ)=-\frac{1}{m}[∑<em>{i=1}^m y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))]+\frac{λ}{2m}∑</em>{j=1}^n θ<em>j^2$$<br>略微不同的是，在神经网络中分类标签和假设函数的输出值都变成了k维的向量，因此神经网络中的代价函数变成了：<br>$$J(θ)=-\frac{1}{m}[∑</em>{i=1}^m ∑<em>{k=1}^Ky_k^{(i)} log⁡(h_θ(x^{(i)} )_k)+(1−y_k^{(i)}) log(1−h_θ (x^{(i)})_k)]+\frac{λ}{2m}∑</em>{l=1}^{L-1}∑_{j=1}^{s_l}∑_{j=1}^{s_l+1} (Θ_{ji}^{(l)})_j^2$$<br>解释：  </p><ol><li>用$(h_Θ(x))_i$来表示第i个输出  </li><li>这个代价函数中$∑_{k=1}^K$表示所有的输出单元之和，这里主要是将$y_k$的值与$(h_Θ(x))_k$的大小作比较   </li><li>正则项的作用是去除那些对应于偏置单元的项，具体而言就是不对$i=0$的项进行求和和正则化。  </li></ol><h2 id="代价函数最小化：反向传播算法"><a href="#代价函数最小化：反向传播算法" class="headerlink" title="代价函数最小化：反向传播算法"></a>代价函数最小化：反向传播算法</h2><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>同之前的线性回归和逻辑回归一样，接下来要求得代价函数的最小值$J(Θ)min$并求出$Θ$。主要的步骤是写出$J(Θ)$并求关于每一个$Θ<em>{ij}^{(l)}$的偏导项$\frac{∂}{∂Θ</em>{ij}^{(l)}}J(Θ)$。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210201185117.png"><br>现在先来讨论如上图所示的神经网络中，只有一个训练样本$(x,y)$的情况：<br>首先先用前向传播算法（见讲义4.2）验证假设函数是否会真的输出结果:<br>$$a^{(1)}=x$$<br>$$z^{(2)}=Θ^{(1)}a^{(1)},并增加一个偏置单元$$<br>$$a^{(2)}=g(z^{2})$$<br>$$z^{(3)}=Θ^{(2)}a^{(2)},并增加一个偏置单元$$<br>$$a^{(3)}=g(z^{3})$$<br>$$z^{(4)}=Θ^{(3)}a^{(3)}$$<br>$$a^{(4)}=g(z^{4})=h_Θ(x)$$<br>接下来，为了计算关于每一个$Θ<em>{ij}^{(l)}$的偏导项$\frac{∂}{∂Θ</em>{ij}^{(l)}}J(Θ)$，就要用到<strong>反向传播算法</strong>（Backpropagation）。<br>从直观上说，对于每一个节点，都要计算每个节点的误差：$δ^{(l)}<em>j$,表示第l层第j个节点的误差。<br>$$δ^{(l)}_j=a_j^{(l)}-y_j=(h_Θ(x))_j-y_j$$<br>y表示训练集中y向量里的第j个元素的值。<br>其向量形式：<br>$$δ^{(l)}=a^{(l)}-y$$<br>这里的$δ^{(l)}$和$a^{(l)}$都是一层每一个误差/输出所构成的向量。<br>具体而言，对于上图所示的4层（$L=4$）神经网络,第四层的误差项：<br>$$δ^{(4)}=a^{(4)}-y$$<br>照例写出前面两层的误差：<br>$$\delta^{(3)}=(Θ^{(3)})^Tδ^{(4)}⋅g’(z^{(3)})$$<br>$$\delta^{(2)}=(Θ^{(2)})^Tδ^{(3)}⋅g’(z^{(2)})$$<br>事实上应用微积分的链式法则，$g’(z^{(3)})=a^{(3)}⋅(1-a^{(3)})$,1是一个每项都为1的向量。<br>反向传播的步骤相当于是从最后一层开始求误差，然后将最后一层的误差传给前一层，反向依次传播。<br>最终将会有：<br>$$\frac{∂}{∂Θ</em>{ij}^{(l)}}J(Θ)=a^{(l)}_iδ^{(l+1)}_i$$<br>此处忽略了正则化项：$λ$。<br>现在将反向传播算法从一个训练样本拓展到一个有m个训练样本：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$,$L$层的神经网络训练集：      </p><p>定义$Δ<em>{ij}^{(l)}=0$用于计算$\frac{∂}{∂Θ</em>{ij}^{(l)}}J(Θ)$,接下来遍历整个训练集：<br>For $i=1$ to $m$:<br>  set $a^{(1)}=x^{(i)}$ #用于将所有的x输入到输入层的激活函数中<br>  用正向传播算法计算$a^{(l)}<del>for</del>l=2,3,…,L$<br>  $δ^{(L)}=a^{(L)}-y^{i}$ #计算最后一层的误差<br>  用反向传播算法计算$\delta^{(L-1)}$到$δ^{(2)}$,<br>  $Δ^{(l)}<em>{ij}:=Δ^{(l)}</em>{ij}+a_j^{(l)}δ^{(l+1)}<em>i$<br>  (写成向量的形式：$Δ^{(l)}:=Δ^{(l)}+δ^{(l+1)}(a_j^{(l)})^T$)<br>结束循环后，令<br>$$D^{(l)}_{ij}:=\begin{dcases}<br>    \frac{1}{m}Δ^{(l)}</em>{ij},j=0\ \<br>     \frac{1}{m}Δ^{(l)}<em>{ij}+λΘ^{(l)}</em>{ij},j \not=0<br>\end{dcases}$$<br>那么最终：<br>$$\frac{∂}{∂Θ<em>{ij}^{(l)}}J(Θ)=D^{(l)}</em>{ij}$$</p><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><ul><li><p>回顾：前向传播模型<br>前向传播的整个过程可以用下图表示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210204143238.png"><br>比如：如果洋红色的部分其权重为$Θ<em>{10}^{(2)}$,红色的权重值为$Θ</em>{11}^{(2)}$，青色的权重值是$Θ<em>{12}^{(2)}$， 那么$z_1^{(3)}=Θ_{10}^{(2)} \times 1+Θ</em>{11}^{(2)} × a_1^{(2)}+Θ_{12}^{(2)}×a_1^{(2)}$。<br>反向传播的过程和前向传播非常类似，只是传播的方向不同。  </p></li><li><p>反向传播的理解<br>关注反向传播的代价函数：<br>$$J(θ)=-\frac{1}{m}[∑<em>{i=1}^m ∑</em>{k=1}^Ky_k^{(i)} log⁡(h_θ(x^{(i)} )<em>k)+(1−y_k^{(i)}) log(1−h_θ (x^{(i)})<em>k)]+\frac{λ}{2m}∑</em>{l=1}^{L-1}∑</em>{j=1}^{s_l}∑_{j=1}^{s_l+1} (Θ_{ji}^{(l)})_j^2$$<br>对于单个的样本:$(x^{(i)},y^{(i)})$，只有一个输出单元并且忽略正则化，那么这个样本的代价函数：<br>$$Cost(i)=y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))$$<br>这个代价函数的功能类似于计算方差，可以近似的看做是方差函数：<br>$$Cost(i)≈(h_\Theta(x^{(i)})-y^{(i)})^2$$<br>它反应了样本模型输出值和样本值的接近程度。<br>反向传播中每个节点的误差：$δ^{(l)}_j$,表示第l层第j个节点的误差。有：<br>$$δ^{(l)}_j=\frac{\partial}{∂z_j^{(l)}}Cost(i)$$<br>$z_j^{(l)}$与$h_\Theta(x^{(i)})$相关。  </p><p>反向传播的整个过程可以用下图表示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210204145244.png"><br>例如对$δ^{(2)}<em>2$，洋红色和红色箭头分别表示两个权重值$Θ</em>{12}^{(2)}$和$Θ_{22}^{(2)}$，有<br>$$δ^{(2)}<em>2=Θ</em>{12}^{(2)} ×δ^{(3)}<em>1 +Θ</em>{22}^{(2)} ×δ^{(3)}_2$$</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>5. 神经网络拟合</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
