<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.4%20%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.4%20%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>一对多分类的实质是对一对一分类的拓展。<br>基本方法是建立一个多输出的神经网络，因此<strong>在多元分类中，最终的输出结果将是一个n维的向量</strong>，输出层的每一个输出单元用于判断是否是某一类（例如：是否是行人，是否是自行车），当判断为结果是某一类时，在理想情况下，这个网络会在这一输出单元输出1，其他的输出单元输出0，最终输出的结果是如：$h_Θ(x)≈\begin{bmatrix}    1\0\0\0 \end{bmatrix}$之类的向量。<br>以前我们在训练集中用一个整数y来表示分类的标签，在多元分类中，我们使用如上所示的向量来表示分类的标签。<br>现在假设函数的模型应该为：<br>$$h_Θ(x^{(i)})≈y^{(i)}$$<br>等式的左右两边输出的结果都是n维的向量。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.3%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.3%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 50%;    padding-left: 20%;}</style><h1 id="神经网络与逻辑函数"><a href="#神经网络与逻辑函数" class="headerlink" title="神经网络与逻辑函数"></a>神经网络与逻辑函数</h1><p>思考下面一个例子：<br>如果$x_1,x_2$都为二进制数，如图有四个样本分布在样本空间内，<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101151549.png"><br>那么假设函数可以写成：<br>$$y=x_1 XNOR x_2$$<br>那么神经网络是否可以生成这样的函数呢？ </p><h2 id="简单逻辑函数的实现——AND-OR-NOT"><a href="#简单逻辑函数的实现——AND-OR-NOT" class="headerlink" title="简单逻辑函数的实现——AND,OR,NOT"></a>简单逻辑函数的实现——AND,OR,NOT</h2><p>为了解决这个问题，我们从AND函数入手：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101151722.png"><br>如图，输入层有两个特征$x_1$和$x_2$，他们是二进制数。目标函数为$y=x_1 AND x_2$.  </p><p><strong>观察激活函数$y=g(x)$,我们发现当$x=4$时，$y=0.99$,当$x=-4$时，$y=0.01$</strong><br>由上述激活函数的性质，为了计算这样的神经网络，首先先增加一个偏置单元 【+1】，对每个单元赋予权重：-30，20,20<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101152043.png"><br>列出真值表：<br>| $x_1$ | $x_2$ | $h_θ(x)$|<br>| —- | —- | —-|<br>|0|0|$g(-30)≈0$|<br>|0|1|$g(-10)≈0$|<br>|1|0|$g(-10)≈0$|<br>|1|1|$g(10)≈1$|</p><p>观察最后一列，我们发现最后一列的输出事实上很接近与AND函数的结果，那么可以认为$h_θ(x) ≈ x_1 AND x_2$</p><p>那么同理，下图的神经网络最终可以生成一个类似于OR函数的假设函数。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101152850.png">  </p><p>下图的神经网络最终可以生成一个类似于NOT函数的假设函数。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101153145.png"><br>可以发现NOT是通过给对应的单元施加一个较大的负数来实现的。</p><h2 id="复杂逻辑函数的实现"><a href="#复杂逻辑函数的实现" class="headerlink" title="复杂逻辑函数的实现"></a>复杂逻辑函数的实现</h2><p>下面我们来试试生成如下的函数：<br>$$h_θ(x)=(NOT x_1)AND(NOT x_2)$$<br>分析：<br>要想使$h_θ(x)=1$,那么当且仅当$x_1=x_2=0$时成立，最终的神经网络如下图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101154246.png">   </p><p>现在我们可以应付本节一开头的问题了——如何使神经网络生成$y=x_1 XNOR x_2$？<br>分析：<br>$$x_1XNORx_2=NOT(X_1 XOR X_2)=(x_1 AND x_2)OR((NOT x_1) AND (NOT x_2))$$<br>以逻辑表达式形式书写：<br>$$h_θ(x)=(x_1.x_2)+\overline{x_1}.\overline{x_2}$$<br>将它分层：<br>第一层： 获取$x_1$和$x_2$<br>第二层：计算  $a_1^{(2)}=x_1.x_2$  和  $a_2^{(2)}=\overline{x_1}.\overline{x_2}$.<br>第三层：计算  $a_1^{(2)}+a_2^{(2)}$.<br>通过这一节的前半部分，我们已经知道了每一层所需要的函数的神经网络构建方法，最终的神经网络将是：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101155720.png">   </p><blockquote><p>实例： 可视化Minst手写字符数据集识别</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 60%;    padding-left: 20%;}</style><h1 id="神经网络的基本模型"><a href="#神经网络的基本模型" class="headerlink" title="神经网络的基本模型"></a>神经网络的基本模型</h1><h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><ul><li>假设： 大脑对于不同功能（听觉，视觉，触觉的处理）的实现是依赖于同样的学习方法  </li><li>依据： 神经重接实验  </li><li>神经元模型<br>神经网络模拟了大脑中的神经元或者是神经网络。先来看大脑中的神经元构成，我们会发现神经元有很多的输入通道（树突），同时通过轴突给其他的神经元传递信号。  将神经元简单抽象：一个计算单元，它从输入端接收一定数目的信息，并作一些处理，并将结果传递给其他的神经元。在计算机中，我们构建一个逻辑单元，它从输入端接收数据集X，并作处理来生成一个激活函数$h_θ (x)=\frac{1}{1+e^{-θ^T X}}$<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201229183932.png"><br>在这个模型之上，输入端会额外增加一个$x_0=1$，称为偏置单元。<br>在神经网络中，$Θ$称为模型的权重，$g(z)=\frac{1}{1+e^{-z}}$称为激活函数。  </li></ul><p>神经网络是一组神经元连接在一起的集合，如图所示<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201229184740.png"><br>第一层称为输入层，我们在这一层输入全部的特征，最后一层称为输出层，这一层的神经元输出假设的最终结果，中间的层称为隐藏层，隐藏层可能不止有一层。<br>统一地，$a_i^{(j)}$将表示第j层的第i个激活项（激活指计算并输出结果），同时，第j层到第j+1层之间的映射由参数矩阵$Θ^{(j)}$确定，那么上图就可以用公式表示为：<br>$$a_1^{(2)}=g(Θ<em>{10}^{(1)}x_0+Θ_{11}^{(1)}x_1+Θ_{12}^{(1)}x_2+Θ_{13}^{(1)}x_3)$$<br>$$a_2^{(2)}=g(Θ</em>{20}^{(1)}x_0+Θ<em>{21}^{(1)}x_1+Θ_{22}^{(1)}x_2+Θ_{23}^{(1)}x_3)$$<br>$$a_3^{(2)}=g(Θ</em>{30}^{(1)}x_0+Θ<em>{31}^{(1)}x_1+Θ_{32}^{(1)}x_2+Θ_{33}^{(1)}x_3)$$<br>$$h_{Θ}(x)=g(Θ</em>{10}^{(2)}a_0^{(2)}+Θ_{11}^{(2)}a_1^{(2)}+Θ_{12}^{(2)}a_2^{(2)}+Θ_{13}^{(2)}a_3^{(2)})$$</p><blockquote><p>如果一个网络在第j层有$s_j$个单元，且在第j+1层有$s_j+1$个单元，那么矩阵$Θ^{(j)}$的维度为$s_{j+1} \times (s_j+1)$</p></blockquote><h2 id="神经网络的向量化-前向传输-Forward-propagation"><a href="#神经网络的向量化-前向传输-Forward-propagation" class="headerlink" title="神经网络的向量化:前向传输(Forward propagation)"></a>神经网络的向量化:前向传输(Forward propagation)</h2><p>对如上的等式，现在将$g()$中的线性加权组合以$z^{(2)}_1,z^{(2)}<em>2,z^{(2)}_3$表示，那么就有：<br>$$a_1^{(2)}=g(z_1^{(2)})$$<br>$$a_2^{(2)}=g(z_2^{(2)})$$<br>$$a_3^{(2)}=g(z_3^{(2)})$$<br>现在就能够定义三个向量使得上述等式转化为向量乘法：<br>$x=\begin{bmatrix}x_0 \x_1\x_2\x_3 \end{bmatrix}$,<br>$z^{(2)}=\begin{bmatrix} z_1^{(2)}\z_2^{(2)}\z_3^{(2)}\end{bmatrix}=Θ^{(1)}x$，$a^{(2)}=\begin{bmatrix} a_1^{(2)}\a_2^{(2)}\a_3^{(2)}\end{bmatrix}$<br>那么上述等式最终就可以转化成：<br>$$z^{(2)}=Θ^{(1)}x$$<br>$$a^{(2)}=g(z^{(2)})$$<br>对于隐藏层的偏置单元，增加一项$a_0^{(2)}=1$.<br>最后计算$z^{(3)}=Θ^{(2)}a^{(2)}$,那么最终得到的假设模型将会是：<br>$$h</em>{Θ}(x)=a^{(3)}=g(z^{(3)})$$</p><p>单看layer2 和 layer3，事实上，这两层做的就是逻辑回归，但输入进逻辑回归的特征不再是原始的特征x，而是通过原始特征生成的特征$a$。<br>而$a$与$x$之间的关系通过θ来定义。 因此可以通过改变$θ$来改变输入层和隐藏层之间的关系。</p><blockquote><p>下一章将说明如何调整$θ$的值来优化假设函数。</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%83%8C%E6%99%AF/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%83%8C%E6%99%AF/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 60%;    padding-left: 20%;}</style><h1 id="神经网络的背景知识"><a href="#神经网络的背景知识" class="headerlink" title="神经网络的背景知识"></a>神经网络的背景知识</h1><h2 id="激活函数算法的局限性"><a href="#激活函数算法的局限性" class="headerlink" title="激活函数算法的局限性"></a>激活函数算法的局限性</h2><p>假设一个数据集拥有非常多的原始特征和数据量，执行激活函数算法，那么次方项、交叉项会非常的多，计算量非常的大，最终的拟合结果也不好。<br>计算机视觉中的例子：<br>计算机读取到的是图片所对应的像素强度的矩阵。 </p><blockquote><p>对于灰度图像来说，像素强度就是每一个像素的灰度值。<br>对于RGB彩色图像来说，图片上的一个像素以三个值（R,G,B）/三维向量 来进行表示  </p></blockquote><p>如果现在设计一个分类器，使得计算机能够区分一个图片的主体是否为汽车。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/472E9216086782CF8029F2818CA1027A.png"><br>以图中的pixel1 和 pixel2的位置为例，我们可以把所有数据集中pixel1和pixel2的像素强度投射到坐标轴上，如图使用一个非线性假设来对图像进行分类。<br>如果对于一个50*50像素的图片数据集，那么训练集中将包含至少2500个原始特征（7500 RGB），这时候用激活函数算法计算量会非常的大。  </p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.3.%20%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.3.%20%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h2><p>对于模型，如果一个模型对于数据的偏差很大，不能能够很好的拟合数据的分布，称为欠拟合，或者说这个算法具有高偏差的特性。 如果一个模型虽然可以穿过所有的数据点，但是其图像波动很大，其同样也不能描述数据的分布，（其数据的分布是无法被泛化处理），称为过拟合，或者说这个算法具有高方差的特性。 在这种情况下，模型的参数过于多（有可能代价函数正好为0），以至于可能没有足够多的数据去约束它来获得一个假设函数。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224205854.png"><br>过拟合现象往往会发生在<strong>参数过多，而训练样本过少的情况</strong>。减小过拟合现象的思路有两种： </p><ol><li>尽可能的去掉那些影响因素很小的变量，这种方法虽然解决了过拟合问题，但是损失了精度。  </li><li><strong>正则化</strong>（Regularization）  </li></ol><h2 id="代价函数的正则化"><a href="#代价函数的正则化" class="headerlink" title="代价函数的正则化"></a>代价函数的正则化</h2><p>对于代价函数：<br>$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2$$<br>增加两个惩罚项$1000\theta^2_3$和$1000\theta^2_4$，代价函数变为：<br>$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+1000\theta^2_3+1000\theta^2_4$$<br>如果要最小化这个函数，那么$\theta_3$与$\theta_4$就要尽可能的接近0，那么最后拟合的结果（假设函数）：$\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$，仍然是一个类似的二次函数.<br>正则化的基本思想是<strong>如果所有的参数足够小，那么假设模型就更简单。</strong>  </p><blockquote><p>事实上，如果参数足够小，得到的函数就会越平滑，越简单，越不容易出现过拟合的问题  </p></blockquote><p>在实际上，对于大量的特征和大量的参数，比如$x_1..x_{100}$和$\theta_0…\theta_{100}$，我们无法确定哪些参数是高阶项的参数，这个时候采用的方法就是对代价函数进行修改，使得所有的参数都尽可能的小。<br>修改后的代价函数方程：<br>$$ J_{\theta}=\frac{1}{2m}[\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+λ\Sigma_{j=1}^{m}\theta_j^2]$$<br>其中$λ\Sigma_{j=1}^{m}\theta_j^2$称为<strong>正则化项</strong>，它的目的是为了<strong>缩小每一项的参数</strong>。</p><blockquote><p>$\theta_0$是否正则化对结果影响不大<br>λ的作用是对“+”号的前后（前：更好的拟合训练集，后：假设函数足够简单）两项进行取舍平衡，称为正则化系数  </p></blockquote><p>如果λ被设置的太大，那么所有参数的惩罚力度被加大，这些参数最后的结构都将全部接近于0，那么最后的假设函数将会变成$h_\theta(x)=θ_0$,最终导致欠拟合。  </p><h2 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h2><p><strong>正则化的梯度下降算法</strong><br>在线性回归中，我们使用修改后的梯度下降算法：<br>Repeat {<br>$$θ_0:=θ<em>0-\alpha\frac{1}{m}\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(j)} \tag{1}$$</p><blockquote><p>$θ<em>0$  不需要正则化<br>$$θ_j:=θ_j-\alpha[\frac{1}{m}\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}θ<em>j] \tag{2}$$<br>$$j=1,2,3,…,n$$<br>}<br>事实上  $\frac{1}{m}\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(j)}+\frac{λ}{m}θ_j=\frac{∂J(θ)}{∂θ<em>j}$<br>$\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}=\frac{∂J(θ)}{∂θ_0}$  </p></blockquote><p>如果将（2）中的$\theta_j$统一，那么就可以得到（3）：<br>$$θ_j:=θ<em>j（1-α\frac{λ}{m}）-\frac{α}{m}\Sigma</em>{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)} \tag{3}$$</p><p>由于$1-α\frac{λ}{m}&lt;1$,且只比1小一点点，也就是说，梯度下降算法每次更新的时候$θ_j$在一开始都会比原来小一点点，再进行原来的梯度下降更新  </p><p><strong>正则化的正规方程算法</strong><br>在之前的讲义中，探讨过设计两个矩阵：<br>$X=\begin{bmatrix} (x^{(1)})^T \ …\ (x^{(m)})^T \end{bmatrix}$ 代表有m个数据的数据集 和 $y=\begin{bmatrix} y^{(1)} \ …\ y^{(m)} \end{bmatrix}$ 代表训练集当中的所有的标签<br>通过：<br>$$θ=(X^TX)^{-1}X^Ty$$<br>（相当于对$J(θ)$中的每一个θ求偏导数，并且使其等于0）<br>可以求出最适合的θ<br>现在改变在正规方程中加入一项：<br>$$θ=(X^TX+λ<br>\begin{bmatrix}<br>0 &amp; 0 &amp; 0 &amp; …&amp;0 \   0 &amp; 1 &amp; 0&amp; …&amp;0 \ 0 &amp; 0 &amp; 1&amp; …&amp;0 \… &amp; … &amp; …&amp; …&amp;…\0 &amp; 0 &amp; 0&amp; …&amp;1<br>\end{bmatrix})^{-1}X^Ty$$<br>来达到同样的效果  </p><blockquote><p>$\begin{bmatrix}<br>0 &amp; 0 &amp; 0 &amp; …&amp;0 \   0 &amp; 1 &amp; 0&amp; …&amp;0 \ 0 &amp; 0 &amp; 1&amp; …&amp;0 \… &amp; … &amp; …&amp; …&amp;…\0 &amp; 0 &amp; 0&amp; …&amp;1<br>\end{bmatrix}$是一个(n+1)的方阵  </p></blockquote><p>如果矩阵X不可逆$（m&lt;=n）$,那么$(X^TX)^{-1}$也同样不可逆,但是经过数学证明，无论如何$(X^TX+λ<br>\begin{bmatrix}<br>0 &amp; 0 &amp; 0 &amp; …&amp;0 \   0 &amp; 1 &amp; 0&amp; …&amp;0 \ 0 &amp; 0 &amp; 1&amp; …&amp;0 \… &amp; … &amp; …&amp; …&amp;…\0 &amp; 0 &amp; 0&amp; …&amp;1<br>\end{bmatrix})^{-1}$ 都是可逆的。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.2.%20%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E5%8C%96/"/>
    <url>/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.2.%20%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="分类算法的优化"><a href="#分类算法的优化" class="headerlink" title="分类算法的优化"></a>分类算法的优化</h1><h2 id="其他代价函数优化算法"><a href="#其他代价函数优化算法" class="headerlink" title="其他代价函数优化算法"></a>其他代价函数优化算法</h2><p>对于代价函数，它可以被拆分成求$J(θ)$和求$\frac{∂J(θ)}{∂θ_j }$  两个基础的部分  </p><p>事实上，除了基础的梯度下降算法能够求到最小值之外，还有如下的集中基本方法：</p><ol><li>共轭梯度算法（Conjugate Gradient）</li><li>BFGS</li><li>L-BFGS</li></ol><p>这些算法有一些共同的特点：</p><ol><li>这些算法利用线搜索算法（一种智能内循环）不需要手动选择学习率 α。</li><li>收敛的速度高于梯度下降算法</li><li>复杂度高于梯度下降算法</li></ol><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>对于同一个数据集x，需要分类的目标不只有两种，意味着离散取值y的值不只有0，1两个值。<br>基本思想是<strong>将一个n元分类问题转化为n个二分类问题</strong>。<br>比如对y=1，2，3:<br>可以先将2，3 设置为负类，使用之前的逻辑分类方法就能够将1与（2，3）分开，重复三次，能得到三个逻辑斯蒂函数：$h_θ^i (x)i=1,2,3$。<br>由于$h_θ^i (x)=P(y=i│x; θ)$， 因此$h_θ^i (x)$表示将1设置为正类别，分类器中是1的概率。<br>最后给定函数$max(h_θ^i (x))$， 表示选择出三个当中概率最高的部分，作为判断的结果。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.1.%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <url>/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.1.%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 60%;    padding-left: 20%;}</style><h1 id="逻辑回归、激活函数及其代价函数"><a href="#逻辑回归、激活函数及其代价函数" class="headerlink" title="逻辑回归、激活函数及其代价函数"></a>逻辑回归、激活函数及其代价函数</h1><h2 id="线性回归的可行性"><a href="#线性回归的可行性" class="headerlink" title="线性回归的可行性"></a>线性回归的可行性</h2><p>对分类算法，其输出结果y只有两种结果{0，1}，分别表示负类和正类，代表没有目标和有目标。<br>在这种情况下，如果用传统的方法以线性拟合$（h_θ (x)=θ^T X）$，对于得到的函数应当对y设置阈值a，高于a为一类，低于a为一类。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224223513.png"><br>对于分类方法，这种拟合的方式极易受到分散的数据集的影响而导致损失函数的变化，以至于对于特定的损失函数，其阈值的设定十分困难。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224223523.png"><br>除此之外，$h_θ (x)$（在分类算法中称为分类器）的输出值很可能非常大或者非常小，并不与{0，1}完全相符</p><h2 id="假设表示"><a href="#假设表示" class="headerlink" title="假设表示"></a>假设表示</h2><p>基于上述情况，要使分类器的输出在[0,1]之间，可以采用假设表示的方法。<br>设$h_θ (x)=g(θ^T x)$，其中$g(z)=\frac{1}{(1+e^{−z} )}$, 称为<strong>逻辑斯蒂函数</strong>（Sigmoid function，又称为<strong>激活函数</strong>，生物学上的S型曲线），有：<br>$$h_θ (x)=\frac{1}{(1+e^{−θ^T X} )}$$<br>其两条渐近线分别为h(x)=0和h(x)=1</p><p>在分类条件下，最终的输出结果是：<br>$$h_θ (x)=P(y=1│x,θ)$$<br>其代表在给定x的条件下 其y=1的概率<br>有:<br>$$P(y=1│x,θ)+P(y=0│x,θ)=1$$</p><h2 id="决策边界（-Decision-boundary）"><a href="#决策边界（-Decision-boundary）" class="headerlink" title="决策边界（ Decision boundary）"></a>决策边界（ Decision boundary）</h2><p>对假设函数设定阈值$h(x)=0.5$，<br>当$h(x)≥0.5$ 时，输出结果y=1.<br>根据假设函数的性质，当 $x≥$0时，$h(x)≥0.5$<br>由于之前用$θ^T x$替换x，则当$θ^T x≥0$时，$h(x)≥0.5，y=1$<br>解出 $θ^T x≥0$，其答案将会是一个在每一个$x_i$轴上都有的不等式函数。<br>这个不等式函数将整个空间分成了y=1 和 y=0的两个部分，称之为决策边界。  </p><h2 id="激活函数的代价函数"><a href="#激活函数的代价函数" class="headerlink" title="激活函数的代价函数"></a>激活函数的代价函数</h2><p>在线性回归中的代价函数：<br>$$J(θ)=\frac{1}{m}∑_{i=1}^m \frac{1}{2} (h_θ (x^{(i)} )−y^{(i)} )^2 $$</p><p>令$Cost（hθ (x)，y）=\frac{1}{2}(h_θ (x^{(i)} )−y^{(i)} )^2$， Cost是一个非凹函数，有许多的局部最小值，不利于使用梯度下降法。对于分类算法，设置其代价函数为：</p><p>对其化简：<br>$$Cost（h_θ (x),y）=−ylog(h_θ (x))−((1−y)log⁡(1−h_θ (x)))$$<br>检验：<br>当 $y=1$时，$−log⁡(h_θ (x))$<br>当 $y=0$时，$−log⁡(1−h_θ (x))$  </p><p>那么代价函数可以写成：<br>$$J(θ)=-\frac{1}{m}[∑_{i=1}^m y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))]$$</p><p>对于代价函数，采用梯度下降算法求θ的最小值：<br>$$θ<em>j≔θ_j−α\frac{∂J(θ)}{∂θ_j}$$<br>代入梯度：<br>$$θ_j≔θ_j−α∑</em>{i=1}^m(h_θ (x^{(i)} )−y^{(i)} ) x_j^i $$</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2.4. 向量化</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.4.%20%E5%90%91%E9%87%8F%E5%8C%96/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.4.%20%E5%90%91%E9%87%8F%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h1><p>对于求和的算法，有时可以转换为矩阵的乘法来进行计算<br>E.g.<br>$$H(θ)(x)=∑_{j=0}^nθ_j x_j   $$<br>如果直接求和，求和的过程会非常冗长<br>而设计两个向量$θ$ $x$<br>则有线性回归假设函数的向量形式：<br>$$h(x)=θ^T x$$<br>对更新函数：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224232957.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2.3. 控制和定义语句</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.3.%20%E6%8E%A7%E5%88%B6%E5%92%8C%E5%AE%9A%E4%B9%89%E8%AF%AD%E5%8F%A5/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.3.%20%E6%8E%A7%E5%88%B6%E5%92%8C%E5%AE%9A%E4%B9%89%E8%AF%AD%E5%8F%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="控制和定义语句"><a href="#控制和定义语句" class="headerlink" title="控制和定义语句"></a>控制和定义语句</h1><p>for i=1:10,<br>Indices=a : b 从a到b的索引<br>Break Continue 与C语言相同<br>While, end 结构体 同C语言  </p><p>选择结构：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> condition,    <br>command   <br>end   <br></code></pre></td></tr></table></figure><p>分支选择结构： </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> condition,   <br>command;   <br>elseif condition,   <br>Command;  <br></code></pre></td></tr></table></figure><p>循环结构：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">while</span> condition，  <br>Conmand;  <br> end  <br></code></pre></td></tr></table></figure><p>定义函数：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp">function y=function(x)  <br>command with Y,x;  <br></code></pre></td></tr></table></figure><p>保存为function.m文件  </p><p>返回多个值的函数：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp">function [y1,y2]=function(x)  <br>Command with y1,y2,x;  <br></code></pre></td></tr></table></figure><p>加载函数：<br>定位到m文件目录下  </p><p><code>addpath（&#39;path&#39;）</code>: 加入Octave路径  </p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2.2. 数据计算和绘制</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.2.%20%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%92%8C%E7%BB%98%E5%88%B6%E5%9B%BE%E5%83%8F/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.2.%20%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%92%8C%E7%BB%98%E5%88%B6%E5%9B%BE%E5%83%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="数据计算和绘制"><a href="#数据计算和绘制" class="headerlink" title="数据计算和绘制"></a>数据计算和绘制</h1><h2 id="对元素的操作"><a href="#对元素的操作" class="headerlink" title="对元素的操作"></a>对元素的操作</h2><p><code>A.\*B</code> A矩阵和B矩阵的每一个元素对应相乘<br><code>.</code> 对每一个元素进行运算操作<br><code>abs(A)</code> 对A每一个元素取绝对值<br><code>v+1</code> 对向量v里面的每一个元素+1<br><code>A’</code> 矩阵A的转置<br><code>pinv(A)</code>对A求逆矩阵，不可逆时即为伪逆矩阵<br><code>max(A)</code> A中最大的元素的值<br><code>max(A,[], DI)</code> A中DI维度下元素最大的值（1 列 2 行）<br><code>ind()</code> 某个元素的位置<br><code>magic(n)</code> 返回n*n的幻方<br><code>find(condition)</code>查找对应条件的元素，并返回一个向量<br><code>sum(A)</code>A所有元素的和<br><code>prod(A)</code>A所有元素的乘积<br><code>ceil(A)</code> 对A向上取整<br><code>floor(A)</code>对A每个元素向下取整<br><code>flipud(A)</code>对A上下翻转  </p><h2 id="绘制图像"><a href="#绘制图像" class="headerlink" title="绘制图像"></a>绘制图像</h2><p><code>Plot(x,y,&#39;r&#39;)</code> 绘制关于x，y的图像 r表示y的函数是红色的（默认为蓝色）<br><code>hold on</code> 保存octave内存中的旧函数图像<br><code>xlabel(&#39;&#39;)</code>添加横轴标签<br><code>ylabel（‘’）</code>添加纵轴标签<br><code>legend(&#39;&#39;,&#39;&#39;)</code> 图例<br><code>title（‘’）</code>添加标题<br><code>print -dpng &#39;xx.png&#39;</code> 在当前路径下以png保存当前图像<br><code>close</code> 关闭当前图像<br><code>figure(1)</code>; 标记图像（多开图像窗口）<br><code>subplot(1,2,1)</code> 把图像分成1x2的网格 从第一个格图开始画图<br><code>axis([0.5 1 -1 1])</code>横轴0.5<del>1 纵轴-1</del>1<br><code>clf</code> 清除一幅图像<br><code>imagesc(A)</code>可视化矩阵<br><code>colorbar</code> 添加颜色条<br><code>colormap gray</code>  生成黑白图像<br><code>,</code>依次执行每一个命令  </p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2.1. 基本命令</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.1.%20%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.1.%20%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h1 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h1><h2 id="基本运算"><a href="#基本运算" class="headerlink" title="基本运算"></a>基本运算</h2><p>代数运算：+ - * / sqrt（）<br>布尔运算：且：&amp;&amp;  或：||  非：！<br>赋值：=  </p><h2 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h2><p><code>disp()</code> 显示（）内的命令到屏幕<br><code>sprintf()</code>用法同c语言中的printf<br><code>format lone</code> 显示变量的更多小数位数<br><code>formate short</code> 显示变量的更少小数位数（4位）<br><code>help fuction</code> 显示function 函数的帮助文档  </p><h2 id="矩阵的快速操作"><a href="#矩阵的快速操作" class="headerlink" title="矩阵的快速操作"></a>矩阵的快速操作</h2><p><code>[a b; c d;]</code> 2x2矩阵<br>[]矩阵符号<br>；换行<br>快速建立步长相等的行向量：  起始参数：步长（默认为1）：终止参数<br><code>ones(a,b)</code> 快速生成axb的矩阵，且所有元素为1<br><code>zeros(a,b)</code>快速生成axb的矩阵，且所有元素为0<br><code>rand(a,b)</code>快速生成axb的矩阵，且所有元素的值为在（0，1）内的随机数<br><code>randn(a,b)</code>快速生成axb的矩阵，且所有元素的值为服从正态分布的随机数<br><code>hist()</code> 快速绘制变量的直方图<br><code>eye(a)</code> 快速生成axa的单位矩阵<br><code>size(row,column)</code>返回矩阵的大小，并将大小存入一个1x2的矩阵中<br><code>length(A)</code>返回向量A的最大维度的值<br><code>rref(A)</code> 求解矩阵A的阶梯型</p><h2 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h2><p><code>pwd</code> 返回Octave当前指向的路径<br><code>cd &#39;path&#39;</code> 使Octave指向path路径<br><code>ls</code> 返回Octave当前指向的路径下所有的文件名称  </p><p><code>Load (&#39;file.dat&#39;)</code>  加载file.dat文件<br>*file.dat 是一个编写好的仅有数据（用固定格式aaa bbb ccc）的文件<br><code>Who</code> 返回Octave当前内存中所有的变量<br><code>Whos</code> 返回Octave当前内存中所有的变量和对应的维度、数据类型、数据大小<br><code>Clear varible</code> 清除varible变量<br><code>Clear</code> 清除内存中所有的变量<br><code>Varible1=varible2(a:b)</code>将varible2中的a到b位数据赋给varible1<br><code>Save file.mat varible</code> 将varible存入file.mat中<br><code>Save file.txt varible ascii</code> 将varible存入file.txt中 编码为ascii  </p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p><code>varible(a,b)</code>定位到varible中的（a，b）变量<br><code>C=[A B]</code> 生成[A B]矩阵（B在A右边）<br><code>C=[A;B]</code> 生成[A B]矩阵（B在A下边）  </p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>1.5. 多项式拟合和正规方程</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.5.%20%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.5.%20%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="多项式拟合和正规方程"><a href="#多项式拟合和正规方程" class="headerlink" title="多项式拟合和正规方程"></a>多项式拟合和正规方程</h1><h2 id="特征点的创建和合并"><a href="#特征点的创建和合并" class="headerlink" title="特征点的创建和合并"></a>特征点的创建和合并</h2><p>对于一个特定的问题，可以产生不同的特征点，<strong>通过对问题参数的重新定义和对原有特征点的数学处理合并拆分，能够得到更加优秀的特征点。</strong></p><h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>对于更多更加常见的数学模型，其拟合往往是非线性关系的，这时候就需要考虑引用多项式来进行拟合，如：$h(x)=θ_0+θ_1 x+θ_2 x^2+θ_3 x^3$  </p><h2 id="正规方程算法"><a href="#正规方程算法" class="headerlink" title="正规方程算法"></a>正规方程算法</h2><p>在微积分中，对于函数$f(x,y)$，其局部最值往往是在$f_x=0$ 且$f_y=0$处取得。<br>因此，对于代价函数$J(θ)$，求$J(θ)$对每一个$θ_i$的偏导数，令它们都为0，即：<br>$$\frac{∂J(θ)}{∂θ_i}=0<del>for</del>i=0,1,2,…,n$$<br>称为<strong>正规方程</strong>(Regular expression)。正规方程提供了一种直接求出最小值的方法，而不需要依赖迭代进行一步一步地运算。 </p><h3 id="正规方程的矩阵形式"><a href="#正规方程的矩阵形式" class="headerlink" title="正规方程的矩阵形式"></a>正规方程的矩阵形式</h3><p>对于数据集${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$,  其中每一个$x^{(i)}$都是一个向量：$x^{(i)}=\begin{bmatrix}x_0^{(i)}\x_1^{(i)}\…\x_n^{(i)}\end{bmatrix}$<br>构建<strong>设计矩阵</strong>（Design matrix）$X=\begin{bmatrix}(x^{(1)})^T\(x^{(2})^T\…\(x^{(m)})^T\end{bmatrix}$和值向量$y=\begin{bmatrix}  y^{(1)}\y^{(2)}\…\y^{(m)}  \end{bmatrix}$<br>将代价函数转化为矩阵方程的形式，再对其求导，令其等于0，得到代价函数取得最小值时的$θ$：<br>$$θ=(X^TX)^{-1}X^Ty$$<br>对比梯度下降算法：<br>正规方程算法不需要学习率和迭代，但<strong>对大规模数量（万数量级以上）的特征点（n），工作效率十分低下</strong>。对于一些如分类算法等等更加复杂的算法，正规方程法并不适用于求它们在极值处的θ值。  </p><h3 id="正规方程的不可逆性"><a href="#正规方程的不可逆性" class="headerlink" title="正规方程的不可逆性"></a>正规方程的不可逆性</h3><p>在使用正规方程时，要注意的问题是，<strong>如果设计矩阵X不可逆（为奇异矩阵），正规方程会无法使用。</strong>  </p><p>设计矩阵为奇异矩阵的常见情况：</p><ol><li>x-I 不满足线性关系  </li><li>正在运行的学习算法中，特征点的数量大于样本点的数量（使得$m≤n$）  </li></ol><p>当设计矩阵X不可逆时，应当尝试删除一些特征点，或者考虑正规化（Regularation）。<br>但是总体而言，<strong>矩阵X不可逆的情况是极少数的。</strong></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>1.4. 调试方法</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.4.%20%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.4.%20%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="调试方法"><a href="#调试方法" class="headerlink" title="调试方法"></a>调试方法</h1><h2 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h2><p>对于某些不具有比较性的样本特征$x_i$ （比如对其他的x来说$x_i$ 相当大或者相当小），梯度下降的过程可能会非常漫长，并且可能来回波动才能最后收敛到全局的最小值。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144432.png"><br>在这样的情况下，可以对$x_i$ 进行缩放（如 $x_i≔αx_i$  或者 $x_i=x_i/α$），使得$x_i$ 与其他的$x$具有可比性，以增加梯度下降的效率。<br><strong>通常将$x$缩放至⟦-1,1⟧</strong>的区间内。（只表示一个大致的范围，这不是绝对的。）</p><h2 id="均值归一"><a href="#均值归一" class="headerlink" title="均值归一"></a>均值归一</h2><p>将$x_i$  替换为$x_i−μ_i$ 使得特征值具有为0的平均值（对$x_0$ 不适用）<br>$$x_i:=(x_i−μ_i)/s_i$$<br>定义$μ_i$  为训练集$X$ 的平均值，$s_i=|x_imax−x_imin |$, 表示$x_i$ 的取值范围（近似值），或者直接设置为$s_i$ 的标准差。</p><h2 id="学习率-Learning-rate"><a href="#学习率-Learning-rate" class="headerlink" title="学习率(Learning rate)"></a>学习率(Learning rate)</h2><p>梯度下降调试的方法：</p><ol><li><p>绘制$minJ(θ)-batch$的图像<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144740.png"><br>原则：每一个batch之后 θ 的值都应该减小，这样的图像能够通过直观地表现变化率来表现梯度下降是否收敛（变化率为0）。  </p></li><li><p>自动收敛测试<br>如果$J(θ)$在某一次迭代之后的下降值小于某个值$ε$后，就能够判断算法已经达到了收敛。<br>$ε$的值比较难取，所以通常采取1.中的方法进行观测。</p></li></ol><p>常见的α过大的$minJ(θ)-batch$的图像：<br>α过大,导致代价函数无法收敛<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144816.png">   </p><p>α过小，导致代价函数收敛速度过慢</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>1.3. 多变量预测</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.3.%20%E5%A4%9A%E5%8F%98%E9%87%8F%E9%A2%84%E6%B5%8B/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.3.%20%E5%A4%9A%E5%8F%98%E9%87%8F%E9%A2%84%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 30%;    padding-left: 20%;}</style><h1 id="多变量预测"><a href="#多变量预测" class="headerlink" title="多变量预测"></a>多变量预测</h1><h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><p>对于多个特征量(Features)，规定符号表示：<br>$n$ 特征的总数量<br>$x^{(i)}$  第i个训练样本的输入特征向量，$i$表示的是一个索引(Index)<br>$x_j^i$  第i个训练样本中特征向量的第j个值  </p><p>此时的假设函数不再是单纯的$h_θ (x)=θ_0+θ_1 x$ 的形式。<br>对于多个特征量，此时的假设函数为：<br>$h_θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$<br>对这个样本进行简化：<br>定义$x_0^i=1$, 定义参数向量：$x=\begin{bmatrix} x_0\x_1\…\x_n\end{bmatrix}n$，系数向量：$θ=\begin{bmatrix}θ_0\θ_1\…\θ_n\end{bmatrix}$<br>有：<br>$$h_θ (x)=θ^T x$$<br>这就是假设函数的向量形式。   </p><h2 id="梯度下降算法在多元线性回归中的应用"><a href="#梯度下降算法在多元线性回归中的应用" class="headerlink" title="梯度下降算法在多元线性回归中的应用"></a>梯度下降算法在多元线性回归中的应用</h2><p>对于假设函数：$h_θ (x)=θ^T x=θ<em>0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}$<br>和损失函数：$J(θ_0,θ_1,…,θ_n)=\frac{1}{2m} ∑</em>{i=1}^m(h_θ (x^{(i)} )−y^{(i)} )^2$<br>此时的梯度下降算法：<br>Repeat{<br>$$θ_j≔θ<em>j−α\frac{∂J(θ)}{∂θ_j}$$<br>}<br>对$\frac{∂J(θ)}{∂θ_j}$进行等价变形：<br>Repeat{<br>$$θ_j≔θ_j−α\frac{1}{m}∑</em>{i=1}^m(h_θ (x^{(i)} )−y^{(i)})  x_j^i$$<br>}</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>1.2. 梯度下降算法</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.2.%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.2.%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<style>img{    width: 40%;    padding-left: 20%;}</style><h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><p>在开始之前，为了方便解释，首先规定几个符号所代表的意义：<br>$m$ 训练集中训练样本的数量<br>$X$  输入变量<br>$Y$  输出变量<br>$(x,y)$ 训练样本<br>$(x^i,y^i)$第i个训练样本（i表示一个索引）  </p><h2 id="监督学习算法的流程"><a href="#监督学习算法的流程" class="headerlink" title="监督学习算法的流程"></a>监督学习算法的流程</h2><p>提供训练集&gt;学习算法得到$h$（假设函数：用于描绘x与y的关系）&gt;预测y 的值  </p><h2 id="代价-损失函数（Cost-function）"><a href="#代价-损失函数（Cost-function）" class="headerlink" title="代价/损失函数（Cost function）"></a>代价/损失函数（Cost function）</h2><p><strong>假设函数(Hypothesis function)**——$h$是用来表示某一个数据集可能存在的线性/非线性关系的函数。对于线性拟合，其假设函数为：<br>$$h_θ(x)=θ_1x+θ_0$$<br>这其中的$θ$是假设函数当中的参数。<br>也可以简化为：<br>$$h_θ(x)=θ_1x$$<br>**代价函数</strong>，在统计学上称为均方根误差函数。当假设函数中的系数$θ$取不同的值时，$\frac{1}{2m}$倍假设函数预测值$h_θ(x^{(i)})$和真实值$y^{(i)}$的差的平方的和之间的函数关系表示为代价函数$J$。<br>$$J(θ_0,θ<em>1)=\frac{1}{2m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2$$ </p><blockquote><p>在这里取1/2的原因是便于消除求导之后产生的2倍,同时也可以进一步缩小$θ$  </p></blockquote><blockquote><p>该函数的自变量是$θ_1$和$θ_0$，因此该函数是三维的函数（如图所示）。<br> <img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131130651.png">  </p></blockquote><p><strong>代价函数在几何上表示为数据集空间内的各点到假设函数的距离的平方的平均值的一半。</strong><br>要想使得数据能够被假设函数很好地拟合，那么代价函数要尽量地小。<strong>当代价函数取到它的最小值即$J(θ<em>1)</em>{min}$时，此时的填入假设函数的$θ$对数据的拟合程度是最好的。</strong><br>对于线性的代价函数，假设函数对数据集的拟合程度越高，对应的$(θ_0,θ_1)$越接近代价函数图像等高线的中心。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131132852.png">   </p><h2 id="梯度下降算法（Gradient-Descent）"><a href="#梯度下降算法（Gradient-Descent）" class="headerlink" title="梯度下降算法（Gradient Descent）"></a>梯度下降算法（Gradient Descent）</h2><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>在微积分中，函数$f(x,y)$在$(x_0,y_0)$处是函数值增加最快的方向是<strong>梯度（Gradient）</strong>的方向，<strong>梯度的反方向是函数值减小最快的方向。</strong><br>梯度的计算公式：<br>$$▿f|_{(x_0,y_0)}=(f_x(x_0,y_0),f_y(x_0,y_0))$$</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>梯度下降算法是一种求解代价函数最小值的方法，它可以用在<strong>多维任意的假设函数</strong>当中。<br>简而言之，梯度下降算法求得$J(θ<em>1)</em>{min}$的主要思路是：   </p><ol><li>给定$θ_0$和$θ_1$的初始值，通常令$θ_0=0$，$θ_1=0$。</li><li>不断改变$θ_0$和$θ_1$的值使得$J(θ_0,θ_1$的值逐渐变小，直到找到$J(θ_0,θ_1)$的最小值或者局部最小值。<br>如果从一个初始值出发，寻找附近的最小值，重复该过程，得到上图，最后得到的值为局部最优解。  <blockquote><p>将梯度下降算法类比为爬山，从一个点开始，不断寻找“下山”的路线，最后找到一个“下山”的出口。——吴恩达  </p></blockquote></li></ol><p>当改变初始值时，会找到另一条“下山”的路径，找到第二个局部最优解（局部最小值）。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131134419.png"><br>对于线性回归的代价函数而言，只存在一个局部最小值。（见代价函数的图像）  </p><h3 id="表示"><a href="#表示" class="headerlink" title="表示"></a>表示</h3><p>梯度下降算法可以表示为：<br>Repeat untill convergence{<br>$$θ_j:=θ_j-α\frac{∂J(θ_0,θ_1)}{∂θ_j},j=0<del>and</del>j=1$$<br>}<br>解释：    </p><ol><li>:=  表示赋值运算符</li><li>α称为<strong>学习率</strong>，用来控制下降的<strong>步长</strong>（Padding），即更新的幅度：  <pre><code>  如果α太小，同步更新的速率会非常的慢     而α过大，同步更新时可能会越过最小值点   </code></pre></li><li>$\frac{∂J(θ<em>0,θ<em>1)}{∂θ_j}$是代价函数的梯度：<br>$$\frac{∂J(θ_0,θ_1)}{∂θ_0}=\frac{1}{m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$<br>$$\frac{∂J(θ_0,θ_1)}{∂θ_1}=\frac{1}{m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131135144.png"><br>△在代价函数中（以简化的代价函数为例），无论初始值在最小值点的左侧还是右侧，通过同步更新都能够使该点被“移动（Update）”到最小值，在最小值点，由于导数值为0，最终同步更新停止在了$θ_j=θ_j$，如前面所说，$θ_j$即为极小值点。  </li></ol><h3 id="同步更新"><a href="#同步更新" class="headerlink" title="同步更新"></a>同步更新</h3><p><strong>同步更新</strong>（Simulaneous update）是实现梯度下降算法的最有效方式。<br>$$temp0:<del>θ_0:=θ_0-α\frac{∂J(θ_0,θ_1)}{∂θ_0}$$<br>$$temp1:</del>θ_1:=θ_1-α\frac{∂J(θ_0,θ_1)}{∂θ_1}$$<br>$$θ_0:=temp0$$<br>$$θ_1:=temp1$$<br>这个更新方程能够同时更新$θ_0$和$θ_1$：$θ_0$更新为$θ_0-J’(θ)$，对$θ_1$同理。<br>更新的方法是计算赋值号右边带入$θ_1$和$θ_2$的值进行计算，得到的两个值分别储存在temp0和temp1中，从上到下进行赋值。  </p><p>对于简化的代价函数：<br>$$θ_1：=θ_1-αJ’(θ_1)$$<br>$$\frac{dJ(θ_1,θ_0)}{dθ_j} =d(\frac{1}{2m}Σ(h_θ(x_i)-y_i))^2)$$   </p><p>将梯度代回代价函数中就得到了<strong>Batch梯度下降法</strong>的基本形式：<br>Repeat untill convergence{<br>$$θ_0:=θ<em>0-α\frac{1}{m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})$$<br>$$θ_1:=θ<em>1-α\frac{1}{m}∑</em>{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}$$<br>}    </p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>1.1. 什么是机器学习</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.1.%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.1.%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h1><h2 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><blockquote><p>A computer  program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.    ——Tom Mitchelle  </p></blockquote><p>简言之，机器学习通过完成任务（T）得到经验（E），进而提升性能（P）。<br>例如：一个自我对弈的跳棋学习机器：E ：自我对弈的棋局 T：下跳棋 P：与新对手玩跳棋时的获胜概率  </p><h2 id="机器学习的主要算法类型"><a href="#机器学习的主要算法类型" class="headerlink" title="机器学习的主要算法类型"></a>机器学习的主要算法类型</h2><ul><li><strong>监督学习</strong>（Supervised）<br>人教会计算机完成任务。<br>根据统计数据做直线或曲线拟合/分离数据，来预测结果。<br>其中包括了两大问题：  <ul><li><strong>回归</strong>（Regression）<br>给算法做一个数据集，包含正确答案，（比如房价-年），用线性/非线性回归方程拟合数据,预测数据。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124639.png"></li><li><strong>分类问题</strong>（<strong>逻辑回归</strong>问题）（Classification/Logical regression）<br>用实数对出现的可能状况分类<br>（比如：1和0表示患乳腺癌/不患乳腺癌 ；1表示患乳腺癌A，2表示患乳腺癌B，0表示不患乳腺癌），在多维坐标系中（每一个维度表示不同的属性），然后用线性或非线性的函数将不同类的数据分开。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124726.png"></li></ul></li><li><strong>无监督学习</strong>（Unsupervised）<br>计算机自己学习，经典的算法分为两大类：    <ul><li><strong>聚类算法</strong><br>对并不明确分类的数据集，计算机根据数据特征自动将数据分为几个簇<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124803.png"></li><li><strong>鸡尾酒会算法</strong>（Cocktail party）<br>略，这里只对鸡尾酒会问题和解决方法作一个概述：<br>鸡尾酒会问题是在计算机语音识别 领域的一个问题。<br>当前语音识别技术已经可以以较高精度识别一个人所讲的话，但是当说话的人数为两人或者多人时，语音识别率就会极大的降低，这一难题被称为鸡尾酒会问题。<br>对于的给定混合信号，分离出鸡尾酒会中 同时说话的每个人的独立信号。<br>鸡尾酒问题的解决方法是把两个收音器分别放在两个人的附近，每个收音器且与两个人的距离是不等距的，如此来分离两个人的声音。</li></ul></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/"/>
    <url>/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="算法优化"><a href="#算法优化" class="headerlink" title="算法优化"></a>算法优化</h1><h2 id="参数展开"><a href="#参数展开" class="headerlink" title="参数展开"></a>参数展开</h2><p>参数展开是一种将矩阵展开为向量的方法，常用于很多高级优化中。<br>例如如下的高级优化：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp">function[jVal,gradient]=costFunction(theta)<br>...<br>optTheta=fminunc(@costFunction,initialTheta,options)<br></code></pre></td></tr></table></figure><p>fminuc是一种高级的优化算法。这些高级优化算法的输入值的形式都是参数向量。<br>在神经网络中，很多参数并非是向量的形式，而是完整的矩阵，比如第l层的参数矩阵$Θ^{(l)}$和梯度矩阵$D^{(l)}$(见5.1.)，这时就需要应用参数展开将这些矩阵展开为向量，方法是在Octave中应用<code>[;]</code>表达将所有的元素从矩阵中取出，展开成一个长向量，并应用<code>reshape</code>语法重新合成矩阵。<br>比如如下的10层神经网络：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210204151039.png">   </p><h2 id="梯度检测"><a href="#梯度检测" class="headerlink" title="梯度检测"></a>梯度检测</h2><p>反向传播算法的实现过程非常的繁琐，因此在与其他算法一同工作的时候可能会产生一些bug，这些bug可能本身不会影响程序的运行，但是最终输出的模型准确度可能会非常低。 因此需要引入梯度检测(Gredient Check)来解决反向传播算法或类梯度下降算法中出现的这类问题。  </p><ul><li><p>从数值上近似梯度<br>要想求出代价函数$J(\Theta)$在某一点$\theta$的梯度（在二维内反映为该点的斜率），可以在$\theta$的两边取$\theta \plusmn \epsilon, \epsilon \rightarrow 0$,$\theta$处的梯度可以近似的表示为（实数形式）：<br>$$\frac{dJ(θ )}{dθ }≈  \frac{J(θ +ε )-J(θ -ε )}{2ε }$$<br>称为双侧差分（Two-side difference）。<br>当$\theta$是$\Theta^{(i)}$的展开时，可以用双侧差分来估计所有的偏导数项：<br>$$\frac{∂ J(θ)}{∂ θ_k}≈ \frac{J(θ _k+ε ,θ_1,…,θ_n )-J(θ _k-ε ,\theta_1,…,θ_n  )}{2ε }$$<br>在Octave中用如下的代码实现：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs Cpp"><span class="hljs-keyword">for</span> i=<span class="hljs-number">1</span>:n,<br>  thetaPlus=theta;<br>  thetaPlus=thetaPlus(i)+epsilon;<br>  thetaMinus=theta;<br>  thetaMinus(i)=thetaMinus(i)-epsilon;<br>  gradApprox(i)=(J(thetaPlus)-J(thetaMinus))/(<span class="hljs-number">2</span>*epsilon);<br>end;<br>Check gradApprox≈DVec<br></code></pre></td></tr></table></figure></li><li><p>总结流程</p><ol><li>利用反向传播算法算出$D^{(i)}$的展开向量DVec</li><li>利用双侧差分计算gradApprox</li><li>DVec和gradApprox作比较    </li><li>关闭双侧差分，利用反向传播进行训练（以提高训练时的算法效率）</li></ol><h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>在最开始执行高级优化或者是神经网络的梯度下降时，应当对$Θ$设置一些初始值，即初始化$Θ$。<br>在逻辑回归中，将参数全部初始化为0的做法会导致神经网络中一个单元出发的所有的参数都相等，导致神经网络中所有的隐藏单元都在计算相同的特征。正确的做法是对$Θ$随机地设定一些值来初始化它，具体的做法是：<br>设置某个区间$[-ϵ,ϵ]$，使得所有$θ$都在这个区间内随机取到，用Octave代码实现：  </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp">theta1 = rand(<span class="hljs-number">10</span>,<span class="hljs-number">11</span>)*(<span class="hljs-number">2</span>*init_epsilon)-init_epsilon; <br><span class="hljs-meta">#rand()的作用是随机生成一个mxn的矩阵，矩阵里面所有的元素值都介于0,1之间。</span><br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0/"/>
    <url>/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络的代价函数"><a href="#神经网络的代价函数" class="headerlink" title="神经网络的代价函数"></a>神经网络的代价函数</h1><h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p>接下来的讲义主要考虑两种分类问题：第一种是二元分类，如之前的讲义所述，y的取值只能是0或者1，输出层只有一个输出单元，假设函数的输出值是一个实数；第二种是多元分类，y的取值是一个k维的向量，输出层有k个输出单元。</p><h2 id="神经网络的代价函数形式"><a href="#神经网络的代价函数形式" class="headerlink" title="神经网络的代价函数形式"></a>神经网络的代价函数形式</h2><p>假设一个神经网络训练集有m个训练样本：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$<br>$L$表示神经网络的总层数，$s_l$表示$l$层中神经元的数量（不包括偏置神经元）。<br>在神经网络中使用的代价函数是在逻辑回归中使用的正则化代价函数：<br>$$J(θ)=-\frac{1}{m}[∑<em>{i=1}^m y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))]+\frac{λ}{2m}∑</em>{j=1}^n θ<em>j^2$$<br>略微不同的是，在神经网络中分类标签和假设函数的输出值都变成了k维的向量，因此神经网络中的代价函数变成了：<br>$$J(θ)=-\frac{1}{m}[∑</em>{i=1}^m ∑<em>{k=1}^Ky_k^{(i)} log⁡(h_θ(x^{(i)} )_k)+(1−y_k^{(i)}) log(1−h_θ (x^{(i)})_k)]+\frac{λ}{2m}∑</em>{l=1}^{L-1}∑_{j=1}^{s_l}∑_{j=1}^{s_l+1} (Θ_{ji}^{(l)})_j^2$$<br>解释：  </p><ol><li>用$(h_Θ(x))_i$来表示第i个输出  </li><li>这个代价函数中$∑_{k=1}^K$表示所有的输出单元之和，这里主要是将$y_k$的值与$(h_Θ(x))_k$的大小作比较   </li><li>正则项的作用是去除那些对应于偏置单元的项，具体而言就是不对$i=0$的项进行求和和正则化。  </li></ol><h2 id="代价函数最小化：反向传播算法"><a href="#代价函数最小化：反向传播算法" class="headerlink" title="代价函数最小化：反向传播算法"></a>代价函数最小化：反向传播算法</h2><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>同之前的线性回归和逻辑回归一样，接下来要求得代价函数的最小值$J(Θ)min$并求出$Θ$。主要的步骤是写出$J(Θ)$并求关于每一个$Θ<em>{ij}^{(l)}$的偏导项$\frac{∂}{∂Θ</em>{ij}^{(l)}}J(Θ)$。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210201185117.png"><br>现在先来讨论如上图所示的神经网络中，只有一个训练样本$(x,y)$的情况：<br>首先先用前向传播算法（见讲义4.2）验证假设函数是否会真的输出结果:<br>$$a^{(1)}=x$$<br>$$z^{(2)}=Θ^{(1)}a^{(1)},并增加一个偏置单元$$<br>$$a^{(2)}=g(z^{2})$$<br>$$z^{(3)}=Θ^{(2)}a^{(2)},并增加一个偏置单元$$<br>$$a^{(3)}=g(z^{3})$$<br>$$z^{(4)}=Θ^{(3)}a^{(3)}$$<br>$$a^{(4)}=g(z^{4})=h_Θ(x)$$<br>接下来，为了计算关于每一个$Θ<em>{ij}^{(l)}$的偏导项$\frac{∂}{∂Θ</em>{ij}^{(l)}}J(Θ)$，就要用到<strong>反向传播算法</strong>（Backpropagation）。<br>从直观上说，对于每一个节点，都要计算每个节点的误差：$δ^{(l)}<em>j$,表示第l层第j个节点的误差。<br>$$δ^{(l)}_j=a_j^{(l)}-y_j=(h_Θ(x))_j-y_j$$<br>y表示训练集中y向量里的第j个元素的值。<br>其向量形式：<br>$$δ^{(l)}=a^{(l)}-y$$<br>这里的$δ^{(l)}$和$a^{(l)}$都是一层每一个误差/输出所构成的向量。<br>具体而言，对于上图所示的4层（$L=4$）神经网络,第四层的误差项：<br>$$δ^{(4)}=a^{(4)}-y$$<br>照例写出前面两层的误差：<br>$$\delta^{(3)}=(Θ^{(3)})^Tδ^{(4)}⋅g’(z^{(3)})$$<br>$$\delta^{(2)}=(Θ^{(2)})^Tδ^{(3)}⋅g’(z^{(2)})$$<br>事实上应用微积分的链式法则，$g’(z^{(3)})=a^{(3)}⋅(1-a^{(3)})$,1是一个每项都为1的向量。<br>反向传播的步骤相当于是从最后一层开始求误差，然后将最后一层的误差传给前一层，反向依次传播。<br>最终将会有：<br>$$\frac{∂}{∂Θ</em>{ij}^{(l)}}J(Θ)=a^{(l)}_iδ^{(l+1)}_i$$<br>此处忽略了正则化项：$λ$。<br>现在将反向传播算法从一个训练样本拓展到一个有m个训练样本：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$,$L$层的神经网络训练集：      </p><p>定义$Δ<em>{ij}^{(l)}=0$用于计算$\frac{∂}{∂Θ</em>{ij}^{(l)}}J(Θ)$,接下来遍历整个训练集：<br>For $i=1$ to $m$:<br>  set $a^{(1)}=x^{(i)}$ #用于将所有的x输入到输入层的激活函数中<br>  用正向传播算法计算$a^{(l)}<del>for</del>l=2,3,…,L$<br>  $δ^{(L)}=a^{(L)}-y^{i}$ #计算最后一层的误差<br>  用反向传播算法计算$\delta^{(L-1)}$到$δ^{(2)}$,<br>  $Δ^{(l)}<em>{ij}:=Δ^{(l)}</em>{ij}+a_j^{(l)}δ^{(l+1)}<em>i$<br>  (写成向量的形式：$Δ^{(l)}:=Δ^{(l)}+δ^{(l+1)}(a_j^{(l)})^T$)<br>结束循环后，令<br>$$D^{(l)}_{ij}:=\begin{dcases}<br>    \frac{1}{m}Δ^{(l)}</em>{ij},j=0\ \<br>     \frac{1}{m}Δ^{(l)}<em>{ij}+λΘ^{(l)}</em>{ij},j \not=0<br>\end{dcases}$$<br>那么最终：<br>$$\frac{∂}{∂Θ<em>{ij}^{(l)}}J(Θ)=D^{(l)}</em>{ij}$$</p><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><ul><li><p>回顾：前向传播模型<br>前向传播的整个过程可以用下图表示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210204143238.png"><br>比如：如果洋红色的部分其权重为$Θ<em>{10}^{(2)}$,红色的权重值为$Θ</em>{11}^{(2)}$，青色的权重值是$Θ<em>{12}^{(2)}$， 那么$z_1^{(3)}=Θ_{10}^{(2)} \times 1+Θ</em>{11}^{(2)} × a_1^{(2)}+Θ_{12}^{(2)}×a_1^{(2)}$。<br>反向传播的过程和前向传播非常类似，只是传播的方向不同。  </p></li><li><p>反向传播的理解<br>关注反向传播的代价函数：<br>$$J(θ)=-\frac{1}{m}[∑<em>{i=1}^m ∑</em>{k=1}^Ky_k^{(i)} log⁡(h_θ(x^{(i)} )<em>k)+(1−y_k^{(i)}) log(1−h_θ (x^{(i)})<em>k)]+\frac{λ}{2m}∑</em>{l=1}^{L-1}∑</em>{j=1}^{s_l}∑_{j=1}^{s_l+1} (Θ_{ji}^{(l)})_j^2$$<br>对于单个的样本:$(x^{(i)},y^{(i)})$，只有一个输出单元并且忽略正则化，那么这个样本的代价函数：<br>$$Cost(i)=y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))$$<br>这个代价函数的功能类似于计算方差，可以近似的看做是方差函数：<br>$$Cost(i)≈(h_\Theta(x^{(i)})-y^{(i)})^2$$<br>它反应了样本模型输出值和样本值的接近程度。<br>反向传播中每个节点的误差：$δ^{(l)}_j$,表示第l层第j个节点的误差。有：<br>$$δ^{(l)}_j=\frac{\partial}{∂z_j^{(l)}}Cost(i)$$<br>$z_j^{(l)}$与$h_\Theta(x^{(i)})$相关。  </p><p>反向传播的整个过程可以用下图表示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210204145244.png"><br>例如对$δ^{(2)}<em>2$，洋红色和红色箭头分别表示两个权重值$Θ</em>{12}^{(2)}$和$Θ_{22}^{(2)}$，有<br>$$δ^{(2)}<em>2=Θ</em>{12}^{(2)} ×δ^{(3)}<em>1 +Θ</em>{22}^{(2)} ×δ^{(3)}_2$$</p></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
