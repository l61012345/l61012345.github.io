<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>1. 信号与系统概述</title>
    <link href="/2021/03/15/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F/1.%20%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/"/>
    <url>/2021/03/15/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F/1.%20%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="信号与系统概述"><a href="#信号与系统概述" class="headerlink" title="信号与系统概述"></a>信号与系统概述</h1><h2 id="按照时间特性的信号分类"><a href="#按照时间特性的信号分类" class="headerlink" title="按照时间特性的信号分类"></a>按照时间特性的信号分类</h2><h3 id="确定信号和随机信号"><a href="#确定信号和随机信号" class="headerlink" title="确定信号和随机信号"></a>确定信号和随机信号</h3><ol><li>确定信号<br>除了间断点外，对于一个确定的时间$t$,都能有一个确定的值$f(t)$与之对应。  </li><li>随机信号<br>又分为稳定的和不稳定的随机信号。  </li><li>伪随机信号  </li></ol><h3 id="连续时间信号和离散时间信号"><a href="#连续时间信号和离散时间信号" class="headerlink" title="连续时间信号和离散时间信号"></a>连续时间信号和离散时间信号</h3><ol><li><p>连续时间信号<br>除了间断点外，在任何时间$t$都能找到一个与之对应的值$f(t)$。<br>连续时间信号的<strong>x轴连续，y轴也连续（可以取到任意值）</strong>。<br>例如模拟信号。  </p></li><li><p>离散时间信号<br>只有在离散的点$n$上存在对应的值$x(n)$。<br>离散时间信号的<strong>x轴离散，y轴连续</strong>。  </p></li></ol><p>数字信号是一种<strong>x轴离散，y轴离散的信号</strong>。  </p><p>模拟信号和数字信号的转换：<br>模拟信号经过取样使得x轴离散后，在通过量化使得y轴也离散，进而得到数字信号。  </p><h3 id="常见的时间连续信号"><a href="#常见的时间连续信号" class="headerlink" title="常见的时间连续信号"></a>常见的时间连续信号</h3><ul><li>指数信号  </li><li>正弦信号<br>衰减的正弦信号：<script type="math/tex; mode=display">f(t)=ke^{-at}sinωt \text{ t>=0}</script></li><li>复指数信号  </li><li>采样信号  <script type="math/tex; mode=display">sinc(x)=\frac{sinx}{x}</script></li><li>奇异信号  <ul><li>单位阶梯信号  <script type="math/tex; mode=display">u(t)=\begin{cases}    0 ~t<0\\    1 ~t>0\\    \end{cases}</script>注意: 当$t=0$时，倾向于定义$u(0)=0.5$。<br>变化：<br>左移的阶梯信号： $u(t+t_0)=\begin{cases}<br>  0 ~t<0\\  1 ~t>0\\<br>\end{cases}$<br>右移的阶梯信号：$u(t-t_0)=\begin{cases}<br>  0 ~t<0\\  1 ~t>0\\<br>\end{cases}$<br>门信号：$g(t)=u(t+\frac{\tau}{2})-u(t-\frac{\tau}{2})$<br>符号函数： $sig(t)=2u(t)-1$  </li><li>单位冲激函数和冲击偶函数<br>推导：<br>面积为1，宽为$τ$，高为$\frac{1}{τ}$的门函数，其对称轴是$x=0$。<br>令$τ→0$,得到宽为$0$，高为$∞$的<strong>单位冲激函数</strong>$δ(t)$，即：  <script type="math/tex; mode=display">δ(t)=lim_{τ→0}\frac{1}{τ}[u(t+\frac{τ}{2})-u(t-\frac{τ}{2})]</script>有：  <script type="math/tex; mode=display">u'(t)=δ(t)</script>冲激函数的表示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210315162155.png" alt=""><br>如上图，其中$(n)$表示强度为$n$的冲激函数，即$nδ(t)$。<br>性质：  <ol><li>赋值性  <script type="math/tex; mode=display">∫δ(t)f(t)dt=f(0)</script></li><li>偶函数  <script type="math/tex; mode=display">δ(t)=δ(-t)</script></li><li>缩放<script type="math/tex; mode=display">δ(at)=\frac{1}{|a|}δ(t)</script>证明思路都是利用函数相等=积分相等，分情况讨论得到。<br>对单位冲激函数求导，得到<strong>冲击偶</strong>函数，即$δ’(t)$。<br>性质：  </li><li>赋值性  <script type="math/tex; mode=display">∫δ'(t)f(t)dt=-f'(0)</script></li><li>奇函数  <script type="math/tex; mode=display">δ'(t)=-δ'(t)</script></li></ol></li></ul></li></ul><h3 id="常见的时间离散信号"><a href="#常见的时间离散信号" class="headerlink" title="常见的时间离散信号"></a>常见的时间离散信号</h3><ul><li>单位采样信号  <script type="math/tex; mode=display">δ(n)=\begin{dcases}    0,n\not ={0} \\       1, n=0 \\      \end{dcases}</script>任何的非连续信号都能够用单位采样信号表示。  </li><li>单位阶梯信号  <script type="math/tex; mode=display">u(n)=\begin{dcases}    0,n≧{0} \\       1, n<0 \\      \end{dcases}</script>$δ(n)$是$u(n)$的差分信号。  <script type="math/tex; mode=display">δ(n)=u(n)-u(n-1)</script></li><li>方波序列  </li><li>三角序列  </li><li>单边指数序列</li><li>正弦序列<br>正弦函数到正弦序列的转换：<br>令$t=nT_s$， 有$f(t)=sin(ω_0t)=sin(ω_0T_sn)=x(n)$<br>令$Ω_0=ω_0T_s$，得到：  <script type="math/tex; mode=display">x(n)=sin(Ωn)</script>注意：  <ol><li>$T_s$表示的是采样的间隔时间，周期$T=nT_s$。  </li><li>对于周期序列，有$x(n)=x(n+N)$。</li></ol></li></ul><h2 id="信号的基本处理"><a href="#信号的基本处理" class="headerlink" title="信号的基本处理"></a>信号的基本处理</h2><h3 id="加法和乘法"><a href="#加法和乘法" class="headerlink" title="加法和乘法"></a>加法和乘法</h3><ul><li>对于连续信号<br>函数直接相加或者是相乘。<br>注意： 两个周期连续信号相加不一定是一个周期信号  </li><li>对于离散信号<br>两个序列的对应位置相加或者是相乘。  <h3 id="积分和微分"><a href="#积分和微分" class="headerlink" title="积分和微分"></a>积分和微分</h3></li><li><p>对于连续信号<br>先做信号分解，对每一段分别积分或者微分。  </p></li><li><p>对于离散信号<br>只有累积（$z(n)=∑x(k)$）和差分（$Δx(n)=x(n+1)-x(n)$或$Δx(n)=x(n)-x(n-1)$）。</p><h3 id="移位、缩放、翻转"><a href="#移位、缩放、翻转" class="headerlink" title="移位、缩放、翻转"></a>移位、缩放、翻转</h3><p>移位遵循<strong>左加右减</strong>的原则。<br>缩放遵循<strong>a&gt;1,图像压缩，a&lt;1，图像扩展</strong>的原则。<br>对于$f(at+b)$，基本思路是令$τ=at+b$，反解出$t$就能知道对$t$的操作。  </p></li></ul><h3 id="信号分解"><a href="#信号分解" class="headerlink" title="信号分解"></a>信号分解</h3><ul><li>奇偶分解<script type="math/tex; mode=display">\begin{aligned}  f(t)=f_e(t)+f_o(t) \\  f_e(t)=\frac{1}{2}[f(t)+f(-t)]\\  f_o(t)=\frac{1}{2}[f(t)-f(-t)]\end{aligned}</script></li><li>复分解  <script type="math/tex; mode=display">f(t)=f_r(t)+jf_i(t)</script></li><li>直流交流分解  <script type="math/tex; mode=display">f(t)=f_A(t)+f_D(t)</script><script type="math/tex; mode=display">f_D(t)=\frac{1}{T}∫^{t_0+T}_{t_0}f(t)dt</script></li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>信号与系统</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>6.4 总结：诊断与调试</title>
    <link href="/2021/03/13/6.%20%E8%AF%8A%E6%96%AD%E4%B8%8E%E8%B0%83%E8%AF%95/6.4.%20%E6%9C%AC%E7%AB%A0%E5%9B%9E%E9%A1%BE/"/>
    <url>/2021/03/13/6.%20%E8%AF%8A%E6%96%AD%E4%B8%8E%E8%B0%83%E8%AF%95/6.4.%20%E6%9C%AC%E7%AB%A0%E5%9B%9E%E9%A1%BE/</url>
    
    <content type="html"><![CDATA[<h1 id="总结：-诊断与调试"><a href="#总结：-诊断与调试" class="headerlink" title="总结： 诊断与调试"></a>总结： 诊断与调试</h1><h2 id="回顾：-改进算法性能的思路"><a href="#回顾：-改进算法性能的思路" class="headerlink" title="回顾： 改进算法性能的思路"></a>回顾： 改进算法性能的思路</h2><p>回到本章最开始的改进算法性能的思路：     </p><ol><li>获得更多的数据集</li><li>选用更少的特征以防止过拟合</li><li>获得更多的特征来补充特征集</li><li>增加多项式特征</li><li>增加正则化参数$λ$   </li><li>减小正则化参数$λ$  </li></ol><p>通过这一章的学习，这些思路有各自的功能和局限性：   </p><ol><li>获得更多的数据集            —仅对高方差有效</li><li>选用更少的特征以防止过拟合   —仅对高方差有效</li><li>获得更多的特征来补充特征集   —通常用于高偏差问题</li><li>增加多项式特征              —仅对高偏差有效</li><li>增加或减小正则化参数$λ$     —仅对高偏差有效</li><li>减小正则化参数$λ$           —仅对高方差有效  </li></ol><h2 id="大型神经网络和小型神经网络"><a href="#大型神经网络和小型神经网络" class="headerlink" title="大型神经网络和小型神经网络"></a>大型神经网络和小型神经网络</h2><p>小型神经网络： 计算量简单<br>大型神经网络： 大计算量，更容易出现过拟合现象<br>通常使用一个正则化的大型神经网络的效果要比小型神经网络更好。<br>大型神经网络的隐藏层数目和单元数目可以通过调参进行优化。  </p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>6. 诊断与调试</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>《机器学习》 周志华 1-5章笔记</title>
    <link href="/2021/03/13/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89(1)/"/>
    <url>/2021/03/13/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E8%A5%BF%E7%93%9C%E4%B9%A6%EF%BC%89(1)/</url>
    
    <content type="html"><![CDATA[<h1 id="《机器学习》-周志华-1-5章笔记"><a href="#《机器学习》-周志华-1-5章笔记" class="headerlink" title="《机器学习》 周志华 1-5章笔记"></a>《机器学习》 周志华 1-5章笔记</h1><blockquote><p>作者为博主的同事 黄欣迪   </p></blockquote><h2 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h2><ul><li><p>1.1 引言  </p><p>什么是机器学习？ 机器学习是致力于研究如何通过计算的手段，利用经验来改善系统自身的性能<br>经验——数据 模型——算法 通过相应的算法分析数据——得出结论<br>一些文献用“模型”指全局性结果（决策树） 用“模式”指局部性结果（一条规则）  </p></li><li><p>1.2 基本术语  </p><p>进行机器学习的前提是要有数据，例如给细胞的数据，并对给定细胞的大小，形态等进行记录<br>这组记录的集合称为一个“数据集”，其中每一条记录是关于一个事件或对象的描述，称为一个“示例”或“样本”<br>对于细胞的描述，如大小、形状，称为“属性”或“特征”<br>属性上的取值例如圆形、椭圆形，称为“属性值”<br>属性张成的空间称为“属性空间”、“样本空间”或“输入空间”<br>例如大小、形状、颜色张成的三维空间对于每一个样本都能找到对应的坐标向量，称为“特征向量”<br>从数据中学得模型的过程称为“学习”或“训练”<br>对于潜在规律自身，称为“真相”或“真实”<br>本书有时将模型称为“学习器”<br>拥有了标记信息的示例，称为“样例”<br>如果预测的是离散值，称为“分类” 若为二分类，则是正类和反类 预测的是连续值，称为“回归”<br>监督学习和无监督学习 前者是分类和回归是前者的代表 聚类是后者的代表<br>泛化模型是我们所想要找到的，强泛化模型可以更好适用于整个样本空间  </p></li><li><p>1.3 假设空间  </p><p>泛化学习：通过对训练集中瓜的学习已获得对没有见过的瓜进行判断的能力<br>假设空间：对所有假设组成的空间进行搜索，搜索目标是找到与训练集fit的假设  </p></li><li><p>1.4 归纳偏好  </p><p>偏好：对于不同的模型，它的偏好是不一样的，例如有一个模型更偏好某一特征，它会根据将该特征进行结果的认定<br>任何一个有效的机器学习模型必有其归纳偏好，否则会被假设空间中看似在训练集中“等效”的假设所迷惑<br>算法A优于算法B P9 具体论证 算法A和算法B的期望相同 与算法无关  </p></li><li><p>1.5 发展历程  </p></li><li>1.6 应用现状</li><li>1.7 阅读材料  <h2 id="第二章-模型评估与选择"><a href="#第二章-模型评估与选择" class="headerlink" title="第二章 模型评估与选择"></a>第二章 模型评估与选择</h2></li><li><p>2.1 经验误差与过拟合  </p><p>分类错误的样本占样本总数的比例称为“错误率”<br>m个样本中a个错误  </p><script type="math/tex; mode=display">E=\frac{a}{m}</script><p>相对的精度=1-错误率<br>过拟合的定义为学习器将训练集的自身特点当作了所有潜在样本的性质，这样会导致泛化性下降<br>难以在机器学习的过程中避免过拟合  </p></li><li><p>2.2 评估方法  </p><p>测试集尽量不要出现在训练集当中  </p></li><li><p>2.2.1 留出法  </p><p>将数据集D划分为两个互斥的集合 一个为训练集S 另一个为测试集T<br>划分尽量保持数据分布的一致性 分层采样<br>一般而言测试集至少含30个样例<br><strong>$\frac{2}{3}$到$\frac{4}{5}$的样本用于训练 其余样本用于测试</strong></p></li><li><p>2.2.2 交叉验证法  </p><p>数据集D划分为k个大小相似的互斥子集 每个子集都尽可能保持数据分布的一致性 P26<br>D——D1 D2 D3…. D10<br>D1 D2….D9 训练集  D10 测试集<br>D1 D2….D8 D10 训练集 D9 测试集<br>10次10折交叉验证<br>留一法：交叉验证法的特例——只留下一个样本作为测试集  </p></li><li><p>2.2.3 自助法  </p><p>给定包含m个样本的数据集D 对它进行采样产生数据集D’ 每次随机从D中挑选一个样本 将其拷贝到D’中 重复执行m次<br>得到了包含m个样本的数据集D’ m次采样中始终不被采集到的概率是</p><script type="math/tex; mode=display">(1-\frac{1}{m})^m</script><p>极限值约为0.368 即D中约有36.8%的样本没有在D’中出现<br>用D/D’作为测试集（/表示集合减法) 这样的测试结果叫做“包外估计”<br>自助法适用于数据量较小的数据集 留出法和交叉验证法适用于数据量足够的数据集  </p></li><li><p>2.2.4 调参与最终模型  </p><p>调参往往需要设定一个步长 在对应步长内取值 因为无法在实数范围内取完所有值<br>测试数据：模型在实际使用中遇到的数据  </p></li><li><p>2.3 性能度量  </p><p>定义：衡量模型泛化能力的评价标准<br>性能度量反映了任务需求——不同的性能度量导致不同的评判结果——模型的好坏是相对的<br>回归任务最常用的性能度量是“均方误差”</p><script type="math/tex; mode=display">E(f;D)=\frac{1}{m}\sum_{m=1}^{m}(f(x_i)-y_i)^2</script><p>对于数据分布D和概率密度p(‘) 均方误差为  </p><script type="math/tex; mode=display">E(f;D)=\int_{x\sim D} (f(x)-y)^2p(x)dx</script><p>以下主要是分类任务中常用的性能度量  </p></li><li><p>2.3.1 错误率与精度  </p><p>错误率和精度是分类任务中最常用的两种性能度量 既可以适用于二分类任务 也能适用于多分类任务<br>对样例集D 分类错误率定义为  </p><script type="math/tex; mode=display">E(f;D)=\frac{1}{m}\sum_{i=1}^{m}\prod_{}(f(x_i)\neq{y_i})^2</script><p>精度定义为  </p><script type="math/tex; mode=display">acc(f;D)=\frac{1}{m}\sum_{i=1}^{m}\prod_{}(f(x_i)\neq{y_i})^2=1-E(f;D)</script></li><li><p>2.3.2 查准率、查全率与F1  </p><p>实例：在信息检索中，经常关心的是“检索出的信息中有多少比例是用户感兴趣的”“用户感兴趣的信息中有多少被检索了出来”<br>“查准率”(precision)和“查全率”(recall)是更为适用于此类需求的性能变量<br>对于二分类问题 可将样例根据真实类别与学习器预测类别的组合划分为四种情形<br>真正例 假正例 真反例 假反例 令其为 TP、FP、TN、FN TP+FP+TN+FN=样例总数<br>查准率P与查全率R分别定义为  </p><script type="math/tex; mode=display">P=\frac{TP}{TP+FP}</script><script type="math/tex; mode=display">R=\frac{TP}{TP+FN}</script><p>查全率和查准率除了在一些简单任务中都比较高以外 一般一个高另一个低<br>画出模型实时的P-R图可以判断该模型的性能 如果P-R曲线被另一个模型包裹 那么可以认为被包裹的模型性能差<br>最终可以根据比较P-R曲线围成面积的大小来确定模型性能的好坏(该值不容易估算)<br>平衡点(Break-Event Point, 简称BEP)：查准率=查全率时的取值(平衡点大 学习性能优)<br>但更常用的是F1度量  </p><script type="math/tex; mode=display">F1=\frac{2*P*R}{P+R}=\frac{2*TP}{样例总数+TP-TN}</script><p>如果在实际问题中对查准率和查全率的偏重不同的话 引入Fβ  </p><script type="math/tex; mode=display">\frac{1}{F1}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R})</script><script type="math/tex; mode=display">\frac{1}{F_\beta}=\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})（\beta>1 查全率影响大）（\beta<1 查准率影响大）</script><p>以上公式可以看出F1是基于查准率和查全率的调和平均定义的<br>Fβ则是基于加权调和平均定义的 与算术平均和集合平均相比 调和平均更重视较小值<br>考虑实际情况中需要在n个二分类混淆矩阵上综合考察查全率和查准率<br>做法一：在各个混淆矩阵中分别计算P和R 计算平均值 代入F1<br>做法二：计算TP、FP、TN、FN的平均值 代入P和R 再代入F1  </p></li><li><p>2.3.3 ROC与AUC  </p><p>学习器一般是为测试样本产生一个实值或概率预测 使用该预测值与分类阈值进行比较 若大于阈值则为正类 反之为反类<br>预测结果的好坏决定了学习器的泛化能力<br>该分类过程相当于在排序中以某个截断点将样本分为两个部分 前一部分判做正例 后一部分判做反例<br>ROC曲线则是以排序本身的好坏来判定学习器泛化性能的<br>ROC曲线全称是“受试者工作特征”(Receiver Operating Characteristic)<br>ROC曲线的定义<br>纵轴：“真正例率”  </p><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}</script><p>横轴：“假正例率”  </p><script type="math/tex; mode=display">FPR=\frac{FP}{TN+FP}</script><p>真正问题中对于有限个测试样例<br>设给定m+个正例 m-个反例进行排序<br>首先将分类阈值设为最大——所有样例均预测为反例 初始的真正例率和假正例率均为0 初始坐标(0,0)<br>现在预测一个样本 如果为真正例 标记点的坐标为  </p><script type="math/tex; mode=display">(x,y+\frac{1}{m^+})</script><p>如果为假正例 标记点的坐标为  </p><script type="math/tex; mode=display">(x+\frac{1}{m^-},y)</script><p>进行学习器比较时 若一个学习器的ROC曲线被另一个学习器的曲线完全包住 则可断言后者的性能优于前者 若两个学习器的ROC曲线发生交叉 则不能断言<br>同样 如果要比较两个学习器 较为合理的判据是比较ROC曲线下的面积 即AUC<br>AUC可以估算为  </p><script type="math/tex; mode=display">AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)*(y_i+y_{i+1})</script><p>但在实际问题中如何考虑排序的误差  </p><script type="math/tex; mode=display">l_{rank}=\frac{1}{m^+m^-}\sum_{x^+\in{D^+}}\sum_{x^+\in{D^+}}(\prod_{}(f(x^+)<f(x^-))+\frac{1}{2}\prod_{}(f(x^+)=f(x^-)))</script><script type="math/tex; mode=display">AUC=1-l_{rank}</script><p>理解：如果正例的预测值小于反例 则记一个罚分 如果正例的预测值等于反例 则记0.5个罚分  </p></li><li><p>2.3.4 代价敏感错误率与代价曲线  </p><p>实际问题中 对于不同类型的错误所造成的后果不同<br>例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者的结果不同<br>对于这类问题可以将预测错误的cost设为cost1和cost2<br>代入之前的公式可以计算出总体代价最小时的错误率<br>在非均等代价下 ROC曲线不能直接反映出学习器的期望总体代价 这时需要使用代价曲线<br>具体可见P36  </p></li><li><p>2.4 比较检验（以下为各种概率论中的假设检验）  </p></li><li><p>2.5 偏差和方差  </p><p>泛化误差可以分解为偏差、方差与噪声之和 P45  </p></li></ul><h2 id="第三章-线性模型"><a href="#第三章-线性模型" class="headerlink" title="第三章 线性模型"></a>第三章 线性模型</h2><ul><li><p>3.1 基本形式  </p><p>给定由d个属性描述的示例  </p><script type="math/tex; mode=display">x=(x_1;x_2...;x_d)</script><p>其中xi是x在第i个属性上的取值<br>线性模型学习的是通过属性的线性组合来进行预测的函数 即  </p><script type="math/tex; mode=display">f(x)=w_1x_1+w_2x_2+w_3x_3+.....+w_dx_d+b</script><p>一般用向量写成  </p><script type="math/tex; mode=display">f(x)=w^Tx+b</script><p>w和b学得后 模型就得以确定</p></li><li><p>3.2 线性回归  </p><p>给定数据集  </p><script type="math/tex; mode=display">D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}</script><p>其中  </p><script type="math/tex; mode=display">x_i=(x_{i1};x_{i2};...;x_{id}), y_i\in{R}</script><p>线性回归试图学得一个线性模型以尽可能准确的预测实值输出标记<br>首先可以将离散型属性通过连续化将其转化为连续值<br>例如二值属性“身高” 高=1  矮=0<br>线性回归试图学得  </p><script type="math/tex; mode=display">f(x_i)=wx_i+b, 使得f(x_i)\approx{y_i}</script><p>均方误差是回归任务中最常用的性能度量 所以试图让均方误差最小化<br>具体推导过程 P54<br>一个属性只有一个权重 d个属性就会有d个权重<br>可以考虑广义的情况 比如lny  </p></li><li><p>3.3 对数几率回归  </p><p>对数几率函数  </p><script type="math/tex; mode=display">y=\frac{1}{1+e^{-z}}</script><p>也就称为sigmoid函数<br>预测值z 通过z来找y y是逼近0或者1 由此判断预测为正或反  </p></li><li><p>3.4 线性判别分析  </p><p>线性判别分析(Linear Discriminant Analysis LDA)是一种经典的线性学习方法，主要用于二分类问题<br>目标 同类别的方差最小 不同类别的方差最大<br>(该方法应用比较少)  </p></li><li><p>3.5 多分类学习  </p></li><li><p>3.6 类别不平衡问题  </p><p>过采样：不能重复采样 会造成过拟合<br>欠采样：去除一些样本 让正反例数量接近 然后再进行学习  </p></li><li><p>3.7梯度下降法(补充)  </p></li></ul><h2 id="第四章-决策树"><a href="#第四章-决策树" class="headerlink" title="第四章 决策树"></a>第四章 决策树</h2><ul><li><p>4.1 基本流程  </p><p>决策树的生成是一个递归过程  学习目的是为了产生一棵泛化能力强 即处理未见示例能力强的决策树  </p></li><li><p>4.2 划分选择  </p><p>最关键的是如何选择最优化分属性 希望随着划分过程的进行 决策树的分支结点所包含的样本尽可能属于同一类别<br>即结点的“纯度”越来越高  </p></li><li><p>4.2.1 信息增益  </p><p>“信息熵”(information entropy)是度量样本集合纯度最常用的一种指标<br>假定当前样本集合D中第K类样本所占比例为$P_k(k=1,2,…,|y|)$，则D的信息熵定义为  </p><script type="math/tex; mode=display">Ent(D)=-\sum_{K=1}^{|y|}p_klog_2P_k</script><p>Ent(D)的值越小 D的纯度越高<br>什么是熵：对一种事物的不确定性就叫熵 比如买西瓜 不知道买哪一个是好瓜 这就是熵<br>信息：消除这种不确定性的事物（调整概率、排除干扰、确定情况）<br>噪音：不能消除某人对某件事情不确定性的事物<br>数据：信息+噪音<br>假设一件事情有八种等可能的结果 相当于抛三枚硬币 熵为3bit<br>若每种情况概率分布不相等(一般分布)  </p><script type="math/tex; mode=display">A=\frac{1}{2}|B=\frac{1}{3}|C=\frac{1}{6}</script><script type="math/tex; mode=display">P(A)=\frac{1}{2}(log_26-log_23)</script><script type="math/tex; mode=display">P(B)=\frac{1}{3}(log_26-log_22)</script><script type="math/tex; mode=display">P(C)=\frac{1}{6}(log_26-log_21)</script><script type="math/tex; mode=display">Ent(D)=P(A)+P(B)+P(C)</script><p>得知信息的前后 不确定性的变化——熵的差额 就是信息的量<br>例如ABCD四道选择题 等可能的话熵为2bit<br>如果知道C有一半可能是正确的 那么  </p><script type="math/tex; mode=display">P(A)=P(B)=P(D)=\frac{1}{6}|P(C)=\frac{1}{2}</script><p>现在的熵为  </p><script type="math/tex; mode=display">\frac{1}{6}log_26+\frac{1}{6}log_26+\frac{1}{2}log_22+\frac{1}{6}log_26=1.79</script><p>所以 知道C有一半可能正确的条件后 现在的不确定性是1.79 那么信息量就是0.21<br>西瓜数据集2.0 P76<br>17个样例 8个好瓜 9个坏瓜 一般分布<br>先算出信息熵 根节点  </p><script type="math/tex; mode=display">Ent(D)=\frac{8}{17}log_2\frac{17}{8}+\frac{9}{17}log_2\frac{17}{9}=0.998</script><p>然后计算每一个特征不同的信息增益<br>三种色泽 先算每一种的Ent 然后分权 相加 与根结点的熵作差 得到有关色泽的信息增益  </p><script type="math/tex; mode=display">Ent(D_1)=\frac{3}{6}log_2\frac{6}{3}+\frac{3}{6}log_2\frac{6}{3}=1.000</script><script type="math/tex; mode=display">Ent(D_2)=\frac{4}{6}log_2\frac{6}{4}+\frac{2}{6}log_2\frac{6}{2}=0.918</script><script type="math/tex; mode=display">Ent(D_3)=\frac{1}{5}log_2\frac{5}{1}+\frac{4}{5}log_2\frac{5}{4}=0.722</script><script type="math/tex; mode=display">Gain(D,色泽)=Ent(D)-\sum_{v=1}^3\frac{|D^v|}{D}Ent(D^v)=0.998-(\frac{6}{17}*1.000+\frac{6}{17}*0.918+\frac{5}{17}*0.722)=0.109</script><p>根据计算的结果信息增益越高 作为第一轮选择<br>以此类推  </p></li><li><p>4.2.2 增益率(C4.5算法)</p><p>信息增益准则对可取值数目较多的属性有所偏好<br>所以C4.5决策树算法 是先计算Gain 也就是分权后相加的熵 再用该改值除以IV(a)  </p><script type="math/tex; mode=display">Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}=\frac{0.109}{\frac{6}{17}log_2\frac{17}{6}+\frac{6}{17}log_2\frac{17}{6}+\frac{5}{17}log_2{17}{5}}</script><p>使用增益率方法时 先选出信息增益前五位的信息 随后利用该方法进行进一步筛选  </p></li><li><p>4.2.3 基尼指数(CART算法)  </p><p>Classification and Regression 该方法既可以进行分类也可以进行回归<br>例子：首先先进行统计 西瓜2.0数据集<br>第一个特征色泽 分别统计在青绿这一信息中 是好瓜的个数和不是好瓜的个数<br>乌黑中 是好瓜的个数和不是好瓜的个数 以此类推<br>如果统计中有缺失值例如？？？ 直接跳过<br>CART都是二叉树的模型 衡量纯度的方法<br>Gini index 抽两次 抽得样本中不同的概率来衡量纯度<br>例如B站 P59  </p><script type="math/tex; mode=display">Gini index=1-(\frac{105}{105+39})^2-(\frac{39}{105+39})^2</script><p>基尼指数越大 表示这两个大概率是不同的 所以纯度就下降<br>希望基尼指数越大越好<br>补充：基尼指数的计算代码  </p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gini_index_single</span>(<span class="hljs-params">a,b</span>):</span>  <br>    single_gini=<span class="hljs-number">1</span>-((a/(a+b))**<span class="hljs-number">2</span>)-((b/(a+b))**<span class="hljs-number">2</span>)  <br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">round</span>(single_gini,<span class="hljs-number">2</span>)  <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gini_index</span>(<span class="hljs-params">a,b,c,d</span>):</span>  <br>    zuo = gini_index_single(a,b)  <br>    you = gini_index_single(c,d)  <br>    gini_index = zuo*((a+b)/(a+b+c+d))<br>                   +you*((c+d)/(a+b+c+d))  <br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">round</span> (gini_index,<span class="hljs-number">2</span>)  <br></code></pre></div></td></tr></table></figure><p>补充回归问题<br>SKlearn 计算每一个阈值所对应均方值的最小值  </p></li><li><p>4.3 剪枝处理  </p></li><li><p>4.3.1 预剪枝  </p><p>目的是防止过拟合<br>首先将之前的西瓜数据集2.0分为5/5的训练集以及3/4的测试集<br>根据5/5的训练集先生成一颗决策树 采用的是信息增益准则<br>引入验证集之后 对于一个结点 先计算不划分该结点时的准确率<br>对于这个例子 5/5的训练集我们认为第一个结点脐部全为好瓜 对于测试集 准确率为42.9%<br>划分后 我们获得的标记和测试集进行比较 准确率为71.4%<br>所以我们决定对脐部进行划分 之后的特征也由此类推<br>这样的方法确实可以防止过拟合的产生 但也带来了欠拟合的风险  </p></li><li><p>4.3.2 后剪枝  </p><p>后剪枝主要是在生成决策树之后 自下而上的进行判断<br>这样的方法能够有效的防范欠拟合的的风险<br>但因为是在生成决策树之后进行 所以训练时间的开销会比未剪枝和预剪枝要大得多  </p></li><li><p>4.4 连续与缺失值  </p><p>连续值处理<br>C4.5算法采用二分法 例子：西瓜数据集3.0<br>对于该方法主要计算两个取值 1、信息增益 2、划分点<br>对于每一个划分点进行计算 找到信息增益最大的点 以及最大的信息增益即可<br>缺失值处理<br>对于有缺失值的数据来说 计算信息增益时我们只需要计算该特征下无缺失值所获得的信息增益 再用它来乘以无缺失值数据所占比例即可<br>例如有在色泽特征下 有14个无缺失值的数据 一共有17个数据  </p><script type="math/tex; mode=display">Gain=\frac{14}{17}*Gain(14)</script><p>其他方法<br>离散值<br>1、众数填充 2、相关性最高的列填充<br>连续值<br>1、中位数 2、相关性最高的列做线性回归进行估计  </p></li></ul><ul><li><p>4.5 多变量决策树  </p><p>单变量决策树生成的函数图像的分割线总是与函数轴垂直或平行<br>多变量决策树生成的函数图像的分割线相对复杂 一般是曲线<br>多变量的分界点主要是对于特征的线性组合进行分割  </p></li></ul><h2 id="第五章-神经网络"><a href="#第五章-神经网络" class="headerlink" title="第五章 神经网络"></a>第五章 神经网络</h2><ul><li><p>5.1 神经元模型  </p><p>这本书主要讲的是神经网络和机器学习两个学科领域的交叉部分<br>M-P神经元模型 神经元接收到来自n个其他神经元传递过来的输入信号 这些输入信号通过带权重的链接进行传递<br>神经元接收到的总输入值将与神经元的阈值进行比较 然后通过激活函数处理以产生神经元的输出<br>常用sigmoid函数作为激活函数 把这样的多个神经元进行链接 就得到了神经网络  </p></li><li><p>5.2 感知机与多层网络  </p><p>感知机(perceptron)由两层神经元组成 输入层接收外界输入信号后传递给输出层 输出层是M-P神经元 亦称“阈值逻辑单元”<br>感知机能容易地实现逻辑与、或、非运算 假设  </p><script type="math/tex; mode=display">y=f(\sum_iw_ix_i-\theta)</script><p>激活函数为阶跃函数 可以很容易地实现三种运算<br>例：与运算 令$w_1=w_2=1$, $\theta$=2 仅当$x_1=x_2=1$时候 y=1<br>一般情况下 给定训练数据集 权重wi以及阈值theta可以通过学习得到<br>我们也可以设定阈值$\theta$ 为一个固定的输入-1,0的“哑结点”(dummy node)所对应的连接权重$w_n+1$<br>那么就只用对权重进行学习<br>对于权重的调整 对训练样例(x,y) 若当前的感知机的输出为y’那么权重调整为  </p><script type="math/tex; mode=display">w_i\gets{w_i}+\Delta{w_i}</script><script type="math/tex; mode=display">\Delta{w_i}=\eta(y-y')x_i</script><p>其中$\eta$为学习率 是一个0-1之间的数 从调整方程可以看出 若感知机对于样本的预测是正确的 那么w不发生变化<br>反之进行权重调整<br>感知机的学习能力非常有限 因为它只拥有一层功能神经元<br>因为上述问题都是线性可分的(linearly separable) 所以存在一个线性超平面能将它们分开 这样感知机的学习过程一定会收敛(converge) 以求得适当的权向量w<br>反之 若问题不是线性可分的 感知器的权重将无法收敛 发生震荡 这时就需要多层神经元来解决问题<br>若引入两层的感知机 则可以解决亦或问题<br>输出层与输入层之间的一层神经元被称为隐层或隐含层(hidden layer) 隐含层和输出层神经元都是拥有激活函数的功能神经元<br>对于“多层前馈神经网络”来说 每层神经元与下一层神经元全互连 神经元之间不存在同层连接 也不存在跨层连接<br>输入层：仅接收输入，不进行函数处理<br>隐层和输出层包含功能神经元 单隐层指只含有一层隐层的网络 单隐层网络也被称为两层网络<br>神经网络的学习过程 就是根据训练数据来调整神经元之间的“连接权”(connection weight)以及每个功能神经元的阈值  </p></li><li><p>5.3 误差逆传播算法(反向传播算法)  </p><p>误差逆传播(BackPropagation, 简称BP)算法就是其中的代表 BP算法不仅可用于多层前馈神经网络 还可以用于其他神经网络<br>BP算法是什么样的<br>给定训练集  </p><script type="math/tex; mode=display">D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}, x_i\in{R^d},y_i\in{R^d}</script><p>即输入示例有d个属性描述 输出l维实质向量(如图所示)<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210313002507.png" alt=""><br>假设隐层和输出层神经元都是用sigmoid函数<br>对于训练例$(x_k,y_k)$ 假定神经网络的输出为$y’_k=(y’_{1k},y’_{2k},…,y’_{lk})$ 即  </p><script type="math/tex; mode=display">y'_{jk}=f(\beta_j-\theta_j)</script><p>则网络在$(x_k,y_k)$上的均方误差为  </p><script type="math/tex; mode=display">E_k=\frac{1}{2}\sum_{j=1}^l(y'_{jk}-y_{jk})^2</script><p>图中的网络有(d+l+1)q+l个参数需确定：输入层到隐层的d<em>q个权值、隐层到输出层的q</em>l个权值、q个隐层神经元的阈值、l个输出层神经元的阈值<br>BP是一个迭代学习算法 对于参数进行更新的规则与之前类似 任意参数v的更新估计式为  </p><script type="math/tex; mode=display">v\gets{v}+\Delta{v}</script><p>推导隐层到输出层的连接权$w_{hj}$  </p></li></ul>]]></content>
    
    
    <categories>
      
      <category>技术杂谈</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>概率论与数理统计-常考知识点</title>
    <link href="/2021/03/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
    <url>/2021/03/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="概率论与数理统计"><a href="#概率论与数理统计" class="headerlink" title="概率论与数理统计"></a>概率论与数理统计</h1><blockquote><p>整理重邮常考的知识点  </p><h2 id="独立、相关、互斥、概率的计算"><a href="#独立、相关、互斥、概率的计算" class="headerlink" title="独立、相关、互斥、概率的计算"></a>独立、相关、互斥、概率的计算</h2><ul><li>独立<br>如果事件A,B相互独立，有$P(AB)=P(A)P(B)$, $E(AB)=E(A)E(B)$</li><li>互斥<br>如果事件A,B互斥，有$P(A+B)=P(A)+P(B)$,即$P(AB)=0$<br>独立与互斥没有任何关系</li><li>不相关<br>如果事件A，B不相关，有$Cov(A,B)=0$<br>独立可以推出不相关，不相关不能推出独立  </li><li>概率中常用的计算公式<ul><li>和事件的概率  <script type="math/tex; mode=display">P(A+B)=P(A)+P(B)-P(AB)</script></li><li>德摩根律  <script type="math/tex; mode=display">P(\overline)=1-P(A+B)</script></li><li>概率拆分   <script type="math/tex; mode=display">P(AB)=P(A(1-P(\overline{B})))=P(A)-P(A\overline{B})</script></li></ul></li></ul></blockquote><h2 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h2><ul><li><p>乘法公式<br>在A的条件下，B发生的概率：</p><script type="math/tex; mode=display">P(B|A)=\frac{P(AB)}{P(A)}</script><p>满足性质：  </p><script type="math/tex; mode=display">P(B|A)=1-P(\overline{B}|A)</script></li><li><p>全概率公式<br>将A事件（条件）划分为多个事件$A_i$,那么事件B发生的概率：  </p><script type="math/tex; mode=display">P(B)=ΣP(A_i)P(B|A_i)</script></li><li><p>贝叶斯公式<br>全概率公式的逆公式，表示已知B事件发生的概率下，在A中某一个划分下发生的概率：</p><script type="math/tex; mode=display">P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}=\frac{P(A_i)P(B|A_i)}{ΣP(A_i)P(B|A_i)}</script></li></ul><h2 id="连续性随机变量的分布"><a href="#连续性随机变量的分布" class="headerlink" title="连续性随机变量的分布"></a>连续性随机变量的分布</h2><ul><li>概率密度函数<br>表示连续性随机变量在数轴上分布的稠密程度,有  <script type="math/tex; mode=display">\int_{-∞}^∞f(x)dx=1</script>性质：  <script type="math/tex; mode=display">P(a≤x≤b)=\int_{a}^bf(x)dx</script></li><li><p>概率分布函数<br>表示连续性随机变量在数轴左端分布的概率情况，即$P(X≤x)$的概率</p><script type="math/tex; mode=display">F(x)=\int_{-∞}^xf(x)dx</script><blockquote><p>分段的概率分布函数不要忘记了还要加上前一段的概率    </p></blockquote><p>性质：  </p><script type="math/tex; mode=display">P(a≤x≤b)=F(b)-F(a)</script><script type="math/tex; mode=display">P(x>a)=1-F(a)</script></li><li><p>正态分布<br>分布函数特性:  </p><ol><li>$Φ(a)=P(x ≤ a)$，$P(x &gt; a)=1-Φ(a)$  </li><li>$Φ(0)=0.5$  </li><li>$Φ(-a)=1-Φ(a)$  </li></ol><p>数值特征：  </p><ol><li>$X∼N(μ,σ^2),E(\overline{X})=μ,D(\overline{X})=Cov(X,\overline{X})=\frac{σ^2}{n}$</li><li>见中心极限定理</li></ol></li><li><p>随机变量之间的函数关系<br>倘若随机变量X，Y 之间存在某种函数关系$Y=g(X)$,给定X的分布函数$F_x(x)$,求Y的概率密度函数   </p><script type="math/tex; mode=display">F_y(y)=P(Y≤y)=P(g(x)≤y)=P(x≤h(y))=F_X(h(y))</script><script type="math/tex; mode=display">f_Y(y)=F'_y(y)=F'_x(h(y))h'(y)=f_x(h(y))h'(y)</script></li></ul><h2 id="二元随机变量"><a href="#二元随机变量" class="headerlink" title="二元随机变量"></a>二元随机变量</h2><ul><li><p>二元连续性随机变量的分布函数</p><script type="math/tex; mode=display">F(x,y)=\iint f(x,y)dA=P(A)</script><p>其中A是由x,y围成的区域，对应x和y的一组规划</p></li><li><p>边缘分布概率密度和分布函数<br>对于$F(x,y)$,  </p><script type="math/tex; mode=display">f_x(x)=∫_{-∞}^{∞}f(x,y)dy</script><script type="math/tex; mode=display">f_y(y)=∫_{-∞}^{∞}f(x,y)dx</script><script type="math/tex; mode=display">F_x(x)=F(x,∞)=\int_{\infty}^x f_x(x)dx</script><script type="math/tex; mode=display">F_y(y)=F(∞,y)=\int_{\infty}^y f_y(y)dy</script></li><li><p>条件分布函数<br>X在Y=y条件下的概率密度：  </p><script type="math/tex; mode=display">f_{X|Y}(x,y)=\frac{f(x,y)}{f_Y(y)}</script></li><li><p>二元随机变量的分布函数  </p><ul><li>Z=X+Y<script type="math/tex; mode=display">f_z(z)=∫ f_x(z-y)f_y(y)dy=∫ f_x(x)f_y(z-x)dx</script><blockquote><p>注意需要考虑$Z-Y$的取值范围，必要的时候要对$X$，$Z-Y$两者的取值范围大小进行分类讨论，始终取最小的区间</p></blockquote></li><li>Z=max{X,Y}  <script type="math/tex; mode=display">F_z(z)=F_x(z)F_y(z)</script></li><li>Z=min{X,Y}<script type="math/tex; mode=display">F_z(z)=1-[(1-F_x(x))(1-F_y(y))]</script></li></ul></li></ul><h2 id="统计特征"><a href="#统计特征" class="headerlink" title="统计特征"></a>统计特征</h2><ul><li><p>数学期望<br>离散型： 略<br>连续型：  </p><script type="math/tex; mode=display">E(x)=\int x f(x)dx</script><script type="math/tex; mode=display">E(g(x))=\iint g(x) f(x,y)dA</script><p>性质：  </p><ul><li>$E(X+Y)=E(X)+E(Y)$  </li><li>X,Y相互独立时：$E(XY)=E(X)E(Y)$  </li></ul></li><li><p>方差</p><script type="math/tex; mode=display">D(x)=E(x^2)-[E(x)]^2</script><script type="math/tex; mode=display">D(x)=Σ[E{(x-E^2(x))}]</script><p>性质:  </p><ul><li>$D(X⨦Y)=D(X)+D(Y)⨦2Cov(X,Y)$<blockquote><p>当X,Y独立的时候才有$D(X⨦Y)=D(X)+D(Y)$，在使用公式之前一定要注意X,Y是否是独立的  </p></blockquote></li><li>$D(Cx)=c^2D(x)$  </li></ul></li><li><p>协方差和相关系数</p><ul><li><p>协方差  </p><script type="math/tex; mode=display">Cov(X,Y)=E[(X-E(X))(Y-E(Y))]=E(XY)-E(X)E(Y)</script><p>协方差反映了两个随机变量的相关性，如果$Cov(X,Y)=0$，则X,Y不相关。<br>相关系数是标准化的协方差：</p><script type="math/tex; mode=display">ρ_{(X,Y)}=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}</script></li><li><p>性质<br>$Cov(X,X)=D(X)$<br>$Cov(aX+bY,cX+dY)=acD(X)+(ad+bc)Cov(X,Y)+bdD(Y)$  </p></li></ul></li><li>方差矩阵  <ul><li>原点矩<br>指  $E(X^k)$  </li><li>中心距<br>指  $E[(X-E(X))^k]$  </li></ul></li></ul><h2 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h2><p>中心极限定理的使用条件都是n比较大的时候。  </p><ul><li><p>李雅普诺夫定理<br>对于n个独立的随机变量$X_1,X_2,…X_n$,当n以概率趋近于无穷时，他们的和服从正态分布。</p><script type="math/tex; mode=display">\frac{ΣX_i-Σ\mu_i}{√{Σσ^2}}∼N(0,1)</script><p>当n个随机变量都是正态分布的时候，他们的和也服从正态分布，即：   </p><script type="math/tex; mode=display">\frac{ΣX_i-n\mu}{σ√n} ∼ N(0,1)</script></li><li><p>棣莫弗-拉普拉斯定理<br>当n足够大时，二项分布可视为正态分布。<br>若$X∼B(n,p)$,<br>有$E(X)=np$,$D(x)=np(1-p)$。<br>那么可以标准化X，有：  </p><script type="math/tex; mode=display">\frac{X-E(X)}{√D(X)}∼N(0,1)</script><blockquote><p>二项分布的极限可以是正态分布，也可以是泊松分布，优先选择泊松分布。</p><h2 id="统计样本的数值特征和分布"><a href="#统计样本的数值特征和分布" class="headerlink" title="统计样本的数值特征和分布"></a>统计样本的数值特征和分布</h2></blockquote></li><li>样本的数值特征<ul><li>样本均值<script type="math/tex; mode=display">\overline{X}=\frac{1}{n}Σx_i</script></li><li>样本方差<script type="math/tex; mode=display">S^2=\frac{1}{n-1}Σ(x_i-\overline{x})^2</script><blockquote><p>n-1</p></blockquote></li></ul></li><li>样本的分布<ul><li>$χ^2$分布<br>如果样本$X_1,X_2,…X_n$服从正态分布，那么$χ^2=Σx_i^2$服从自由度为n的$\chi^2$分布。<br>$χ^2$统计量：  <script type="math/tex; mode=display">\chi^2=\frac{(n-1)S^2}{\sigma^2}</script>分位点：<br>双侧α分位点：$\chi^2_{1-\frac{α}{2}}$,$\chi^2_{\frac{α}{2}}$<br>单侧α分位点：$\chi^2_{1-α}$，$\chi^2_{α}$   </li><li>t分布<br>如果样本$X ∼ N(0,1)$,且$Y ∼ χ^2_{n}$, X,Y独立，那么$t=\frac{X}{√\frac{Y}{n}}$服从自由度为n的t分布。<br>t统计量：  <script type="math/tex; mode=display">t=\frac{x-\overline{μ}}{S}</script>分位点：<br>双侧α分位点：$t_{\frac{α}{2}}$,$t_{-\frac{α}{2}}$<br>单侧α分位点：$t_{α}$,$-t_{α}$  </li><li>F分布<br>设$U ∼ χ^2(n_1)$,$V ∼ χ^2(n_2)$,U,V相互独立，则称$F=\frac{\frac{U}{N_1}}{\frac{V}{N_2}}$服从自由度为$(n_1,n_2)$的F分布。  <h2 id="样本的点估计法"><a href="#样本的点估计法" class="headerlink" title="样本的点估计法"></a>样本的点估计法</h2></li></ul></li><li>矩估计法<br>设X的概率密度函数为$f(x:θ_1,…,\theta_k)$, 用样本1~k阶矩（$E(x^k)$）代替总体1~k阶矩建立k个方程，联立求解，结果是含有A的式子。<ul><li>结论<script type="math/tex; mode=display">μ=\overline{x},~~~~~σ^2=B_2=A_2-A_1^2</script></li></ul></li><li>最大似然估计法<br>设X的概率密度函数为$f(x:θ_1,…,\theta_k)$, 通过如下方法找出参数的估计量：   <ol><li>求解似然函数$L(\theta)=Πf(x)$   </li><li>对似然函数取对数$ln(L(\theta))=ln(Πf(x))$  </li><li>求导或者是偏导数，使每一个结果都等于0：$\frac{\partial ln(L(\theta)}{\partial θ}=\frac{\partial ln(Πf(x))}{\partial θ}=0$    </li><li>求解θ  <blockquote><p>特殊情况： 均匀分布估计a,b的值，需要设最大值最小值函数      </p></blockquote></li></ol><ul><li>统计量的选取  <ul><li>无偏性<br>如果$θ$的估计量 $θ̂$ 的数学期望存在，且$E(θ̂)=θ$，称$θ̂$是$θ$的无偏估计量。</li></ul><ul><li>有效性<br>如果$θ$的估计量 $θ̂_1$,$θ̂_2$的方差存在，且$D(θ̂_1)&lt;D(θ̂_2)$,称$θ̂_1$比$θ̂_2$更有效。  </li><li>相合性*<br>如果$θ$的估计量 $θ̂$以概率趋近于$θ$，称$θ̂$是$θ$的相合估计量。  <h2 id="区间估计"><a href="#区间估计" class="headerlink" title="区间估计"></a>区间估计</h2>区间估计遵循以下方法：  </li></ul></li></ul></li></ul><ol><li>选取一个合适的统计量    </li><li>找到α分位点   </li><li>反解出关于参数的不等式  </li><li>代值求出不等式，即为置信区间  <h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2>假设检验遵循以下方法：     </li><li>提出原假设和备择假设，备择假设中不能有等于符号   </li><li>找到α分位点，根据备择假设的符号来判断是单边检验还是双边检验 <blockquote><p>拒绝域的符号和备择假设的符号是相同的  </p></blockquote></li><li>找到拒绝域   </li><li>计算统计量的值，并与分位点的值进行比对，看统计量是否落在了拒绝域     <h2 id="随机过程的数值特征和平稳性"><a href="#随机过程的数值特征和平稳性" class="headerlink" title="随机过程的数值特征和平稳性"></a>随机过程的数值特征和平稳性</h2></li></ol><ul><li>数值特征<br>均值函数：$μ_x(t)=E(X(t))$<br>    例如：若 $X(t)=At+B$,那么$μ_x(t)=E(X(t))=tE(A)+E(B)$<br>自相关函数：$R_{xx}(t_1,t_2)=E[X(t_1).X(t_2)]$<br>方差函数：$D_x(t)=R_{xx}(t,t)-[μ_x(t)]^2$  </li><li>平稳性<br>验证随机过程是否具有宽平稳性需要有三步：  </li></ul><ol><li>验证该过程是否是二阶矩过程*  </li><li>求均值函数是否是一个常数  </li><li>求自相关函数$R(t,t+τ)$是否只与τ有关  <h2 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h2></li></ol><ul><li><p>概率转移矩阵<br>表示状态由现态转移到n次态的概率 $P=[现态（竖） \backslash 次态（横）]$</p><script type="math/tex; mode=display">P(n)=p^n</script><p>$p$表示一步转移矩阵，$P(n)$为n步转移矩阵</p></li><li><p>转移概率</p><script type="math/tex; mode=display">P_{ij}(m,m+n)=P(X_{m+n}=a_j|X_m=a_i)</script><p>表示马尔科夫链在时刻m，状态$a_i$下在时刻m+n下转移到状态$a_j$的概率  </p><ul><li>利用全概率公式可以推出$P(X_{m+n})$的概率</li><li>利用乘法公式可以推出$P(X_{m},X_{m+n})$的概率</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>复习笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>电路与器件-常考知识点</title>
    <link href="/2021/03/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%94%B5%E8%B7%AF%E4%B8%8E%E5%99%A8%E4%BB%B6/"/>
    <url>/2021/03/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%94%B5%E8%B7%AF%E4%B8%8E%E5%99%A8%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="电路与器件"><a href="#电路与器件" class="headerlink" title="电路与器件"></a>电路与器件</h1><h2 id="电学部分知识点"><a href="#电学部分知识点" class="headerlink" title="电学部分知识点"></a>电学部分知识点</h2><h3 id="静态电路分析"><a href="#静态电路分析" class="headerlink" title="静态电路分析"></a>静态电路分析</h3><h4 id="1-Y-Delta-形电路转换"><a href="#1-Y-Delta-形电路转换" class="headerlink" title="1. Y- $\Delta$ 形电路转换"></a>1. Y- $\Delta$ 形电路转换</h4><p>现推方法：<br>从Y电路的两个节点看整个电路，必然只有两个电阻被串联使用<br>从$\Delta$ 电路的两个节点看整个电路，电路呈现一个电阻与两个串联电阻并联的情况<br>列出两个电路的方程，求解即可<br>公式：  </p><script type="math/tex; mode=display">R_3=\frac{R_a R_b}{R_A+R_B+R_C}</script><blockquote><p>注：$R_3$是Y- $\Delta$ 形电路中属于Y的，且在$\Delta$电路中 $R_a$与$R_b$中间的电阻  </p><p>当$R_A=R_B=R_C$时，  </p><script type="math/tex; mode=display">R_Y=\frac{R_Δ}{3}</script></blockquote><h4 id="2-最大功率传输定理"><a href="#2-最大功率传输定理" class="headerlink" title="2. 最大功率传输定理"></a>2. 最大功率传输定理</h4><p>最大功率传输定理针对的是某一部分的最大功率<br>当负载$R_L$的电阻值与内电路电阻值相等时，有负载的功率最大<br>结合戴维南定理，可以将$R_L$外的所有部分等效为一个内电路，当<br>$R_L=R_{Th}$ 时有$R_L$的功率最大。  </p><h3 id="动态电路元件"><a href="#动态电路元件" class="headerlink" title="动态电路元件"></a>动态电路元件</h3><h4 id="1-电容"><a href="#1-电容" class="headerlink" title="1. 电容"></a>1. 电容</h4><ul><li><p>连接方式<br>   串联：</p><script type="math/tex; mode=display">\frac{1}{C_t}=\frac{1}{C_1}+\frac{1}{C_2}+...+\frac{1}{C_n}</script><p>   并联：</p><script type="math/tex; mode=display">C_t=C_1+C_2+...+C_n</script></li><li><p>动态响应方程<br>  时间常数：  </p><script type="math/tex; mode=display">\tau = RC</script><p>  未充电： 断路<br>  充电阶段：</p><script type="math/tex; mode=display">v_C(t)=E(1-e^{-\frac{t}{τ}})</script><script type="math/tex; mode=display">i_C(t)=\frac{E}{R}e^{-\frac{t}{τ}}</script><p>  充电完成：短路（理想）<br>  开关断开的瞬间:</p><script type="math/tex; mode=display">u(0_+)=u(0_-)</script><p>  放电阶段：  </p><script type="math/tex; mode=display">v_C(t)=Ee^{-\frac{t}{τ}}</script><script type="math/tex; mode=display">i_C=\frac{E}{R}e^{-\frac{t}{τ}}</script><p>  放电完成： 断路</p></li><li><p>交流电路响应<br>阻抗：  </p><script type="math/tex; mode=display">Z_c=X_c=-\frac{1}{ωC}j=\frac{V_m}{I_m}</script><blockquote><p>j是虚数单位</p></blockquote><p>随着频率的增加，阻抗会逐渐减小</p></li></ul><h4 id="2-电感"><a href="#2-电感" class="headerlink" title="2. 电感"></a>2. 电感</h4><ul><li>电感的定义<script type="math/tex; mode=display">L=\frac{Φ}{I}</script></li><li>连接方式<br>串联：<script type="math/tex; mode=display">L_t=L_1+L_2+...+L_n</script>并联：<script type="math/tex; mode=display">\frac{1}{L_t}=\frac{1}{L_1}+\frac{1}{L_2}+...+\frac{1}{L_n}</script></li><li><p>动态响应方程<br> 时间常数：  </p><script type="math/tex; mode=display">\tau = \frac{L}{R}</script><p>  未充电： 短路<br>  充电阶段：</p><script type="math/tex; mode=display">v_L(t)=Ee^{-\frac{t}{τ}}</script><script type="math/tex; mode=display">i_L(t)=\frac{E}{R}（1-e^{-\frac{t}{τ}}）</script><p>  充电完成：断路（理想）<br>  开关断开的瞬间:</p><script type="math/tex; mode=display">i(0_+)=i(0_-)</script><p>  放电阶段：  </p><script type="math/tex; mode=display">v_L(t)=Ee^{-\frac{t}{τ}}</script><script type="math/tex; mode=display">i_L=\frac{E}{R}e^{-\frac{t}{τ}}</script><blockquote><p>注：放电过程应当与L并联一个电阻以保护整个电路的安全，因此此处的R的阻值与原来相比已经发生了变化  </p></blockquote><p>  放电完成： 短路</p></li><li><p>交流电路响应<br>阻抗： </p><script type="math/tex; mode=display">Z_L=X_L= ωL=\frac{V_m}{I_m}</script><p>随着频率的增加，阻抗会逐渐增加</p></li><li><p>谐振<br>当电路处于谐振状态时， 有：  <strong>$-X_c=X_L$</strong><br>根据该公式可以求出谐振频率。<br>在谐振电路中：$I=\frac{E}{R}$<br>谐振的时候功率因子为1.<br>品质因数（Q）：</p><script type="math/tex; mode=display">Q=\frac{Q(power)}{P}=\frac{X_L}{R}（串联）=\frac{R}{X_c}(并联)</script></li></ul><h3 id="交流电基础"><a href="#交流电基础" class="headerlink" title="交流电基础"></a>交流电基础</h3><ol><li><p>复角表达<br>以 $v=V_msin(\omega t+ θ)$为例：</p><script type="math/tex; mode=display">v=V_msin(\omega t+ θ) →V_{rms} ∠θ</script><script type="math/tex; mode=display">V_{rms}=\frac{V_m}{\sqrt{2}}</script><script type="math/tex; mode=display">V=V_{rms}=V_{rms} ∠θ</script><blockquote><p>相位角相同才能用复角表示</p></blockquote></li><li><p>RLC-交流电电路的功率<br>平均功率/有功功率：</p><script type="math/tex; mode=display">P=V_{rms}I_{rms}cos\theta=\frac{V_{m}I_{m}}{2}cos\theta</script><blockquote><p>在不含LC的交流电电路中：$P=V_{rms}I_{rms}=\frac{V_{m}I_{m}}{2}$  </p></blockquote><script type="math/tex; mode=display">P=I_{rms}^2R</script><p>功率因子：</p><script type="math/tex; mode=display">cos\theta=\frac{P}{S}</script><p>视在功率：</p><script type="math/tex; mode=display">S=V_{rms}I_{rms}</script><script type="math/tex; mode=display">S=I_{rms}^2Z</script><p>无功功率：</p><script type="math/tex; mode=display">Q=V_{rms}I_{rms}sin\theta</script><script type="math/tex; mode=display">Q=I_{rms}^2X</script><script type="math/tex; mode=display">Q=\sqrt{S^2-P^2}</script><blockquote><p>一般采用通过计算P和S的方式来计算Q  </p></blockquote></li></ol><h3 id="无源滤波器"><a href="#无源滤波器" class="headerlink" title="无源滤波器"></a>无源滤波器</h3><ol><li><p>增益<br>功率增益：</p><script type="math/tex; mode=display">A_{p}=\frac{P_o}{P_i}</script><p>对数形式：</p><script type="math/tex; mode=display">A_{p}=10lg(\frac{P_o}{P_i})</script><blockquote><p>10  </p></blockquote><p>电压增益：</p><script type="math/tex; mode=display">A_{v}=\frac{V_o}{V_i}</script><p>对数形式：</p><script type="math/tex; mode=display">A_{v}=20lg(\frac{V_o}{V_i})</script><blockquote><p>20  </p></blockquote></li><li><p>滤波器电路的连接和功能  </p><ul><li>根据电容和电感频率响应的特性具体问题具体分析  </li><li>截止频率在$X_L=R$或者$X_C=R$时</li><li>相位角：$\theta=arctan(\frac{f_{cutoff}})$</li><li>在截止频率时，相位角为45°</li><li>无源带通滤波器的结构是高通和低通滤波器并联</li></ul></li></ol><h3 id="变压器"><a href="#变压器" class="headerlink" title="变压器"></a>变压器</h3><ol><li><p>变压器的性质</p><ul><li>变压器可以变换<strong>阻抗</strong>，<strong>电压</strong>，<strong>电流</strong></li><li>变压器的耦合系数：<script type="math/tex; mode=display">k=\frac{Φ_m}{Φ_p}</script><blockquote><p>$Φ_m$：次级磁通量，$Φ_p$：初级磁通量  </p></blockquote></li></ul><p>初级电动势：</p><script type="math/tex; mode=display">e_p=N_p \frac{d \Phi_p}{dt}=L_p\frac{d i_p}{dt}</script><p>次级电动势：</p><script type="math/tex; mode=display">e_s=N_s \frac{d \Phi_m}{dt}=kN_s \frac{d \Phi_p}{dt}</script></li><li><p>互感系数（Mutual Inductance）  </p><script type="math/tex; mode=display">M=N_s\frac{d \Phi_m}{di_p}=N_s \frac{d \Phi_p}{di_s}</script><script type="math/tex; mode=display">M=k\sqrt{L_p L_s}</script><p>有，</p><script type="math/tex; mode=display">e_p=M\frac{di_p}{dt}   和  e_p=M\frac{di_s}{dt}</script><blockquote><p>注意下标  </p></blockquote></li><li><p>比例关系  </p><script type="math/tex; mode=display">a=\frac{N_p}{N_s}=\frac{e_p}{e_s}=\frac{i_s}{i_p}</script></li></ol><h2 id="电子元件部分知识点"><a href="#电子元件部分知识点" class="headerlink" title="电子元件部分知识点"></a>电子元件部分知识点</h2><h3 id="半导体原理"><a href="#半导体原理" class="headerlink" title="半导体原理"></a>半导体原理</h3><ol><li>半导体类型<br>N型半导体： 填入电子<br>P型半导体： 抽去原有的电子  </li><li>PN结及性质<br>PN结： P型半导体和N型半导体拼接在一起，使得电流的方向仅能从P极到N极<br>正向偏置： 电流由P到N，P-N结的电阻非常的小，可视为短路<br>反向偏置： 电流由N到P，P-N结的电阻非常大，可视为断路  </li></ol><h3 id="二极管电路"><a href="#二极管电路" class="headerlink" title="二极管电路"></a>二极管电路</h3><ol><li><p><strong>二极管的单向导通性</strong><br>对于理想二极管，顺箭头方向可视为导线，逆箭头方向可视为断路  </p></li><li><p><strong>二极管电路分析</strong><br>先假设二极管是导通的，求出二极管所在支路的电流方向，如果解出电流方向为逆箭头方向，则实际的二极管处于反向偏置状态，假设错误；如果解出的电流方向为顺箭头方向，则二极管处于正向偏置状态，假设正确，以此来判断电路中的二极管是否处于导通状态  </p><blockquote><p>错误的假设情况下 需要重新计算电路</p></blockquote></li><li><p>非理想二极管的等效模型<br>非理想的二极管可以等效为一个理想的二极管和一个0.7V的直流电压源串联  </p></li><li><p>二极管的应用</p><ul><li><strong>半波整流器</strong><br>结构：交流电源和二极管串联<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225123418.png" alt=""><br>分析：交流电源的某一方向可以通过二极管，达到整流器的作用</li><li><strong>全波/桥式整流器</strong><br>结构：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225123320.png" alt=""><br>分析： 无论是交流电源的前半期还是后半期，流过电阻的电流始终是同一个方向 </li></ul><ul><li>全波/变压器整流器<br>结构：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225123456.png" alt=""><br>分析： 变压器的输出端被分成了两段，在交流电的前半期还是后半期，电流都能通过其中的一半电路，流过电阻的电流是同一个方向<blockquote><p>由于引入了变压器，这种整流器的噪声非常的大</p><ul><li>并联限流器<br>结构：二极管和直流电压源E串联<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225123629.png" alt=""><br>分析：当$|V_{sin}|<E$时，二极管导通$V_{out}=E$     当$|V_{sin}|>E$时，二极管导通$V_{out}=V_{sin}$</li><li><strong>峰值限流器</strong><br>结构：二极管和（电容器||电阻）结构串联<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225123629.png" alt=""><br>分析： 在交流电源的前半期，二极管导通，电容器断路，处于充电状态<br>  在交流电源的后半期，二极管断开，电容器放电维持电路的工作  </li></ul></blockquote></li></ul></li></ol><h3 id="运算放大器"><a href="#运算放大器" class="headerlink" title="运算放大器"></a>运算放大器</h3><ol><li><p>放大器的增益<br> 线性增益： 输出与输入的比值是一个定值<br> <strong>电压增益</strong>： $A_v=\frac{V_o}{V_i}$<br><strong>电流增益</strong>:  $A_i=\frac{I_o}{I_i}$<br> <strong>功率增益</strong>:  $A_P=\frac{P_o}{P_i}=A_vA_i$<br> <strong>增益的指数形式</strong>: $A_p=10lg{\frac{P_o}{P_i}}$  </p><blockquote><p>10  </p></blockquote><p>$A_v=20lg{\frac{V_o}{V_i}}$  </p><blockquote><p>20  </p></blockquote><p> 饱和状态： 对于有两个电源的运算放大器，输出电压不会超过最大/最小饱和电压  </p></li><li><p><strong>理想放大器的直流线性等效模型</strong>  </p><ul><li><p><strong>电压等效模型</strong><br>结构:<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225123821.png" alt=""><br>分析：</p><script type="math/tex; mode=display">\frac{V_s}{V_i}=\frac{R_s+R_i}{R_i} \tag{1}</script><script type="math/tex; mode=display">\frac{A_{V_o}V_i}{V_o}=\frac{R_O+R_L}{R_L} \tag{2}</script><script type="math/tex; mode=display">A_v=\frac{V_o}{V_s}=\frac{V_o}{V_i}\frac{V_i}{V_s}=\frac{A_{V_o}}{(1+\frac{R_s}{R_i})(1+\frac{R_O}{R_L})}</script><blockquote><p>对于理想的运算放大器:<br>$A_o=∞,R_i=∞,R_o=0$  </p></blockquote></li><li><p><strong>电流等效模型</strong><br>结构： </p><p>分析：</p></li></ul></li></ol><ol><li><p><strong>级联放大器的增益计算</strong></p><ul><li><strong>一般形式</strong>：<script type="math/tex; mode=display">A_t=\Pi A_i</script></li><li><strong>指数形式</strong>：<script type="math/tex; mode=display">A_t =\Sigma A_i</script></li></ul></li><li><p>运算放大器的符号和端口</p><ul><li>pin1：反相输入端</li><li>pin2：同相输入端 </li><li>pin3：输出端</li></ul></li><li><p><strong>运算放大器电路分析</strong></p><ul><li><strong>虚短路和虚接地</strong><br>在线性应用当中，电流从+ 流向 -时（同相输入）运算放大器的同相输入端和反相输入端之间可以认为是短路的，称为虚短路。<br>当电流从-流向+时（反相输入），反相输入端相当于接地，称为虚接地。</li><li><p><strong>反相输入的运算法放大器电路分析</strong><br>   电路图:<br>   <img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225123925.png" alt=""><br>   如图，<br>   由虚接地$V_-=0$:</p><script type="math/tex; mode=display">i_1=\frac{V_i}{R_1}</script><script type="math/tex; mode=display">i_2=\frac{-V_o}{R_2}</script><p>   同时，$i_1=i_2$<br>   电压增益：<strong>$A_v=\frac{V_o}{V_i}=-\frac{R_2}{R_1}$</strong>  </p><blockquote><p>反向放大器使用负反馈牺牲增益来增加精度  </p></blockquote><p> <strong>反相加法放大器（2个输入电阻的情况）</strong>：  </p><script type="math/tex; mode=display">i_1=\Sigma_{x=1}\frac{V_x}{R_x} \tag{1}</script><script type="math/tex; mode=display">i_2=\frac{-V_o}{R_f} \tag{2}</script><script type="math/tex; mode=display">i_1=i_2 \tag{3}</script><script type="math/tex; mode=display">V_o=-R_f \Sigma_{x=1}\frac{V_x}{R_x}</script></li><li><p>同相输入的运算放大器电路分析    </p><div class="hljs code-wrapper"><pre><code>  电路图:</code></pre></div><p>   <img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225123941.png" alt=""><br>   如图，<br>   由虚短路$V_+=V_-=V_i$:  </p><script type="math/tex; mode=display">i_1=\frac{0-V_i}{R_1}</script><script type="math/tex; mode=display">i_2=\frac{V_o-V_i}{R_2}</script><p>   同时，$i_1=i_2$<br>   电压增益：<strong>$A_v=\frac{V_o}{V_i}=1+\frac{R_2}{R_1}$</strong>  </p><div class="hljs code-wrapper"><pre><code>  **同相加法放大器（2个输入电阻的情况）**：</code></pre></div><p>   设输入节点电压为$V_n$, 有：</p><script type="math/tex; mode=display">\frac{V_o}{V_n}=1+\frac{R_f}{R_b} \tag{1}</script><p>   由<em>叠加定理</em>：  </p><script type="math/tex; mode=display">V_n=\frac{R_2}{R_1+R_2}V_1+\frac{R_1}{R_1+R_2}V_2 \tag{2}</script><script type="math/tex; mode=display">V_o=(1+\frac{R_f}{R_b}) (\frac{R_2}{R_1+R_2}V_1+\frac{R_1}{R_1+R_2}V_2)</script></li></ul></li></ol><h3 id="三极管电路"><a href="#三极管电路" class="headerlink" title="三极管电路"></a>三极管电路</h3><ol><li>三极管电路的符号和三种模式 <ul><li>三极管有PNP和NPN型两种，无论是哪一种，三极管的箭头永远是在基极（B）和发射极（E）两端，由P型半导体指向N型半导体(即激活态下三极管BE的电流方向)</li><li>三种模式： <strong>激活态</strong>（相当于放大器）、截止态、饱和态（CE之间短路） </li></ul></li><li><p>激活状态下的电路分析  </p><ul><li><p>电流关系  </p><script type="math/tex; mode=display">i_E=i_B+i_c</script><script type="math/tex; mode=display">\frac{i_C}{i_B}=\beta</script><blockquote><p>当$β&gt;100$时，通常可以认为$i_C=i_E$  </p><script type="math/tex; mode=display">V_{BE}=0.7V</script><p>$V_B$和$V_E$的孰大孰小由半导体类型决定  </p><p>善用<em>戴维南等效定理</em>，对复杂的三极管电路进行化简</p></blockquote></li></ul><ul><li><strong>KVL在直流三极管电路下的应用</strong><br>如果E最后未接地而接的电源，则可以对BE间进行KVL分析，列出方程，结合电流关系解出方程 </li></ul></li></ol><h3 id="反馈模型"><a href="#反馈模型" class="headerlink" title="反馈模型"></a>反馈模型</h3><ol><li><strong>反馈模型的结构</strong><br> <img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225124457.png" alt=""><br>开环增益：当电路中没有反馈结构B时候的输入与输出之比：$A=\frac{x_o}{x_i}$</li><li><strong>反馈放大器的闭环增益</strong>  <script type="math/tex; mode=display">A_f=\frac{x_o}{x_i}=\frac{A}{1+AB}</script>推导：<script type="math/tex; mode=display">x_o=Ax_i \tag{1}</script><script type="math/tex; mode=display">x_f=Bx_o \tag{2}</script><script type="math/tex; mode=display">x_s=x_f+x_i=x_i+ABx_i \tag{3}</script><script type="math/tex; mode=display">A_f=\frac{x_o}{x_i}=\frac{A}{1+AB}</script></li><li><strong>反馈放大器的优点</strong><br>当$AB$足够大时，$A_f=\frac{1}{B}$,<br>反馈电路（B）通常是由无源器件（RLC）组成，因此此时的增益十分稳定而且可以直接精确地计算得出，即：<ul><li>准确</li><li>可预测</li><li>稳定 </li></ul></li></ol><h3 id="电闸管"><a href="#电闸管" class="headerlink" title="电闸管"></a>电闸管</h3><ol><li><p><strong>肖克利二极管</strong>  </p><ul><li><p>结构<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225124603.png" alt=""><br>两个PNP，NPN三极管串联，其中：<br>$B_1 →C_2$ 且 $C_1 →B_2$<br>$E_1=A,E_2=K$</p></li><li><p>特性曲线<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225124629.png" alt=""><br>当AK之间的电压非常小时,流过肖克利二极管的电流$I_A$非常小，即肖克利二极管表现大电阻的特性<br>当$V_{AK}=V_{BR}$时，A的三极管很容易饱和，此时会回落到某个电压，此后肖克利二极管可视为小电阻或者导线</p></li></ul></li><li><p><strong>锯齿波发生器</strong></p><ul><li>结构<br>肖克利二极管和电容器并联<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225124651.png" alt=""></li><li>分析<br>肖克利二极管和电容器两端的电压相等，设为$V_C$,<br>当$V_C<V_{BR}$时，肖克利二极管表现大电阻特性，电容器充电  当$V_C>V_{BR}$时，肖克利二极管表现小电阻特性（相当于短路），电容器迅速放电</li></ul></li><li><p><strong>SCR整流器</strong></p><ul><li>结构<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225124913.png" alt=""><br>两个三极管串联，其中：<br>$B_1 →C_2$ 且 $C_1 →B_2$<br>$E_1=A,E_2=K，B_2=G$</li><li><p>原理<br>给$B_2$一个非常小的电流$i_G$,</p><script type="math/tex; mode=display">i_{B2}=i_G</script><script type="math/tex; mode=display">i_{C2}=\beta_2 i_G=i_{B1}</script><script type="math/tex; mode=display">i_{C1}=\beta_1 i_{C2}=\beta_1 \beta_2 i_G=i_{B2}</script><script type="math/tex; mode=display">...</script><p>最终$i_{B2}$会非常大  </p></li><li><p>IG0=0时的特性曲线<br>同肖克利二极管</p></li></ul></li></ol><h3 id="标准电源调节"><a href="#标准电源调节" class="headerlink" title="标准电源调节"></a>标准电源调节</h3><ol><li><strong>线性调节公式</strong><script type="math/tex; mode=display">Line Regulation=\frac{\frac{\Delta V_{out}}{V_{out}}\times 100\%}{ΔV_{in}}</script></li><li><strong>串联电压调节器</strong><br>结构：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225125122.png" alt=""></li><li><strong>调整晶体管电路</strong><br>结构：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225124950.png" alt=""><br>原理：<br>肖克利二极管为运算放大器的+提供稳压的作用。<br>当$V_{out}$因为各种原因上升时，与之相连接的分压器$R_2-R_3$会分走一部分电压，对于放大器的反相输入端$V_-=\frac{R_3}{R_2+R_3}$,$V_-$上升，对于整个运算放大器的输入端，有：<script type="math/tex; mode=display">V_{in}(↓)=V_+(-)-V_-(↑)</script>因此输出端$V_{out}=V_{B}$下降，对于可控晶体管，其$V_B(↓)$导致$V_E(↓)$,最终调整$V_{out}(↓)$。<br>与此同时，$V_C(↓)$使得$V_{REF}(↑)$,使得$V_+$上升，但由于肖克利二极管的存在$V_+$的上升幅度不明显。</li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>复习笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>大学物理下-常考知识点</title>
    <link href="/2021/03/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%A7%E5%AD%A6%E7%89%A9%E7%90%86(%E4%B8%8B)/"/>
    <url>/2021/03/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%A7%E5%AD%A6%E7%89%A9%E7%90%86(%E4%B8%8B)/</url>
    
    <content type="html"><![CDATA[<h1 id="大学物理下"><a href="#大学物理下" class="headerlink" title="大学物理下"></a>大学物理下</h1><blockquote><p>整理重邮常考的知识点  </p><p>参考书目为《物理学 （上）》 东南大学 第六版</p><h2 id="导体和介质"><a href="#导体和介质" class="headerlink" title="导体和介质"></a>导体和介质</h2><ul><li>导体的性质  <ol><li>自由电荷分布在导体表面  </li><li>静电平衡时，导体内部电场为0  </li><li>导体表面与叠加电场垂直  </li><li>越尖锐的表面，电荷积聚越多  </li></ol></li></ul></blockquote><ul><li><p>电位移矢量  </p><script type="math/tex; mode=display">\vec{D}=ɛ_0ɛ_r\vec{E}</script></li><li><p>静电屏蔽  </p><ol><li>将物体放入导体壳（法拉第笼）内部，导体壳内部的物体不会受到外电场的影响  </li><li>将法拉第笼接地，里面放入电荷，导体壳内部的电荷不会对外界有影响</li></ol></li><li><p>极化电场<br>真空中的电场$E_0$进入介质后，电场大小变为$E=ɛ_rE_0$,定义电位移矢量$D=ɛ_0ɛ_rE_0$。极化强度：$\vec{P}=\frac{∑\vec{p}}{Δv}$,它在数值上与电荷面密度$σ$相同。<br>电极化率：  </p><script type="math/tex; mode=display">\chi_E=ɛ_r-1=\frac{P}{ɛ_0E}</script><blockquote><p>$ɛ_r&lt;1$</p></blockquote></li><li><p>电容器<br>电容器的最大能量：</p><script type="math/tex; mode=display">W=\frac{1}{2}CU^2</script><p>电场的能量密度：</p><script type="math/tex; mode=display">w_E=\frac{1}{2}ɛE^2</script></li></ul><h2 id="恒定磁场"><a href="#恒定磁场" class="headerlink" title="恒定磁场"></a>恒定磁场</h2><ul><li><p>电场与磁场的对偶性<br>|  | 磁场 | 电场|<br>|—|—|—|<br>|高斯定理|$∫\vec{B}.d\vec{S}=0$|$∫\vec{B}.d\vec{S}=\frac{∑q_{in}}{ɛ_0}$|<br>|安培定理|$∫\vec{B}.d\vec{l}=μ_0I$|$∫\vec{E}.d\vec{S}=0$|<br>|结论|无源有旋|有源无旋|</p></li><li><p>左/右手定则<br>|右手定则 | 左手定则|<br>|—|—|<br>|判断电流产生的磁场|判断导体收到的安培力|<br>|判断感应电流的方向|  |   </p></li><li><p>洛伦兹力的应用<br>洛伦兹力提供向心力，不做功  </p><script type="math/tex; mode=display">F_l=qBv=m\frac{v^2}{R}=F_向</script><ol><li>磁聚焦</li><li>回旋加速器</li><li>霍尔效应</li></ol></li><li><p>磁介质<br>磁化强度：$M=\frac{∑m}{ΔV}$,m表示磁矩<br>磁场强度：$B=\mu_0\mu_rH$<br>磁化率：  </p><script type="math/tex; mode=display">χ_r=\mu_r-1</script><script type="math/tex; mode=display">M=(\mu_r-1)H</script><blockquote><p>$\mu_r$没有大小限制  </p></blockquote><p>若$\mu_r<1$, 称介质为抗磁质   若$\mu_r>1$, 称介质为顺磁质<br>若$\mu_r&gt;&gt;1$, 称介质为铁磁质  </p></li></ul><h2 id="电磁感应"><a href="#电磁感应" class="headerlink" title="电磁感应"></a>电磁感应</h2><ul><li>涡旋电场<br>涡旋电场是有变化的磁场所产生的电场。  <script type="math/tex; mode=display">∮E_Bd\vec{l}=-\frac{dΦ_B}{dt}</script><script type="math/tex; mode=display">∯E_Bd\vec{S}=0</script>结论：涡旋电场是无源有旋的电场  </li><li>自感<br>自感系数：  <script type="math/tex; mode=display">L=\frac{Φ}{I}</script>通过自感产生的电势：  <script type="math/tex; mode=display">ϵ_L=-\frac{dΦ}{dt}=-L\frac{di}{dt}</script></li><li><p>互感<br>互感系数：  </p><script type="math/tex; mode=display">M=\frac{Φ_{21}}{I_1}</script><p> 其中$Φ_{21}$表示在2中由1产生的磁通量，$I_1$是1的电流  </p></li><li><p>磁场能<br>自感耗能：  </p><script type="math/tex; mode=display">W_m=\frac{1}{2}LI^2</script><p>整个电路的能量：  </p><script type="math/tex; mode=display">\int_0^t ϵIdt=\frac{1}{2}LI^2+I^2Rt</script><p>能量密度：</p><script type="math/tex; mode=display">W=\frac{1}{2}LI^2=\frac{B^2V}{2μ}</script><p>V为螺线管/螺绕环的体积  </p><script type="math/tex; mode=display">w_M=\frac{W}{V}=\frac{B^2}{2μ}=\frac{1}{2}BH</script><p>由$B=\mu_0\mu_rH$,  </p><script type="math/tex; mode=display">w_m=\frac{1}{2}\frac{B^2}{\mu_0\mu_r}=\frac{1}{2}\mu_0\mu_rH^2</script></li></ul><h2 id="麦克斯韦方程"><a href="#麦克斯韦方程" class="headerlink" title="麦克斯韦方程"></a>麦克斯韦方程</h2><ul><li><p>光的两大特性</p><script type="math/tex; mode=display">C=\frac{1}{\sqrt{\mu_0ɛ_0}}</script><p>  表示光速是恒定的  </p><p>  由折射光介质比$n=\frac{C}{V}$，有  </p><script type="math/tex; mode=display">n=\frac{1}{\sqrt{\mu_rɛ_r}}</script><p>  表示光是一种电磁波  </p></li><li><p>位移电流<br>  位移电流是假想出来的电流，它遵循所有的电传导定律<br>  全电流是<strong>传导电流</strong>和<strong>位移电流</strong> （以及运流电流*）的总称</p></li><li><p>麦克斯韦方程组的积分形式<br>|名称|方程$<del>~</del><del>~</del><del>~</del><del>~</del><del>~</del>~~~$|结论|<br>|—-|————————-|—-|<br>| 电场中的高斯定理| $∮\vec{D}.d\vec{S}=∫_D ρdV=q$|静电场有源|<br>| 电场中的安培环路定理 |$∮\vec{E}.d\vec{l}=-∫\frac{∂B}{\partial t}dS+0$ |涡旋电场有旋，静电场无旋|<br>|磁场中的高斯定理|$∮\vec{B}.d\vec{S}=0$|磁场无源|<br>|磁场中的安培环路定理|$∮\vec{H}.d\vec{l}=∫(j_c+\frac{\partial D}{∂t})dS$|磁场有旋|  </p></li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>复习笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>数字系统与微处理器-常考知识点</title>
    <link href="/2021/03/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%95%B0%E5%AD%97%E7%B3%BB%E7%BB%9F%E4%B8%8E%E5%BE%AE%E5%A4%84%E7%90%86%E5%99%A8/"/>
    <url>/2021/03/12/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%95%B0%E5%AD%97%E7%B3%BB%E7%BB%9F%E4%B8%8E%E5%BE%AE%E5%A4%84%E7%90%86%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="Knowledge-points-of-Digital-Systems-and-Microprocessors"><a href="#Knowledge-points-of-Digital-Systems-and-Microprocessors" class="headerlink" title="Knowledge points of Digital Systems and Microprocessors"></a>Knowledge points of Digital Systems and Microprocessors</h1><blockquote><p>*: appeared in all 3-year final tests<br> (Given table): the reference is given in the test paper</p><h2 id="Basic-knowledge-of-binary-code-and-Boolean-algebra"><a href="#Basic-knowledge-of-binary-code-and-Boolean-algebra" class="headerlink" title="Basic knowledge of binary code and Boolean algebra"></a>Basic knowledge of binary code and Boolean algebra</h2><p><strong>Conversion between Dec  BCD, Hex and Binary*</strong><br>X-based to Dec:  </p><script type="math/tex; mode=display">∑_{k=0}^na \times x^k</script><p>ec to X-based:<br>Successive division  </p></blockquote><p><strong>m-in-n/m-out-of-n/m-of-n codes</strong><br>N stands for total number bits<br>M stands for m bits must be set to 1<br>If the number of  1 in a data package is not equal to m, error happens  </p><p><strong>Parity code</strong><br>The number of 1 in a line is consistently to be even/odd.<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225125946.png" alt="">    </p><p><strong>Truth table equivalence*</strong><br>If two Boolean functions share the same truth table, they are equal.  </p><h2 id="Combinational-logic"><a href="#Combinational-logic" class="headerlink" title="Combinational logic"></a>Combinational logic</h2><p><strong>1st and 2nd canonical form*</strong><br>1st canonical form: (.)+(.)<br>the result of Kmap: 0: F, 1:T<br>2nd canonical form: (+).(+)<br>the result of Kmap: 0:T, 1:F<br>Relation:   $F_{1st}+F_{2nd}=1$<br>They share the same Kmap.  </p><p><strong>Conversion between NAND NOR version*</strong><br>1st canonical form to NAND version:   </p><script type="math/tex; mode=display">A.B+C.D=\overline{(!A.!B).(!C.!D)}</script><p>2nd canonical form to NOR version:   </p><script type="math/tex; mode=display">(A+B).(C+D)=\overline{\overline{A+B}+\overline{C+D}}</script><p><strong>Cellular Logic</strong><br>Recursive function   </p><h2 id="Sequential-logic"><a href="#Sequential-logic" class="headerlink" title="Sequential logic"></a>Sequential logic</h2><p><strong>Structure of SRFF</strong><br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225130211.png" alt="">    </p><p><strong>Characteristics of TFF and DFF*</strong><br>TFF:<br>Structure:<br>Asynchronous TFF:  J=K=1 CLK=T<br>Synchronous TFF:    J=K=T CLK=CLK<br>Functionality:<br>T=0 keep the state<br>T=1 change the state<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225130730.png" alt=""><br>DFF:<br>Structure:<br>$J=D$ $K=\overline{D}$<br>Functionality:<br>whenever the current state is, the next state will be D.<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225130846.png" alt="">                    </p><p><strong>Feedback Shift Register*</strong><br>Method: move the head bits to the tail, with possible options.   </p><p><strong>Counter</strong><br>Counter is used to generate a sequence.  </p><p><strong>Circuit design (Transition diagram, P/N state table, Transition table, K-maps)*</strong><br>Transition diagram:<br>Set A=00 B=01 C=11 D=10<br>Input is IA/B/C/D, list the transition table.  </p><h2 id="Digital-System"><a href="#Digital-System" class="headerlink" title="Digital System"></a>Digital System</h2><p><strong>Multiplexer and design*</strong><br>Simplify some variables in Boolean function, the simplified variables will be used for encoder part.<br>The meaning of n to 1: the number of control bits will be 2^k=n  </p><p><strong>Memory and how to determine its size</strong><br>The memory is determined by the number of logic gates which connected to R/W line.<br>The size of memory= number of gates x number of layers bits.<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201225131054.png" alt=""></p><h2 id="Introduction-of-PIC"><a href="#Introduction-of-PIC" class="headerlink" title="Introduction of PIC"></a>Introduction of PIC</h2><p><strong>The basic information of PIC16F876A*</strong>, including:<br>The size of Program memory/Code memory is 8k bits, Data memory is 13k bits, Program counter(PCL) is 13k bits and located in data memory.<br>The maximum level of stacks is 8, with 13 bits width.<br>Number of Pins is 40 (DIP40).<br>Harvard Architecture.<br>There are 35 instructions for PIC16F876A.<br>The location and functionality of SFRs:  </p><ul><li>W register: outside of program memory, the data transition station  </li><li>STATUS register: Program memory, determine which bank in program memory will be used  </li></ul><p><strong>Functionality of some assembly language instructions (Given the table)*</strong>, including:  </p><ul><li>BSF: set the file register to be 1  </li><li>BCF: clear the file register to be 0  </li><li>NOP: do nothing but create a 1 bit delay  </li><li>BTFSC: bit check, if file register is cleared, then skip the following line  </li><li>BTFSS: bit check, if file register is set, then skip the following line  </li></ul><p><strong>Conversion between Two’s complement and  Dec*</strong><br>Positive Dec number: convert to binary number, then keep it.<br>Negative Dec number: remove “-“,convert to binary number, do complement on the number, then +1.  </p><p><strong>Conversion between assembly language instructions and machine code*</strong> (Give the table)<br>Structure of machine code:   </p><ul><li>operation code     </li><li>destination     </li><li>file address  </li></ul><p>4 kinds of characters of instruction’s 14-bit Opcoder:  </p><ul><li>K literal </li><li>X don’t care (default to be 0)</li><li>D W register (1 for W register to File register, 0 for File register to W register)</li><li>F  file register’s address</li><li>b  bit address within an 8-bit file register</li></ul><p><strong>Subroutine’s explanation by explaining instructions like CALL, GOTO and RETURN and time consumption*</strong><br>Time/Cycle consumption: Normal execution costs 1 us, GOTO and RETURN instruction costs 2us.  </p><p><strong>Track the value of W register</strong><br><strong>Output Devices*</strong>  </p><ul><li><p>7-Segments:<br>Port A: which 7-segments will be lighten<br>Port B: display the number (0:light, 1:dark)  </p></li><li><p>Keypad:<br>Switch Bounce: There should be a delay to wait the time in order to let the key bounce (about10-3s) be finished, or may cause the hazard.  </p></li></ul><p><strong>ADC</strong><br>Precession and LSB:  $LSB=\frac{Range}{2^k}$ , k is the number of bits<br>Quantity error: ⁺-0.5 LSB</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>复习笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>6.3 学习曲线</title>
    <link href="/2021/03/12/6.%20%E8%AF%8A%E6%96%AD%E4%B8%8E%E8%B0%83%E8%AF%95/6.3.%20%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF/"/>
    <url>/2021/03/12/6.%20%E8%AF%8A%E6%96%AD%E4%B8%8E%E8%B0%83%E8%AF%95/6.3.%20%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 30%;<br>    padding-left: 40%;<br>}</style></p><h1 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h1><p>学习曲线是一种检查算法是否正常运行的方法。 具体方法如下：<br>改变训练样本的总数$m$, 分别计算一系列的训练误差$J_{train}(\theta)$和交叉验证误差$J_{cv}(\theta)$。得到结论：如果训练样本的总数很小，模型往往能够很好的拟合，随着样本数的增大，假设模型的平均训练误差会逐渐增大。对于验证集，由于验证集当中的样本都是未被训练过的，在训练样本数很低时，模型的泛化程度不高，因此如果训练样本的总数很小时，假设模型的平均验证误差会很高，随着样本数的增大，平均验证误差会逐渐减小。如下图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210312162734.png" alt="">   </p><h2 id="高偏差和高方差学习曲线"><a href="#高偏差和高方差学习曲线" class="headerlink" title="高偏差和高方差学习曲线"></a>高偏差和高方差学习曲线</h2><h3 id="高偏差学习曲线"><a href="#高偏差学习曲线" class="headerlink" title="高偏差学习曲线"></a>高偏差学习曲线</h3><p>如果模型不能很好的拟合数据，即出现了高偏差。  在训练集总数$m$非常小时，训练误差非常大，随着随着训练集总数$m$的增大，验证误差会逐渐的减小，最终停留在一个较高的水平。 对于训练误差，随着训练集总数$m$的增大，训练误差误差会越来越大，最终趋近于验证误差。  因为模型的参数过少，因此最终验证误差和训练误差会非常的接近。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210312163507.png" alt=""><br>从上图可以看出：<strong>如果一个模型具有高偏差的特性，选用更多的训练集数据并不能改善准确度。</strong>   </p><h3 id="高方差学习曲线"><a href="#高方差学习曲线" class="headerlink" title="高方差学习曲线"></a>高方差学习曲线</h3><p>模型在过拟合下，随着训练集总数$m$的增大，由于模型的泛化程度底下，因此训练误差会越来越高，并最终保持在一定水平。  同高偏差学习曲线一样，高方差模型的验证误差很大，并最终保持在一定水平。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210312164157.png" alt=""><br>从上图可以看出：<strong>如果一个模型具有高方差的特性，选用更多的训练集数据能够改善准确度。</strong>    </p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>6. 诊断与调试</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>6.2 方差与偏差</title>
    <link href="/2021/03/07/6.%20%E8%AF%8A%E6%96%AD%E4%B8%8E%E8%B0%83%E8%AF%95/6.2.%20%E6%96%B9%E5%B7%AE%E5%92%8C%E5%81%8F%E5%B7%AE/"/>
    <url>/2021/03/07/6.%20%E8%AF%8A%E6%96%AD%E4%B8%8E%E8%B0%83%E8%AF%95/6.2.%20%E6%96%B9%E5%B7%AE%E5%92%8C%E5%81%8F%E5%B7%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="方差和偏差"><a href="#方差和偏差" class="headerlink" title="方差和偏差"></a>方差和偏差</h1><h2 id="判断方法"><a href="#判断方法" class="headerlink" title="判断方法"></a>判断方法</h2><p>运行一个学习算法时，如果模型表现不理想，有高可能性是发生了欠拟合（<strong>高偏差（Bias）</strong>）或者过拟合（<strong>高方差（Variance）</strong>）问题。 那么如何判断算法究竟出现了哪一种问题？<br>上一讲中已经定义过训练，测试和验证误差。 通常情况下，假设多项式模型中多项式的次数与训练和测试、验证误差的关系如下图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210307143518.png" alt=""><br>通过上图能够判断模型到底出现了欠拟合还是过拟合：<br><strong>如果训练和测试误差都很高，那么有高概率是发生了欠拟合问题。</strong> <strong>如果训练误差低，测试误差高（远大于训练误差），那么有高概率发生了过拟合问题。</strong>    </p><h2 id="偏差和方差与正则化算法"><a href="#偏差和方差与正则化算法" class="headerlink" title="偏差和方差与正则化算法"></a>偏差和方差与正则化算法</h2><p>假设已经得到了一个$d=4$的多项式模型：</p><script type="math/tex; mode=display">h_θ(x)=θ_0+θ_1x+θ_2x^2+θ_3x^3+θ_4x^4</script><p>对其代价函数增加一个正则化项使得参数尽量缩小:   </p><script type="math/tex; mode=display">J_{\theta}=\frac{1}{2m}[\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+λ\Sigma_{j=1}^{m}\theta_j^2]</script><p>下图表示了正则化系数$λ$的大小与拟合情况的关系：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210307144737.png" alt=""><br><strong>过大的$λ$容易发生欠拟合问题，而过小的$λ$（近似于$λ=0$）则无法起到规避过拟合的作用。</strong><br>如何设置合适的$λ$呢？<br>通过正则化的代价函数$J(θ)$求出最合适的一组$θ$，此时模型的<strong>训练误差$J_{train}(θ)$应当不包含正则化项</strong>，也就是：</p><script type="math/tex; mode=display">J_{train}(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2</script><p>在加入正则化算法后，测试一系列的$λ$的值，并得到一系列最优化的代价函数，并求到一系列的$Θ$,再用验证集计算验证集误差$J_{cv}(θ)$,最终选择验证集误差最小的那一组$θ$,使用测试集计算出测试误差$J_{test}$。<br>下图表示了$λ$的大小与训练误差和验证误差的关系：<br> <img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/9BBF22BD7E2387E07D92813FDF1EBA03.png" alt="">   </p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>6. 诊断与调试</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>计算机结构基础-课堂笔记</title>
    <link href="/2021/03/04/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%93%E6%9E%84%E4%B8%8E%E6%8E%A5%E5%8F%A3/1.%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%93%E6%9E%84%E5%9F%BA%E7%A1%80/"/>
    <url>/2021/03/04/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%93%E6%9E%84%E4%B8%8E%E6%8E%A5%E5%8F%A3/1.%20%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%93%E6%9E%84%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="计算机结构基础"><a href="#计算机结构基础" class="headerlink" title="计算机结构基础"></a>计算机结构基础</h1><blockquote><p>讲义复习<br>BUL EE2623 Computer Architecture and Interface<br>Dr. Takebumi Itagaki   </p></blockquote><h2 id="冯诺依曼架构"><a href="#冯诺依曼架构" class="headerlink" title="冯诺依曼架构"></a>冯诺依曼架构</h2><p>组成部分： CPU/ALU,I/O, Buses, Main Memory<br>特征： 所有的部分都通过总线连接<br>总线的类别： 数据总线，地址总线，控制总线</p><h2 id="数制"><a href="#数制" class="headerlink" title="数制"></a>数制</h2><p>十六进制，十进制，八进制，二进制的相互转化。</p><h3 id="二进制整数的表达和运算"><a href="#二进制整数的表达和运算" class="headerlink" title="二进制整数的表达和运算"></a>二进制整数的表达和运算</h3><h4 id="Unsigned"><a href="#Unsigned" class="headerlink" title="Unsigned"></a>Unsigned</h4><p>  不表示负数，因此第一位比特位到最后一位都可以用来表示数   </p><ul><li><p>逻辑运算<br>AND（乘法）  OR（加法）  NOT（取反）  XOR（取异）</p><h4 id="Signed"><a href="#Signed" class="headerlink" title="Signed"></a>Signed</h4><p>可以表示正数，负数，0， 第一位比特位表示正负号：0为正，1为负。<br>因此表示范围折半，以8比特为例，Unsigned表示的范围为0~255，而Signed表示的范围为-128~127（中间有0，故不是128）。   </p><ul><li>转换<br>转换为Signed的方法是先找到十进制绝对值对应的二进制数，取反码后+1得到二补码，即Signed Number。  </li><li>加减法<br>二补码的加法是<strong>异或运算</strong>，比特相同取0，不同取1。<br>减法看做是与负数相加。   </li></ul><p>加法要注意判断是否发生溢出，有两种思路可以判断是否发生了溢出。    </p></li></ul><ol><li>转换为十进制加减法，看是否发生了溢出</li><li>遵循两个原则：<br>a. 进位值的比特位数与数据的比特位数相同<br>b. 如果进位值的前两位是”01”或者是”10”，那么就发生了溢出。</li></ol><ul><li>乘法<br>类似于十进制乘法：第一行的所有位与第二行的每一个比特位分别相乘并作移位，最后相加。 单个比特位的乘法遵循：“除了$1×1=1$外，其余结果都为0。（与0相乘都为0。）”     </li></ul><h3 id="IEEE-754"><a href="#IEEE-754" class="headerlink" title="IEEE 754"></a>IEEE 754</h3><p>IEEE 754是用来表示二进制浮点数的标准，其中有32位，64位，128位表示方法，以32位为例：32位机对浮点数的表示方法为：</p><script type="math/tex; mode=display">\begin{array}{c}\text{Number of Bits} & 1 & 8 & 23 \\\text{Content}& Sign &Exponent &Fraction\end{array}bias: +127</script><ul><li>Bias<br>为了让Signed更方便的进行比较，将Signed转换成二进制后的指数部分人为地加一个Bias使得指数部分转变到Unsigned的范围内（0和255特殊处理）便于比较。<br>Bias的值规定是$2^n-1$，n为总的比特位数。<br>IEEE 754 能够表示0， 正规数（Exp部分不为0），非正规数（Exp部分为0，如果Exp小于0则把Exp划到0后，剩下的部分进入小数部分），无穷和其他未规定的计算结果（NaN）。   </li><li>十进制转IEEE754的方法<ol><li>确定正负</li><li>小数部分（$2^{-k}$）和整数部分（$2^{k}$）分别写出二进制形式，合并，中间以小数点隔开</li><li>将小数点移动到第一位和第二位末尾，假设移动距离为$x$，后面的指数部分写作$2^x$。</li><li>取现在的小数部分，并在末尾补0直到小数部分的总比特数为23位。  </li><li>指数部分$x+bias$，并转换为二进制填入Exp中。</li></ol></li></ul><h2 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h2><h3 id="分级存储"><a href="#分级存储" class="headerlink" title="分级存储"></a>分级存储</h3><p>由于CPU的速度受到内存读取速度的牵制，因此有必要将内存进行分级，读写速度越快的内存越靠近CPU，但是由于成本等原因，其储存空间越小。   将内存可以大致分为几层：  </p><ul><li>CPU内部的寄存器：用于存放临时数据，读写速度非常快，储存空间非常小，断电消失。  </li><li>CPU外部的存储（RAM）： 用于存放程序和数据，读写速度相对快，储存空间相对大，断电消失。  </li><li>外部的永久储存（硬盘）：读写速度慢，储存空间大，断电可以保存。  <h3 id="数据的组织"><a href="#数据的组织" class="headerlink" title="数据的组织"></a>数据的组织</h3><script type="math/tex; mode=display">\begin{array}{c}bit & 1  \\byte & 4  \\word & 8   \\Longword & 16 \end{array}</script>在主存储器中，每一个Byte都对应了一个地址，但是英特尔和摩托罗拉有两种不同的存放数据的方式，一种(Intel)是先存最低位（LSB），再存最高位(MSB)，另一种（摩托罗拉）是先存最高位（MSB），再存最低位（LSB）。<br>因此数据在两种机器间必须先要转换，才能存放再另一种机器中。  </li></ul><h3 id="程序与指令"><a href="#程序与指令" class="headerlink" title="程序与指令"></a>程序与指令</h3><ul><li>CPU中模块的连接方式<br>寄存器： 数据总线<br>ALU： 地址总线<br>控制单元(CU): 控制总线<h4 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h4>寄存器可以分为用户可见（数据寄存器，地址寄存器，条件寄存器 ）和不可见的两种寄存器。<br>状态和控制寄存器：<br>PC（指针）：指向下一条指令对应的地址<br>MAR（内存地址寄存器），MBR（内存缓冲寄存器）：将指令从内存放入缓冲区，协调CPU与存储的速度。<br>指令寄存器： 存放当前的指令  </li></ul><h4 id="指令的运行"><a href="#指令的运行" class="headerlink" title="指令的运行"></a>指令的运行</h4><ol><li>将PC指向的内容移到MAR,PC指向下一条指令</li><li>将MAR的内容移到MBR</li><li>将MBR的内容移到IR</li><li>将IR的控制内容交给CU，地址内容交给MAR</li><li>将MAR的内容移到MBR</li><li>ALU执行</li><li>ALU返回结果到指定内存<br>有两种执行指令的方式：   </li></ol><ul><li>硬件连接（Hardwiring）<br>一种指令对应一个硬件<br>优点：<br>快速，一个时钟内就能够解码一条指令</li></ul><ul><li>Micro-Programming<br>每条指令都转换成最原始的指令（Micro-instructions），对应了一个指定的门或者触发器等等，在CU中被执行。<br>因此Micro-Programming 是仅次于逻辑电路的第二层级，它也被称作固件，将软件与硬件连接起来。<br>优点：    <ol><li>可以创建任何复杂的指令或指令集</li><li>灵活，能够允许用户进行micro-program</li><li>有更高级的语言层</li></ol></li></ul><h2 id="栈和缓存"><a href="#栈和缓存" class="headerlink" title="栈和缓存"></a>栈和缓存</h2><h3 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h3><p>原则：后进先出<br>入栈（Push）： 将新节（node）放入栈顶<br>出栈（Pop）： 将栈顶节移除栈<br>溢出： 节数量超过了栈能够容纳的最大限度<br>优点： </p><ol><li>分配简单  </li><li>当函数退出的时候栈会自动回收    </li></ol><p>缺点：  </p><ol><li>有溢出可能</li><li>由于函数结束后栈会自动回收，因此如果有别的函数想要使用当前函数的返回值时，就必须要在当前函数执行结束前复制返回值以避免被回收。  </li></ol><p>使用栈结构与使用寄存器结构相比：   </p><ol><li>指令更小，因为操作对象（操作数）不需要指定地址</li><li>执行速度更慢，因为堆栈在外部内存中    </li></ol><h3 id="缓冲"><a href="#缓冲" class="headerlink" title="缓冲"></a>缓冲</h3><p>原则： 先进后出<br>溢出： 写速度大于读速度<br>空缓冲： 读速度大于写速度</p><h2 id="拓扑结构（物理）"><a href="#拓扑结构（物理）" class="headerlink" title="拓扑结构（物理）"></a>拓扑结构（物理）</h2><script type="math/tex; mode=display">\begin{array}{c}    类型 & 优点 & 缺点 \\    Liner & \text{1. 连接简单  2.用线短} & \text{1. 如果主线断所有都断 2. 主线断时难以查错 3. 两端需要端子} \\    Ring & \text{1.加入简单} &\text{1. 一处断，处处断} \\    Star* & \text{1. 连接简单 2.设备之间不会有干扰 3.简单查错 4.加入和移除新设备简单 } &\text{1. 需要更多的线 2. Hub断，所有断 3.成本高（Hub） } \\     Tree* & \text{1.点对点的连线} & \text{1.难以布线 2.主线断，处处断}\end{array}</script><h2 id="拓扑协议"><a href="#拓扑协议" class="headerlink" title="拓扑协议*"></a>拓扑协议*</h2><h3 id="以太网"><a href="#以太网" class="headerlink" title="以太网"></a>以太网</h3><p>如果网络没人广播，目标就广播。如果有人广播，目标等待其他人结束广播后广播。  </p><h3 id="本地会话"><a href="#本地会话" class="headerlink" title="本地会话"></a>本地会话</h3><p>如果目标需要传输和发送空令牌，数据就会附加到空令牌上，在Token Ring中传输直到找到接收者。   </p><h3 id="ATM"><a href="#ATM" class="headerlink" title="ATM"></a>ATM</h3><p>所有的数据以固定大小的小包传送，通常在两个局域网间使用。  </p><h2 id="I-O-接口"><a href="#I-O-接口" class="headerlink" title="I/O 接口"></a>I/O 接口</h2><p>I/O 接口的功能是为CPU和总线提供一个标准的借口。同时兼容各种I/O设备的借口需求，并且释放CPU对于I/O设备的管理。   </p><h3 id="I-O的控制方式"><a href="#I-O的控制方式" class="headerlink" title="I/O的控制方式"></a>I/O的控制方式</h3><ol><li>程序I/O<br>在I/O执行命令时，CPU一直等待并检测I/O是否完成任务，直到I/O执行完成。<br>特点：<br>程序I/O损失了CPU大部分的计算力，是一种低下的连接方式。   </li><li>中断I/O<br>CPU发出一个指令到I/O，之后做其他的任务直到I/O完成操作。 I/O完成操作后，会发出中断指令中断CPU当前的任务。 CPU处理完来自I/O的任务后，继续执行之前的任务。   </li><li>DMA<br>由于冯诺依曼架构中内存，I/O，存储都通过总线连接，使得数据在I/O 与内存之间能够直接传输数据成为可能。<br>整块数据在I/O与内存之间的传送在DMA控制器的控制下完成，仅在传输数据块的开始和结束时才需要CPU干预。<br>DMA会在CPU不使用总线时接管总线。    <h3 id="I-O接口的类型"><a href="#I-O接口的类型" class="headerlink" title="I/O接口的类型"></a>I/O接口的类型</h3>PCI，ATA，ISA，USB等等……</li></ol><h2 id="串行和并行连接"><a href="#串行和并行连接" class="headerlink" title="串行和并行连接"></a>串行和并行连接</h2><p>串行连接： 一比特一时钟发送数据<br>并行连接： 所有的比特会一起发送   </p><h3 id="串行连接的方式"><a href="#串行连接的方式" class="headerlink" title="串行连接的方式"></a>串行连接的方式</h3><ol><li>串行连接的时分复用<br>CPU在每一个单位时间内都处理不同并行通道内的一个数据包。  每个通道的前几个比特是用来与系统进行时间同步的。<br>例子： GSM（2G），TD-CMDA（3G）</li><li>Teletype<br>Teletype的一个数据包内通常包括： Start bit， Content， Stop Bit， Parity Bit。<br>串行需要信号去控制数据传输的暂停和恢复以适配不同速度的数据流。   <h3 id="硬件-UART"><a href="#硬件-UART" class="headerlink" title="硬件-UART"></a>硬件-UART</h3>UART 遵循先进先出原则，传输异步信号。   </li></ol><h3 id="并行连接"><a href="#并行连接" class="headerlink" title="并行连接"></a>并行连接</h3><p>并行连接往往需要不止一条数据线传输，因此体积会更大，同时数据传输更容易受到线长的限制。<br>并行连接一般有4种Pins： Data Pins, Control Pins, Status Pins, Ground Pins。      </p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><ol><li>通常串行传输比并行传输速度更快。   </li><li>串行传输不易出现Clock Skew（时钟信号在不同组件中到达的时间有差异）。</li><li>串行传输占用空间更少。</li><li>串行线路不容易受到周围线路的影响。</li><li>串行线路避免内Crosstalk（数据在传输时对另外的线路产生影响）。   </li><li>串行线路由于Pin数更少，因此更便宜。   </li></ol><h2 id="并行算法"><a href="#并行算法" class="headerlink" title="并行算法"></a>并行算法</h2><h3 id="并行算法的类型"><a href="#并行算法的类型" class="headerlink" title="并行算法的类型*"></a>并行算法的类型*</h3><ol><li>Pipline<br>在执行第一个任务的同时准备后面的任务。<br>所有的任务都不可能比整个任务链的处理速度更快，先前的任务必须要完成后才能完成之后的任务。 但是大多数的算法都不会有太长的计算链。   </li></ol><h3 id="并行计算"><a href="#并行计算" class="headerlink" title="并行计算"></a>并行计算</h3><p>并行计算体现在两点：  </p><ol><li>一个处理器执行时分复用（多线程）       </li><li>拥有不止一个处理器 （多核心）<br>弗莱因分类法将并行计算机按照指令和数据流分为四类：SISD，MISD，SIMD，MIMD。 即单/多指令，单/多数据流。<br>通常n个并行处理器比n倍快的单个处理器效率更慢，但是并行系统更便宜。    <h3 id="基本定律"><a href="#基本定律" class="headerlink" title="基本定律"></a>基本定律</h3>阿姆达尔定律: 理想中对整个系统最大的改进是对系统中的部分进行改进。<br>古斯塔夫森定律*:  $S_{latency}=1-p+sp$<br>$S_{latency}$： 整个任务执行延迟的理论加速。<br>$s$：受益于系统资源改善的部分的加速。<br>$p$：整个任务中，相对于改进前受益于改善后的部分所占的百分比。     </li></ol><h3 id="并行计算的层级"><a href="#并行计算的层级" class="headerlink" title="并行计算的层级"></a>并行计算的层级</h3><ol><li>比特层级<br>加倍字长。 部分需要两个字长才能运行的指令（需要两个时钟）变成一个字，进而在一个时钟内就能运行。     </li><li>指令层级<br>基础的分量级处理器只能在一个时钟内运行不到一个指令。<br>对这些指令重排，并整合成指令群。现代处理器还将pipline分成了多级，因此在一个时钟内可以运行一条指令。<br>超标量处理器有多个处理单元，因此可以在一个时钟内运行多条指令。但是指令来源于同一个指令流。多核处理器也可以在一个时钟内运行多条指令，但指令来源于多个指令流。      </li><li>任务层级<br>将任务分成多个子任务并交给不同的处理器运行。   </li></ol><h3 id="并行计算的例子"><a href="#并行计算的例子" class="headerlink" title="并行计算的例子*"></a>并行计算的例子*</h3><ol><li>分布式计算<br>与并行计算并无太多区别，可以理解为并行计算。   </li><li>网格计算<br>通过Internet将不同的计算机连接（需要中间件兼容），但是由于Internet的低带宽高延迟特性，因此性能往往不好。  </li><li>可重构计算（FGPA）<br>将一个可编程门</li><li>GPU</li><li>向量处理器</li></ol><h2 id="哈佛架构"><a href="#哈佛架构" class="headerlink" title="哈佛架构"></a>哈佛架构</h2><p>和冯诺依曼架构最大的不同是：内存被分为了两部分：Program Memory（静态内存）和 Data Memory（动态内存）。</p><h3 id="与冯诺依曼架构对比"><a href="#与冯诺依曼架构对比" class="headerlink" title="与冯诺依曼架构对比"></a>与冯诺依曼架构对比</h3><ol><li>哈佛架构的CPU可以在同一时间读取指令和数据（同时性），且指令和数据不会在fetch时竞争（独立性）。     </li><li>哈佛架构允许Program Memory 和 Data Memory的储存介质不同。    </li><li>哈佛架构允许像访问数据一样访问指令储存器的内容。</li><li>冯诺依曼架构中代码被视为数据，数据也被看作代码。 因此冯诺依曼架构允许从硬盘中读取和执行程序。<br>现代的处理器通常是哈佛架构和冯诺依曼架构的混合架构，通常被视作是改进的哈佛架构。   如：x86，ARM， PowerPC。     </li></ol><h2 id="验证码"><a href="#验证码" class="headerlink" title="验证码"></a>验证码</h2><h3 id="奇偶校验码"><a href="#奇偶校验码" class="headerlink" title="奇偶校验码"></a>奇偶校验码</h3><p>在数据包后加上一个校验位，使得所有的比特位中1的总数为奇数/偶数。<br>缺点： 无法探测错误的位置。 对于两位以上的错误，无法矫正错误。   </p><h3 id="汉明码"><a href="#汉明码" class="headerlink" title="汉明码"></a>汉明码</h3><p>在一串$d$位数据的$2^n$位上加入校验位，校验位的数量$c$满足：$2^c-1&gt;=d$。<br>第$n$个校验码的校验条件满足： 从第$2^n$位开始，使用偶校验检测$2^n$位，再跳过$2^n$位，再检测$2^n$位，重复直到检测最后一位。<br>缺点： 当只有一个校验码出错时，有50%的概率是数据出错，有50%的概率是校验码自己出错，无法确定。  有两个比特出错时可以检验，但无法校正。  </p><h3 id="前向纠错码"><a href="#前向纠错码" class="headerlink" title="前向纠错码"></a>前向纠错码</h3><p>前向纠错是一种差错控制方式，它是指信号在被送入传输信道之前预先按一定的算法进行编码处理，加入带有信号本身特征的冗码，在接收端按照相应算法对接收到的信号进行解码，从而找出在传输过程中产生的错误码并将其纠正的技术。<br>前向纠错码分为两种：卷积码（一个比特一个比特地处理）和块码（一个块一个块地处理）。 块纠错码的代表是里得-所罗门码。   </p><h3 id="循环冗余校验"><a href="#循环冗余校验" class="headerlink" title="循环冗余校验"></a>循环冗余校验</h3><p>循环冗余校验是利用里得-所罗门码进行校验的一种方式，具体是将整个数据包的每一位转换成多项式的系数（值）和次数（位置）。在伽罗瓦域中以输入值作为被除数做多项式除法，得到的余数作为校验的结果。<br>循环冗余校验不能检测恶意插入的错误。   </p><h3 id="哈希函数"><a href="#哈希函数" class="headerlink" title="哈希函数"></a>哈希函数</h3><p>将原数据输入哈希函数，得到的将是一个哈希值序列的信息摘要。对原数据任何的更改都会导致哈希值序列发生改变，因此可以检测恶意或偶然出现的错误。     </p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>Internet—和校验<br>数字卫星电视信号—循环冗余校验<br>硬盘—循环冗余校验<br>条形码<br>快速响应矩阵（QR二维码）—循环冗余校验     </p><h2 id="CPU的类型"><a href="#CPU的类型" class="headerlink" title="CPU的类型"></a>CPU的类型</h2><p>CPU按照指令集的复杂程度可以分为5类架构：<br>Complex/ Reduced/ Minimal/ One/ Zero Instruction Set Computer  </p><h3 id="CISC"><a href="#CISC" class="headerlink" title="CISC"></a>CISC</h3><p>每一条指令都能够运行一些复杂的低层级的操作，比如算术运算或者从内存加载等等，所有的操作都在一条指令内被执行。<br>优点： 一些复杂或者高级的操作（例如循环）能够直接被整合为一条指令。<br>缺点： 从简单的指令加载复杂的操作并不一定能够提高电脑的性能。<br>常见的CISC处理器架构： x86<br>现代x86处理器能够解码并将指令划分到动态的缓冲微操作中，以便能够在单线程中运行更大的负指令集，同时精简了并行计算。   </p><h3 id="RISC"><a href="#RISC" class="headerlink" title="RISC"></a>RISC</h3><p>RISC 的设计遵循： 如果简化的指令集能够使得每条指令更快的运行，那么简化的指令集就能提高性能。<br>特点：     </p><ol><li>基本上大多数的指令都有相似的结构和固定的长度。通常为一个字长加上固定比特位的操作码。   </li><li>寄存器大多相似且通用（浮点寄存器除外），能够简化编译设计。   </li><li>简单的地址模型。  </li><li>硬件支持的数据类型更少。<br>优点：有更好的pipeline stagesstages 和更快的时钟频率 因此更高效。<br>常见的RISC处理器架构： ARM 和 PowerPC     </li></ol><h3 id="MISC"><a href="#MISC" class="headerlink" title="MISC"></a>MISC</h3><p>处理器有很少的基本操作和对应的操作码，通常基于栈结构。<br>优点： 指令和解码单元更小，因此单个指令的运行速度更快。<br>缺点： 指令更依赖于串行结构，减少了并行计算。<br>常见的MISC处理器架构： INMOS</p><h3 id="OISC-和-ZISC"><a href="#OISC-和-ZISC" class="headerlink" title="OISC 和 ZISC"></a>OISC 和 ZISC</h3><p>OISC： 只使用一条指令，不需要操作码。<br>ZISC： 基于纯模式匹配而不需要微指令。   </p><h2 id="特殊化处理器"><a href="#特殊化处理器" class="headerlink" title="特殊化处理器*"></a>特殊化处理器*</h2><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><p>GPU是一种用于加速建图和图像处理的专门化处理器。 GPU内部高度并行化。<br>功能： 加速渲染多边形和纹理匹配， 解码高清视频等等。  </p><h3 id="DSP"><a href="#DSP" class="headerlink" title="DSP"></a>DSP</h3><p>专门化的数字信号处理器，用于声音、图像、雷达系统。<br>功能： 数字信号与模拟信号的转换和处理。<br>DSP 的指令集优化用于处理数学运算、特殊的地址模型、特殊化的循环控制。<br>DSP 中的数学计算常常是Fixed-point 计算。<br>DSP 不支持虚拟内存和内存保护。   </p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
      <category>计算机结构与接口</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>6.1 性能评估</title>
    <link href="/2021/03/02/6.%20%E8%AF%8A%E6%96%AD%E4%B8%8E%E8%B0%83%E8%AF%95/6.1.%20%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0/"/>
    <url>/2021/03/02/6.%20%E8%AF%8A%E6%96%AD%E4%B8%8E%E8%B0%83%E8%AF%95/6.1.%20%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="性能评估"><a href="#性能评估" class="headerlink" title="性能评估"></a>性能评估</h1><h2 id="下一步做什么"><a href="#下一步做什么" class="headerlink" title="下一步做什么"></a>下一步做什么</h2><p>从第一章到现在，我们已经学习了许多中机器学习的方法。但是在面对如今眼花缭乱的算法时，应当如何选择最合适的算法来对数据集进行学习并改进这个算法？<br>思考如下的例子：<br>假设已经用波士顿房价数据集得到了线性回归的代价函数：   </p><script type="math/tex; mode=display">J_{\theta}=\frac{1}{2m}[\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+λ\Sigma_{j=1}^{m}\theta_j^2]</script><p>然而使用这个模型对一个新的房价数据集进行预测的时候，发现误差非常的大，如何改进这个算法的性能？<br>有如下几种解决思路：   </p><ol><li>获得更多的数据集</li><li>选用更少的特征以防止过拟合</li><li>获得更多的特征来补充特征集</li><li>增加多项式特征</li><li>增加或减小正则化参数$λ$    </li></ol><p>很多人从如上的解决思路中随机地选择几项对人工智能进行优化，然而对于某一种人工智能算法，上述的解决思路不一定每一种都有效。运用本章讲的一些技巧后，能够对如上的解决思路中每一项的有效性进行判别，从而更高效地调试神经网络。<br>下面将介绍两种评估机器学习性能的方法，它们被称为<strong>机器学习诊断法</strong>(Machine learning diagnostic)。通过运行这些方法，人们可以了解算法在哪里出现了问题，也能告诉人们做什么样的改进尝试才是有意义的。    </p><h2 id="评估假设"><a href="#评估假设" class="headerlink" title="评估假设"></a>评估假设</h2><p>在3.3.中已经介绍过了过拟合现象，因此单纯地得到预测值和标签的距离很小并不能说明这个假设模型是一个好的模型。<br>那么有什么方法能够排除过拟合地评估假设的性能呢？   </p><p>答案是划分测试集和训练集， 具体而言：将数据集划分(Split)为测试集和训练集（通常是以3：7的比例随机选择（Shuffle）），将训练集中的样本进行机器学习，测试集进行验证。<br>从训练集进行线性回归得到最适合的参数$J(θ)$，再将测试集带入其中：   </p><script type="math/tex; mode=display">J_{test}(\theta)=\frac{1}{2m_{test}}[\Sigma_{i=1}^{m_{test}}(h_θ(x_{test}^{(i)})-y_{test}^{(i)})^2+λ\Sigma_{j=1}^{m_{test}}\theta_j^2]</script><p>回归分类中对数据集的划分和验证类型大致同上，只是$J_{test}(\theta)$的形式略有差别。<br>对于回归分类还有另一形式的测试度量叫做<strong>错误分类</strong>(Misclassification Error/ 0-1 misclassification error)。<br>定义预测值和标签的误差$err(h_θ(x),y)$，有：   </p><script type="math/tex; mode=display">err(h_θ(x),y)= \begin{cases}    1  \text{   if     }   h_θ ≥ 0.5, y=0 \text{   or     } h_θ ≤ 0.5, y=1 \\    0  \text{   if     }   h_θ ≥ 0.5, y=1 \text{   or     } h_θ ≤ 0.5, y=0\end{cases}</script><p>定义测试集的误差：</p><script type="math/tex; mode=display">TestError=\frac{1}{m_{test}}∑_{i=1}^{m_{test}}err(h_θ(x^{(i)}_{test}),y^{(i)})</script><h2 id="训练集，测试集和验证集"><a href="#训练集，测试集和验证集" class="headerlink" title="训练集，测试集和验证集"></a>训练集，测试集和验证集</h2><p>如果要从如下的多项式中选择一个作为假设模型：  </p><script type="math/tex; mode=display">h_θ(x)=θ_0+θ_1x</script><script type="math/tex; mode=display">h_θ(x)=θ_0+θ_1x+θ_2x^2</script><script type="math/tex; mode=display">...</script><script type="math/tex; mode=display">h_θ(x)=θ_0+θ_1x+θ_2x^2+...+θ_nx^n</script><p>设$d$表示多项式中$x$的最大次数，要测试它们对于样本的泛化能力（即过拟合的程度，泛化能力低意味着模型过拟合，模型能够很好的拟合当前的数据集，但是对新的数据并不敏感），最简单的思路是可以对每一个模型都投入数据集，然后得到最优化的一组向量$Θ^{(d)}$，并从测试集求出每一个$J_{test}(Θ^{(d)})$，然后看哪一个模型的$J_{test}(Θ^{(d)})$最小。  但是由于我们用测试集拟合了$d$，并选择了一个最好的$d$，因此$Θ^{(d)}$很可能是对泛化误差的乐观假设。<br>解决办法是对一个数据集分为三个部分：训练集，测试集，和<strong>交叉验证集</strong>（以CV表示）。通常的比例是60%作为训练集，20%作为测试集，20%作为交叉验证集。<br>那么定义训练误差，测试误差，验证误差分别为：   </p><script type="math/tex; mode=display">J_{train}(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2</script><script type="math/tex; mode=display">J_{test}(\theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_θ(x_{test}^{(i)})-y_{test}^{(i)})^2</script><script type="math/tex; mode=display">J_{cv}(\theta)=\frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}(h_θ(x_{cv}^{(i)})-y_{cv}^{(i)})^2</script><p>现在使用验证集来选择模型：<br>同样地，每一个模型都投入数据集，然后得到最优化的一组向量$Θ^{(d)}$，但是将这些Θ投入验证集并使用交叉验证得到一系列的$J_{cv}(\theta^{(d)})$,找到最合适的$d$，再放入测试集中运行。  </p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>6. 诊断与调试</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>5.3. 回顾：神经网络的实现与梯度下降算法</title>
    <link href="/2021/03/02/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.3.%20%E6%95%B4%E5%90%88%E5%88%B0%E4%B8%80%E8%B5%B7/"/>
    <url>/2021/03/02/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.3.%20%E6%95%B4%E5%90%88%E5%88%B0%E4%B8%80%E8%B5%B7/</url>
    
    <content type="html"><![CDATA[<h1 id="回顾：神经网络的实现与梯度下降算法"><a href="#回顾：神经网络的实现与梯度下降算法" class="headerlink" title="回顾：神经网络的实现与梯度下降算法"></a>回顾：神经网络的实现与梯度下降算法</h1><h2 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h2><h3 id="选择神经网络的架构"><a href="#选择神经网络的架构" class="headerlink" title="选择神经网络的架构"></a>选择神经网络的架构</h3><p>即选择神经元之间的连接模式，和神经网络的层数，每一层的单元数。   </p><ul><li>输出和输入单元<br>输入单元的数目由分类问题中要区分的类别个数，即特征的维度数量所确定。<br>注意：多元分类问题中输出单元应该是一个多维的向量，对应的维度为1。</li><li>隐藏层<br>通常只有一层隐藏层；如果选择构建多个隐藏层，通常情况下每一个隐藏层中的单元数都是相同的。<br>单元数越多越好，但是隐藏单元数的增加会导致计算量的增大。因此每一个隐藏层中隐藏单元的数目通常与输入层的维度，即特征的数目相匹配（是特征数目的整数倍$k=1,2,3…$）。</li></ul><h3 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h3><ol><li>随机初始化权重，通常初始化为接近于0的值。</li><li>执行前向传播算法，得到$h_θ(x^{(i)})$的值。</li><li>计算代价/损失函数$J(Θ)$。</li><li>执行方向传播算法来计算$\frac{∂}{∂Θ_{jk}^{(l)}}J(Θ)$具体执行方法是用一个循环<code>for i = 1:m</code>对每一个样本执行前向传播和反向传播算法，得到每一个单元的激励值$a^{(l)}$和误差$δ^{(l)}$。</li><li>使用梯度检查，将反向传播算法得到的$\frac{∂}{∂Θ_{jk}^{(l)}}J(Θ)$与用数值近似得到的$J(Θ)$的梯度进行比较，确定两个值是接近的。</li><li>停用梯度检查。  </li><li>用梯度下降算法或者其他的一些高级的优化方法与反向传播算法结合，并最小化$J(Θ)$的$Θ$。  </li></ol><h2 id="梯度下降算法在神经网络中的应用"><a href="#梯度下降算法在神经网络中的应用" class="headerlink" title="梯度下降算法在神经网络中的应用"></a>梯度下降算法在神经网络中的应用</h2><p><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210302114441.png" alt=""><br>如图所示的参数与$J(Θ)$的关系中（图中只有两个参数），图中每一点的高度表示了$J(Θ)$的值，也代表了在该点的参数取值下，预测值$h_Θ(x^{(i)})$与实际标签$y^{(i)}$的差距。<br>同之前一样，梯度下降算法从随机的一点开始求这一点的梯度（即下降的最快方向），然后沿着梯度方向持续下降，直到得到局部最优点。</p><h2 id="案例：-ALVINN无人驾驶转向"><a href="#案例：-ALVINN无人驾驶转向" class="headerlink" title="案例： ALVINN无人驾驶转向"></a>案例： ALVINN无人驾驶转向</h2><p>Dean Pomerieau使用三层神经网络ALVINN来训练计算机进行无人驾驶。<br>将汽车转向进行量化，左急转和右急转分别对应了坐标轴上仅有的两个极值点。 每隔两秒，ALVINN就会生成一张前方的路况图，并记录驾驶者的行驶方向，在最开始ALVINN的转向是随机的，通过训练路况图和行驶方向的关系，ALVINN最终做出的转向决定与人类驾驶员的转向基本相同。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>5. 神经网络拟合</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>5. 课后练习-使用更多的分类器</title>
    <link href="/2021/02/27/Machine%20Learning-NAU/5.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-%E6%89%8B%E5%86%99%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/"/>
    <url>/2021/02/27/Machine%20Learning-NAU/5.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-%E6%89%8B%E5%86%99%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h1 id="课后练习-5"><a href="#课后练习-5" class="headerlink" title="课后练习 5"></a>课后练习 5</h1><h2 id="Tasks"><a href="#Tasks" class="headerlink" title="Tasks:"></a>Tasks:</h2><ul><li>Study k-Nearest Neighbours classifiers sklearn.neighbors.KNeighborsClassifier — scikit-learn<br>0.24.1 documentation (scikit-learn.org)</li><li>Study RandomForrest classifiers sklearn.ensemble.RandomForestClassifier — scikit-learn<br>0.24.1 documentation (scikit-learn.org)</li><li>Study Naïve Bayes classifiers 1.9. Naive Bayes — scikit-learn 0.24.1 documentation (scikitlearn.org)<h2 id="Programming-exercise"><a href="#Programming-exercise" class="headerlink" title="Programming exercise:"></a>Programming exercise:</h2>This tutorial will use the MNIST dataset which was explored in tutorial 3.<h3 id="Q1-Train-a-k-Nearest-Neighbours-classifier-for-handwritten-digit-recognition-with-MNIST-dataset"><a href="#Q1-Train-a-k-Nearest-Neighbours-classifier-for-handwritten-digit-recognition-with-MNIST-dataset" class="headerlink" title="Q1. Train a k-Nearest Neighbours classifier for handwritten digit recognition with MNIST dataset."></a>Q1. Train a k-Nearest Neighbours classifier for handwritten digit recognition with MNIST dataset.</h3>Try different parameter settings and study how the performance varies.</li><li>Plot the accuracy vs k while changing the number of neighbours (k) with values [1, 3, 5, 7, 9]<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><br>digits = datasets.load_digits()<br>labels = digits.target<br><br>data = images.reshape(<span class="hljs-built_in">len</span>(images), -<span class="hljs-number">1</span>)<br>x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=<span class="hljs-number">0.2</span>, shuffle=<span class="hljs-literal">False</span>)<br>g = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]<br>accurancy = []<br><span class="hljs-keyword">for</span> g_ <span class="hljs-keyword">in</span> g:<br>    clf = KNeighborsClassifier(n_neighbors = g)<br>    clf.fit(x_train, y_train)<br>    acc = clf.predict(x_test, y_test)<br>    accurancy.append(acc)<br><br>plt.plot(g, accurancy)<br>plt.show()<br></code></pre></div></td></tr></table></figure><h3 id="Q2-Train-a-RandomForrest-classifier-for-handwritten-digit-recognition-with-MNIST-dataset"><a href="#Q2-Train-a-RandomForrest-classifier-for-handwritten-digit-recognition-with-MNIST-dataset" class="headerlink" title="Q2. Train a RandomForrest classifier for handwritten digit recognition with MNIST dataset."></a>Q2. Train a RandomForrest classifier for handwritten digit recognition with MNIST dataset.</h3>Try different parameter settings and study how the performance varies.</li><li>Plot the accuracy vs max_depth while changing the max depth parameter with values [1, 2, 4, 8, 16]<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForrest<br><br>digits = datasets.load_digits()<br>labels = digits.target<br><br>data = images.reshape(<span class="hljs-built_in">len</span>(images), -<span class="hljs-number">1</span>)<br>x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=<span class="hljs-number">0.2</span>, shuffle=<span class="hljs-literal">False</span>)<br>g = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">16</span>]<br>accurancy = []<br><span class="hljs-keyword">for</span> g_ <span class="hljs-keyword">in</span> g:<br>    clf = RandomForrest(max_depth = g)<br>    clf.fit(x_train, y_train)<br>    acc = clf.predict(x_test, y_test)<br>    accurancy.append(acc)<br>plt.plot(g, accurancy)<br>plt.show()<br></code></pre></div></td></tr></table></figure><h3 id="Q3-Train-a-Gaussian-Naive-Bayes-classifier-for-handwritten-digit-recognition-with-the-MNIST-dataset"><a href="#Q3-Train-a-Gaussian-Naive-Bayes-classifier-for-handwritten-digit-recognition-with-the-MNIST-dataset" class="headerlink" title="Q3. Train a Gaussian Naive Bayes classifier for handwritten digit recognition with the MNIST dataset."></a>Q3. Train a Gaussian Naive Bayes classifier for handwritten digit recognition with the MNIST dataset.</h3><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForrest<br><br>digits = datasets.load_digits()<br>labels = digits.target<br><br>data = images.reshape(<span class="hljs-built_in">len</span>(images), -<span class="hljs-number">1</span>)<br>x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=<span class="hljs-number">0.2</span>, shuffle=<span class="hljs-literal">False</span>)<br>clf1 = sk_bayes.BernoulliNB(alpha=<span class="hljs-number">1.0</span>,<br>                            binarize=<span class="hljs-number">0.0</span>,<br>                            fit_prior=<span class="hljs-literal">True</span>,<br>                            class_prior=<span class="hljs-literal">None</span>)<br>clf1.fit(x_train, y_train)  <br>acc_BN = clf1.score(x_test, y_test) <br>acc.append(acc_BN)<br></code></pre></div></td></tr></table></figure></li><li>Plus： Displaying the wrong images  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-comment"># 显示错误的图片</span><br>clf = RandomForrest(max_depth = g)<br>clf.fit(x_train, y_train)<br>predictions = clf.predict(x_test)<br><span class="hljs-comment"># clf.predict_proba() 显示每张图有多少概率是哪个标签</span><br>print(predictions) <span class="hljs-comment"># 这样会输出所有图片的预测标签</span><br>print(y_test)<br></code></pre></div></td></tr></table></figure><h3 id="Q4-Do-a-comparison-between-the-four-classifiers-SVM-–-Tutorial-3-kNN-RandomForrest-and-NaiveBayes-by-plotting-the-best-performing-accuracy-value-for-each-classifier-in-a-bar-chart"><a href="#Q4-Do-a-comparison-between-the-four-classifiers-SVM-–-Tutorial-3-kNN-RandomForrest-and-NaiveBayes-by-plotting-the-best-performing-accuracy-value-for-each-classifier-in-a-bar-chart" class="headerlink" title="Q4. Do a comparison between the four classifiers (SVM – Tutorial 3, kNN, RandomForrest and NaïveBayes) by plotting the best performing accuracy value for each classifier in a bar chart."></a>Q4. Do a comparison between the four classifiers (SVM – Tutorial 3, kNN, RandomForrest and NaïveBayes) by plotting the best performing accuracy value for each classifier in a bar chart.</h3></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>课后练习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>5. 特征</title>
    <link href="/2021/02/23/Machine%20Learning-NAU/5.%20%E7%89%B9%E5%BE%81/"/>
    <url>/2021/02/23/Machine%20Learning-NAU/5.%20%E7%89%B9%E5%BE%81/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 60%;<br>    padding-left: 30%;<br>}</style></p><h1 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h1><h2 id="特征的选择"><a href="#特征的选择" class="headerlink" title="特征的选择"></a>特征的选择</h2><p>曾在第四讲中提到过特征的选择，特征的选取可以从颜色、形状、直方图等等来提取。<br>好的特征应该具有如下的性质：  </p><ul><li>计算简便</li><li>鲁棒性</li><li>储存小</li><li>好的区分度</li><li>更优的距离度量    <h3 id="NP-hard！"><a href="#NP-hard！" class="headerlink" title="NP hard！"></a>NP hard！</h3>尝试试所有的特征组合是一种在直觉上认为的简便方案，它是一种非确定性多项式(Non-Deterministic polynomial Hard，NP hard)问题，该问题在理论上能够用一种称为贪心算法(Greedy approach)的方法尝试解决：     <ul><li>对于F 个可能的特征，选择其中能给出最高准确率的一个特征X</li><li>对于剩下的 F-1个可能的特征，选择与特征X组合能够给出最高准确率的一个特征。</li><li>重复上述过程，直到选择出所有的特征。  </li></ul></li></ul><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>主成分分析（Principal Component Analysis，PCA）是另一种选择出更好的一组特征的方法。<br>设$y ∈ R^k$是图像(或者上一级的特征向量)$x ∈ R^d$的特征向量，$k&lt;&lt;d$，有：   </p><script type="math/tex; mode=display">y=W^Tx</script><ul><li><p>$W$<br>$W$是一个$d × k$的正交矩阵，由x计算得到。<br>设均值矩阵$E[x]=0$,协方差矩阵$E[xx^T]=C_x$,如下是计算$W$的方法：<br>设回复向量（recovered vector）$x_r=Wy$,误差$ε =x-x_r=x-WW^Tx$,</p><script type="math/tex; mode=display">\begin{aligned}    |ϵ|^2 & = ϵ^Tϵ \\      & =(x-WW^TX)^T(X-WW^Tx) \\    & =x^Tx-x^TWW^Tx-x^TWW^Tx+x^TWW^TWW^Tx \\    & =x^Tx-x^TWW^Tx\end{aligned}</script><p>令$k=1$,此时$W$是一个向量:</p><script type="math/tex; mode=display">\begin{aligned}  E[ ϵ^Tϵ] & =E[x^Tx-x^Tww^Tx]\\  & =E[x^Tx]-W^TC_xW\end{aligned}</script><p>那么需要找到使得$E[ ϵ^Tϵ]$最小的$w$，这一步与求$max_w w^TC_xw$等价。<br>设$J=\frac{w^TC_xw}{W^TW}$以正规化W：<br>令$\frac{dJ}{dw}=0$，得到：</p><script type="math/tex; mode=display">\frac{2C_xw}{w^Tw}-[\frac{w^TC_xw}{w^Tw}]·\frac{2w}{w^Tw}=0</script><script type="math/tex; mode=display">C_xw=Jw</script><p>这个公式正好是协方差矩阵$C_x$的特征值方程公式，即:$w$为协方差矩阵$C_x$的特征向量，$J$是它的特征值。    </p><p>如果对$C_x$进行特征分解(Eigen composition),得到一组特征值，将特征值从大到小排序，从中选取k个最大的特征值所对应的特征向量，组成矩阵$W$。  </p><script type="math/tex; mode=display">W=[w_1 |w_2 |...|w_k]</script><p>w称为主成分，所有的主成分两两正交。<br>最终得到的PCA是：  </p><script type="math/tex; mode=display">y=W^T(x-E[x])</script><p>PCA将依据数据的分布，改变样本空间的坐标原点和坐标轴（表示原来的特征），并将所有的坐标轴变更为主成分，如图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210222144956.png" alt=""><br>y事实上是x的压缩集，改变后的y中的元素都是不相关的。   </p></li></ul><p>整个PCA过程中，k的选取十分的重要，通常k的选取遵循如下的规则：<br>记 $λ$是$C_x$特征分解后得到的从大到小特征值，有:    </p><script type="math/tex; mode=display">\frac{∑^kλ_i}{∑^dλ_i}≈90\%</script><p>即前k个特征值的和大约是总的特征值和的0.9左右。   </p><ul><li>案例<br>1990年 Turk 和 Pentland应用PCA算法对人脸图像进行处理，称为特征子脸技术。特征子脸技术的基本思想是：从统计的观点，寻找人脸图像分布的基本元素，即人脸图像样本集协方差矩阵的特征向量，以此近似地表征人脸图像。这些特征向量称为特征脸(Eigenface)。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210222150404.png" alt="">   </li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>讲义</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>7. 总结</title>
    <link href="/2021/02/23/Machine%20Learning-NAU/7.%20%E6%80%BB%E7%BB%93/"/>
    <url>/2021/02/23/Machine%20Learning-NAU/7.%20%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>分类是将标签赋予给输入图像的过程。</li><li>选择特征是一门艺术。   </li><li>选择好的特征能够让分类器更好的工作。（有些分类器需要很长的工作时间。）</li><li>用测试集对分类器进行性能评估是重要的一步。   </li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>讲义</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>6. 人脸识别与感知机</title>
    <link href="/2021/02/23/Machine%20Learning-NAU/6.%20%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B8%8E%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <url>/2021/02/23/Machine%20Learning-NAU/6.%20%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B8%8E%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="人脸识别与感知机"><a href="#人脸识别与感知机" class="headerlink" title="人脸识别与感知机"></a>人脸识别与感知机</h1><h2 id="早期人脸识别技术"><a href="#早期人脸识别技术" class="headerlink" title="早期人脸识别技术"></a>早期人脸识别技术</h2><p>最早的人脸识别技术由Sung Kah Kay (MIT), Henry Rowley (CMU)运用ANN识别得来，方法大致为：</p><ul><li>将有人脸图像分割为$20 × 20 px$的矩阵块</li><li>对每个块运用亮度矫正和直方图均衡</li><li>放入神经网络学习，检测每一个矩阵块是否为人脸，如果不是，则平移矩阵块，并不断缩小矩阵的大小，再次检测。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210223201717.png" alt="">   </li></ul><h3 id="Viola-Jones-人脸检测方法"><a href="#Viola-Jones-人脸检测方法" class="headerlink" title="Viola-Jones 人脸检测方法"></a>Viola-Jones 人脸检测方法</h3><p><strong>Viola-Jones 人脸检测方法</strong> （Viola Jones Face detection）是Paul viola 和 Michael J Jones共同提出的一种人脸检测框架。它极大的提高了人脸检测的速度和准确率。 目前的人脸识别设备大多采用这种方法。<br>这种方法主要提取了4类最基本的特征。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210223202823.png" alt=""><br>右图： 特征A主要是用于检测双眼，特征C主要用于检测额头的部位。   </p><ul><li>特征的加强与感知机<br>Viola-Jones 人脸检测方法使用了加强的特征，具体方法为：   <ol><li>称一个简单的分类器叫做<strong>单特征感知机</strong>（single-feature perceptron），对每一种特征都使用一种不同的单特征感知机，得到加权的一些数据，如果有K个特征，就要用K个不同的单特征感知机。  </li><li>对之前获得的数据进行加权。  </li><li>训练所有的K个单特征感知器。  </li><li>在此阶段选择一个最佳分类器。  </li><li>与先前选择的其他分类器组合。  </li><li>重新加权新得到的所有数据。</li><li>再次学习所有K个分类器，选择最佳分类器，合并，重新加权。</li><li>重复执行，直到选择了T个分类器。   </li></ol></li></ul><p>在当时，运用这种方法训练5k张人脸和9.5k张非人脸图像花费了一周的时间，最后的结果是一个38层的感知机，它提取了6060个特征。    </p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>讲义</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2. 课后练习-数学方法</title>
    <link href="/2021/02/23/Machine%20Learning-NAU/2.%20a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/"/>
    <url>/2021/02/23/Machine%20Learning-NAU/2.%20a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="Math-Fundamentals-PDF"><a href="#Math-Fundamentals-PDF" class="headerlink" title="Math Fundamentals(PDF)"></a>Math Fundamentals(PDF)</h1><p><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/6059DD76E50982B30ABBD29933ACF908.png" alt=""><br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/Page1(1" alt="">.jpg)<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/EFA4476CFD3B735B19412D28A35D1BC5.png" alt="">     </p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>课后练习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>6. 大作业-创建一个交通标志分类器</title>
    <link href="/2021/02/23/Machine%20Learning-NAU/6.%20%E6%9C%80%E5%90%8E%E5%A4%A7%E4%BD%9C%E4%B8%9A/"/>
    <url>/2021/02/23/Machine%20Learning-NAU/6.%20%E6%9C%80%E5%90%8E%E5%A4%A7%E4%BD%9C%E4%B8%9A/</url>
    
    <content type="html"><![CDATA[<h1 id="Traffic-Sign-Recognition"><a href="#Traffic-Sign-Recognition" class="headerlink" title="Traffic Sign Recognition"></a>Traffic Sign Recognition</h1><p>Traffic-sign recognition (TSR) is a technology by which a vehicle is able to recognize the traffic signs<br>put on the road e.g. “speed limit” or “children” or “turn ahead”. This is a very important technology<br>in self-driving cars.<br>This project will give you the chance to train different models using various features to classify traffic<br>signs.<br>The project is divided into 3 difficulty levels. Beginner, Expert and Bonus.   </p><h2 id="Beginner-Level"><a href="#Beginner-Level" class="headerlink" title="Beginner Level"></a>Beginner Level</h2><p>For this level we use the Chinese Traffic Sign Database (Traffic Sign Recogntion Database (ia.ac.cn)).<br>This is available as a zip file in your project folder under the name “Dataset_1.zip”.<br>This dataset consists of 5998 images belonging to 58 classes. Each image is named “XXX_yyyy.png”.<br>Here XXX represent the class (traffic sign type) and yyyy represents the image number within each<br>class.<br>For the beginner level, we make use of the “starter.py” code, which you can find in the project<br>directory.<br>Follow along with the tasks and fill in the blanks of the given code to complete beginner level.<br>Follow the tasks with “starter.py” and fill in the missing code for each section.</p><h3 id="T1-Reading-images"><a href="#T1-Reading-images" class="headerlink" title="T1: Reading images."></a>T1: Reading images.</h3><ul><li>Change the dataset_path to point to the unzipped Dataset_1/images folder in your<br>computer.</li><li>The given loop will go through all the files in the folder, variable i gives each file name.</li><li>Complete the code to read the images and append them to list X</li><li>The labels for each image has been already appended to list y for you<br>At the end of T1, you should have X, y with 5998 entries on each.<h3 id="T2-Pre-processing-images"><a href="#T2-Pre-processing-images" class="headerlink" title="T2: Pre-processing images."></a>T2: Pre-processing images.</h3></li><li>Given loop will go through all images in X and resize them to 48x48 pixels.</li><li>Complete the code to convert the images to grayscale. (Hint: use the cvtColor function in<br>opencv)</li><li>Complete the code to append the pre-processed images to X_processed list.<br>At the end of T2, you should have X_processed with 5998 entires of resized and grayscale images.<br>T3: Calculating Features and Splitting train/test sets.</li><li>Install skimage using anaconda. (you can follow the same instructions given for installing<br>sklearn with the package name “scikit-image”)</li><li>The given code will use skimage and extract hog features for you.</li><li>Write code to split X_features and y into training and testing sets. Make use of the<br>“sklearn.model_selection.train_test_split” to do this. Use a 80-20 split and make sure to<br>shuffle the samples.<br>At the end of T3, you should have x_train, x_test, y_train and y_test. Training sets should have 4798<br>samples and the test sets should have 1200 samples.<br>T4: Training and testing the classifier.</li><li>Use the sklearn SVM package to train a classifier using x_train and y_train.</li><li>Use the x_test and y_test to evaluate the classifier and print the accuracy value.<h2 id="Expert-Level"><a href="#Expert-Level" class="headerlink" title="Expert Level:"></a>Expert Level:</h2>We will build upon the beginner level code to try out different techniques and improve our model.<br>The same dataset will be used here.<br>Complete the following tasks,<h3 id="T1-Different-pre-processing-techniques"><a href="#T1-Different-pre-processing-techniques" class="headerlink" title="T1: Different pre-processing techniques"></a>T1: Different pre-processing techniques</h3>What are other pre-processing steps you can use?<br>Examples: Keep 3 channels (RGB), add a gaussian blur to reduce noise, etc.<br>Try few other pre-processing techniques and evaluate how they affect accuracy<h3 id="T2-Different-features"><a href="#T2-Different-features" class="headerlink" title="T2: Different features"></a>T2: Different features</h3>What are other feature extraction methods you can use?<br>Explore some other feature extraction methods given in skimage (Module: feature —<br>skimage v0.19.0.dev0 docs (scikit-image.org))<br>Try few other feature extraction methods and evaluate how they affect accuracy, you can<br>also try different packages here (no need to stick with skimage)<h3 id="T3-Different-Classification-Models"><a href="#T3-Different-Classification-Models" class="headerlink" title="T3: Different Classification Models"></a>T3: Different Classification Models</h3>What are other classification models you can use?<br>Try other classifiers including but not limited to; RandomForrest, kNN and Decision Tree.<br>For each classifier, change parameters and evaluate how the parameters affect accuracy.<h2 id="Bonus-Level"><a href="#Bonus-Level" class="headerlink" title="Bonus Level"></a>Bonus Level</h2>This level is for you to apply what you learned to a more challenging dataset from scratch. The<br>dataset is, German Traffic Sign Recognition Benchmark (GTSRB) (German Traffic Sign Benchmarks<br>(rub.de)). This is available in the “Dataset_2.zip” file.<br>This dataset is already split into training and testing sets for you.<h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3>Write a python program to load the images from this dataset, into X, y. Then do suitable preprocessing, feature extraction and model training to develop a Traffic Sign Recognition system.<br>Report on the methods used and the results obtained by your Traffic Sign Recognition system.</li></ul><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> glob<br><span class="hljs-keyword">from</span> cv2 <span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br><span class="hljs-keyword">from</span> skimage.feature <span class="hljs-keyword">import</span> hog<br><span class="hljs-keyword">from</span> skimage.feature <span class="hljs-keyword">import</span> canny<br><span class="hljs-keyword">from</span> skimage.feature <span class="hljs-keyword">import</span> local_binary_pattern<br><br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">import</span> sklearn.naive_bayes <span class="hljs-keyword">as</span> sk_bayes<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><br><span class="hljs-keyword">import</span> datetime<br><br><span class="hljs-comment"># load the dataset</span><br>dataset_path = <span class="hljs-string">&quot;Dataset_1\\images\\&quot;</span><br>X = []  <span class="hljs-comment"># 图像集</span><br>y = []  <span class="hljs-comment"># 标签集</span><br>print(<span class="hljs-string">&quot;Loading from the dataset...&quot;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> glob.glob(dataset_path + <span class="hljs-string">&#x27;*.png&#x27;</span>, recursive=<span class="hljs-literal">True</span>):<br>    label = i.split(<span class="hljs-string">&quot;images&quot;</span>)[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>:<span class="hljs-number">4</span>]  <span class="hljs-comment"># 根据文件名开头划分标签</span><br>    y.append(label)<br>    img = cv2.imread(i)<br>    X.append(i)  <span class="hljs-comment"># 加载图像</span><br><br><span class="hljs-comment"># Preprocess</span><br>x_prossessed = []<br>print(<span class="hljs-string">&quot;Preprocessing the images...&quot;</span>)<br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X:<br>    x = cv2.imread(x)<br>    temp_x = cv2.resize(x, (<span class="hljs-number">48</span>, <span class="hljs-number">48</span>))<br>    <span class="hljs-comment"># 图像处理部分，给出了如下三种图像处理的方式，可以组合，也可以单独使用，不需要的直接注释掉然后append（36行）对应的结果就行</span><br>    <span class="hljs-comment"># convert to gray 转换为灰度图</span><br>    x_process_gray = cv2.cvtColor(temp_x, cv2.COLOR_BGR2GRAY)<br>    <span class="hljs-comment"># gave the gaussian blur 高斯滤波</span><br>    x_process_gau = cv2.GaussianBlur(x_process_gray, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># gave the medianBlur 中值滤波</span><br>    x_process_md = cv2.medianBlur(x_process_gau, <span class="hljs-number">3</span>)<br>    x_prossessed.append(x_process_md)<br><br><span class="hljs-comment"># calculate features</span><br><span class="hljs-comment"># 提取特征，给出了下面的三种方法，三选一，选一个取消注释。</span><br>X_features = []<br>print(<span class="hljs-string">&quot;Calculating features...&quot;</span>)<br><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> x_prossessed:<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    # Hog features : 提取图像的直方图信息</span><br><span class="hljs-string">    x_feature_hog = hog(x,</span><br><span class="hljs-string">                    orientations=8,</span><br><span class="hljs-string">                    pixels_per_cell=(10, 10),</span><br><span class="hljs-string">                    cells_per_block=(1, 1),</span><br><span class="hljs-string">                    visualize=False,</span><br><span class="hljs-string">                    multichannel=False)</span><br><span class="hljs-string">    X_features.append(x_feature_hog)</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    # canny features： 提取图像的边缘特征</span><br><span class="hljs-string">    x_feature_canny = canny(np.array(x), sigma=1.0)</span><br><span class="hljs-string">    X_features.append(x_feature_canny)</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>   <br>    <span class="hljs-comment"># LBP features: 对光照有很强的鲁棒性</span><br>    x_feature_lbp = local_binary_pattern(x, <span class="hljs-number">8</span>, <span class="hljs-number">1.0</span>, method=<span class="hljs-string">&#x27;default&#x27;</span>)<br>    X_features.append(x_feature_lbp)<br>    <br><span class="hljs-comment"># 把X_features 转换为长向量</span><br>images = np.array(X_features).reshape((<span class="hljs-built_in">len</span>(np.array(X_features)), -<span class="hljs-number">1</span>))<br><br><span class="hljs-comment"># 划分训练集和测试集</span><br>print(<span class="hljs-string">&quot;Trainning the dataset...&quot;</span>)<br>x_train, x_test, y_train, y_test = train_test_split(images,<br>                                                    y,<br>                                                    test_size=<span class="hljs-number">0.2</span>,<br>                                                    shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 分类器，下面给出了三种分类器</span><br>acc = []  <span class="hljs-comment"># 创建一个列表用于存放准确率</span><br>timecosts = []  <span class="hljs-comment"># 创建一个列表用于存放耗时</span><br><span class="hljs-comment"># 支持向量机分类器</span><br>print(<span class="hljs-string">&quot;Trainning by svm...&quot;</span>)<br>time1 = (datetime.datetime.now())  <span class="hljs-comment"># 第一个时间戳</span><br>clf1 = svm.SVC()<br>clf1.fit(x_train, y_train)  <span class="hljs-comment"># 数据拟合</span><br>time2 = (datetime.datetime.now())  <span class="hljs-comment"># 第二个时间戳</span><br>acc_svm = clf1.score(x_test, y_test)  <span class="hljs-comment"># 计算准确率</span><br>acc.append(acc_svm)<br>timecost = <span class="hljs-built_in">str</span>(time2 - time1)  <span class="hljs-comment"># 计算时间差</span><br>timecosts.append(timecost)<br><br><span class="hljs-comment"># 伯努利分布的朴素贝叶斯分类器</span><br>print(<span class="hljs-string">&quot;Trainning by BN...&quot;</span>)<br>time1 = (datetime.datetime.now())  <span class="hljs-comment"># 第一个时间戳</span><br>clf2 = sk_bayes.BernoulliNB(alpha=<span class="hljs-number">1.0</span>,<br>                            binarize=<span class="hljs-number">0.0</span>,<br>                            fit_prior=<span class="hljs-literal">True</span>,<br>                            class_prior=<span class="hljs-literal">None</span>)<br>clf2.fit(x_train, y_train)  <span class="hljs-comment"># 数据拟合</span><br>time2 = (datetime.datetime.now())  <span class="hljs-comment"># 第二个时间戳</span><br>acc_NB = clf2.score(x_test, y_test)  <span class="hljs-comment"># 计算准确率</span><br>acc.append(acc_NB)<br>timecost = <span class="hljs-built_in">str</span>(time2 - time1)  <span class="hljs-comment"># 计算时间差</span><br>timecosts.append(timecost)<br><br><span class="hljs-comment"># KNN分类器 k=1 (对于这个KNN，k越大，越不行)</span><br>print(<span class="hljs-string">&quot;Trainning by KNN...&quot;</span>)<br>time1 = (datetime.datetime.now())  <span class="hljs-comment"># 第一个时间戳</span><br>clf3 = KNeighborsClassifier(n_neighbors=<span class="hljs-number">1</span>)<br>clf3.fit(x_train, y_train)  <span class="hljs-comment"># 数据拟合</span><br>time2 = (datetime.datetime.now())  <span class="hljs-comment"># 第二个时间戳</span><br>acc_KNN = clf3.score(x_test, y_test)  <span class="hljs-comment"># 计算准确率</span><br>acc.append(acc_KNN)<br>timecost = <span class="hljs-built_in">str</span>(time2 - time1)  <span class="hljs-comment"># 计算时间差</span><br>timecosts.append(timecost)<br><br>clfname = [<span class="hljs-string">&#x27;SVM&#x27;</span>, <span class="hljs-string">&#x27;NB&#x27;</span>, <span class="hljs-string">&#x27;KNN&#x27;</span>]<br>print(<span class="hljs-string">&quot;---------result-------------&quot;</span>)<br>print(<span class="hljs-string">&quot;accurancy is:&quot;</span>)<br>print(clfname)  <span class="hljs-comment"># 输出分类器的名字</span><br>print(acc)  <span class="hljs-comment"># 输出准确率</span><br>print(<span class="hljs-string">&quot;timecost:&quot;</span>)<br>print(timecosts)<br><br>print(<span class="hljs-string">&quot;Displaying the Confusion Matrixes&quot;</span>)<br><span class="hljs-comment"># 显示混淆矩阵</span><br><span class="hljs-comment"># 混淆矩阵显示有点慢 但是三张都可以显示 【要关闭当前之后才能显示下一张】</span><br><span class="hljs-comment"># svm算法的混淆矩阵</span><br>print(<span class="hljs-string">&quot;cm of svm&quot;</span>)<br>cm1 = metrics.plot_confusion_matrix(clf1, x_test, y_test)  <span class="hljs-comment"># 创建混淆矩阵</span><br>plt.get_current_fig_manager().window.state(<span class="hljs-string">&#x27;zoomed&#x27;</span>)<br>plt.show()  <span class="hljs-comment"># 显示混淆矩阵</span><br><br><span class="hljs-comment"># BN算法的混淆矩阵</span><br>print(<span class="hljs-string">&quot;cm of bn&quot;</span>)<br>cm2 = metrics.plot_confusion_matrix(clf2, x_test, y_test)  <span class="hljs-comment"># 创建混淆矩阵</span><br>plt.get_current_fig_manager().window.state(<span class="hljs-string">&#x27;zoomed&#x27;</span>)<br>plt.show()  <span class="hljs-comment"># 显示混淆矩阵</span><br><br><span class="hljs-comment"># KNN的混淆矩阵</span><br>print(<span class="hljs-string">&quot;cm of knn&quot;</span>)<br>cm3 = metrics.plot_confusion_matrix(clf3, x_test, y_test)  <span class="hljs-comment"># 创建混淆矩阵</span><br>plt.get_current_fig_manager().window.state(<span class="hljs-string">&#x27;zoomed&#x27;</span>)<br>plt.show()  <span class="hljs-comment"># 显示混淆矩阵</span><br><br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>课后练习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hexo搭建博客记录</title>
    <link href="/2021/02/20/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/hexo%E5%8D%9A%E5%AE%A2%E7%97%9B%E7%82%B9/"/>
    <url>/2021/02/20/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/hexo%E5%8D%9A%E5%AE%A2%E7%97%9B%E7%82%B9/</url>
    
    <content type="html"><![CDATA[<h1 id="Hexo搭建博客记录"><a href="#Hexo搭建博客记录" class="headerlink" title="Hexo搭建博客记录"></a>Hexo搭建博客记录</h1><p>搭建这个博客前前后后花了大概一周左右的时间，基本上把能踩的雷全都踩过了，现在记录一下搭建过程中的问题和解决办法。</p><h2 id="Github-Pages-相关问题"><a href="#Github-Pages-相关问题" class="headerlink" title="Github Pages 相关问题"></a>Github Pages 相关问题</h2><h3 id="Github-Pages-无法创建页面-显示“Pages-build-faild”-但是没有任何报错信息"><a href="#Github-Pages-无法创建页面-显示“Pages-build-faild”-但是没有任何报错信息" class="headerlink" title="Github Pages 无法创建页面 显示“Pages build faild” 但是没有任何报错信息"></a>Github Pages 无法创建页面 显示“Pages build faild” 但是没有任何报错信息</h3><p>第一个遇到的问题是Github Pages 始终反馈无法创建（build）页面，反馈邮件当中没有任何关于错误的信息。<br>debug非常多次之后发现是由于Git 把所有的代码都同步到了项目里：<br>自己用的VS Code来写的博客。自己的VS Code里面本来就设置好了git，因此就直接用VS Code里面的git把项目里面所有的文件都提交上去了……<br>所以解决办法是不要用VS Code里面的git来提交代码，正确的做法是用git bash里的<code>hexo g</code>生成文件后，用<code>hexo d</code>提交。  </p><h3 id="Github-Pages-反馈邮件“You-are-attempting-to-use-a-Jekyll-theme-which-is-not-supported-by-GitHub-Pages-”"><a href="#Github-Pages-反馈邮件“You-are-attempting-to-use-a-Jekyll-theme-which-is-not-supported-by-GitHub-Pages-”" class="headerlink" title="Github Pages 反馈邮件“You are attempting to use a Jekyll theme, which is not supported by GitHub Pages.”"></a>Github Pages 反馈邮件“You are attempting to use a Jekyll theme, which is not supported by GitHub Pages.”</h3><p>本地build正常，但是同步到github pages后会收到来自github的邮件：   </p><blockquote><p>You are attempting to use a Jekyll theme, which is not supported by GitHub Pages. Please visit <a href="https://pages.github.com/themes/">https://pages.github.com/themes/</a> for a list of supported themes. If you are using the “theme” configuration variable for something other than a Jekyll theme, we recommend you rename this variable throughout your site. For more information, see <a href="https://help.github.com/en/articles/adding-a-jekyll-theme-to-your-github-pages-site">https://help.github.com/en/articles/adding-a-jekyll-theme-to-your-github-pages-site</a>.      </p></blockquote><p>这个问题是由更换主题时直接clone的主题，导致项目下面有两个repo造成的。<br>解决方法有两个：</p><ol><li>直接在主题的release页面中下载发布的压缩包，解压之后放theme里【推荐】</li><li>在git中使用submodule：<code>git submodule add url</code></li></ol><h3 id="Github-Pages-显示-404-“There-isn’t-a-GitHub-Pages-site-here-”"><a href="#Github-Pages-显示-404-“There-isn’t-a-GitHub-Pages-site-here-”" class="headerlink" title="Github Pages 显示 404 “There isn’t a GitHub Pages site here.”"></a>Github Pages 显示 404 “There isn’t a GitHub Pages site here.”</h3><p>这个问题是由Github 里博客对应的项目名称不是username.github.io造成的，解决方法是把项目的名字命名为username.github.io     </p><h3 id="Github-Pages-显示“The-custom-domain-for-your-GitHub-Pages-site-is-pointed-at-an-outdated-IP-address-You-must-update-your-site’s-DNS-records-if-you’d-like-it-to-be-available-via-your-custom-domain-”"><a href="#Github-Pages-显示“The-custom-domain-for-your-GitHub-Pages-site-is-pointed-at-an-outdated-IP-address-You-must-update-your-site’s-DNS-records-if-you’d-like-it-to-be-available-via-your-custom-domain-”" class="headerlink" title="Github Pages 显示“The custom domain for your GitHub Pages site is pointed at an outdated IP address. You must update your site’s DNS records if you’d like it to be available via your custom domain.”"></a>Github Pages 显示“The custom domain for your GitHub Pages site is pointed at an outdated IP address. You must update your site’s DNS records if you’d like it to be available via your custom domain.”</h3><p>原因是因为在云解析DNS控制台中没有加入github给的DNS或DNS已经失效。<br>解决方法是在[<a href="https://docs.github.com/en/github/working-with-github-pages/managing-a-custom-domain-for-your-github-pages-site]页面的">https://docs.github.com/en/github/working-with-github-pages/managing-a-custom-domain-for-your-github-pages-site]页面的</a><br>“6 To confirm that your DNS record configured correctly, use the dig command, replacing EXAMPLE.COM with your apex domain. Confirm that the results match the IP addresses for GitHub Pages above. “中找到github提供的DNS，然后加入到云解析DNS控制台的解析记录中。<br>同时还要删除很多教程中提到的用<code>ping username.github.io</code>ping出来的ip地址。  </p><h3 id="Github-Pages-显示“Domain’s-DNS-record-could-not-be-retrieved-”"><a href="#Github-Pages-显示“Domain’s-DNS-record-could-not-be-retrieved-”" class="headerlink" title="Github Pages 显示“Domain’s DNS record could not be retrieved.”"></a>Github Pages 显示“Domain’s DNS record could not be retrieved.”</h3><p>原因是网站的解析设置有问题造成的，在解析控制台中的解析记录要注意解析类型和主机记录的对应关系：<br>A类对应@，CNAME对应www或者别的。<br>还有可能是项目中的CNAME文件和github settings中custom domain设置的域名前面加了www.等其他东西。<br>同时解析线路一定要选择默认。    </p><h2 id="博客访问问题"><a href="#博客访问问题" class="headerlink" title="博客访问问题"></a>博客访问问题</h2><h3 id="无梯访问username-github-io-显示“已拒绝连接”"><a href="#无梯访问username-github-io-显示“已拒绝连接”" class="headerlink" title="无梯访问username.github.io 显示“已拒绝连接”"></a>无梯访问username.github.io 显示“已拒绝连接”</h3><p>解决办法是更换为自己的域名，可以在阿里云，腾讯云之类的地方购买自己的域名，然后在github上对应项目的设置中Custom domain一项中填写自己买的域名（前面不要加www.之类的，就是纯域名）。<br>同时解析线路一定要选择默认。   </p><h3 id="访问博客显示“连接已关闭”"><a href="#访问博客显示“连接已关闭”" class="headerlink" title="访问博客显示“连接已关闭”"></a>访问博客显示“连接已关闭”</h3><p>通常是由于github pages在build和分发的过程中出错导致的，要去github上对应的项目的设置中看github pages的报错信息，具体问题具体分析。  </p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="git-频繁要求输入邮箱和用户名"><a href="#git-频繁要求输入邮箱和用户名" class="headerlink" title="git 频繁要求输入邮箱和用户名"></a>git 频繁要求输入邮箱和用户名</h3><p>  这个问题是由于_config.yml末尾的deploy: repo:中设置的是https网址导致的<br>  正确的填写方法是用SSH链接：<br>  <figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs JAVA">deploy:<br>type: <span class="hljs-string">&#x27;git&#x27;</span><br>repo: git<span class="hljs-meta">@github</span>.com:username/username.github.io.git<br>branch: master<br></code></pre></div></td></tr></table></figure></p><h3 id="博客中的LaTeX-MathJaX公式显示混乱"><a href="#博客中的LaTeX-MathJaX公式显示混乱" class="headerlink" title="博客中的LaTeX/MathJaX公式显示混乱"></a>博客中的LaTeX/MathJaX公式显示混乱</h3><p>这个是由于renderer-marked的转义与markdown本身出现了冲突所造成的。<br>解决方法：  </p><ol><li>卸载原来的公式渲染引擎改用kramed<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">npm uninstall hexo-renderer-marked --save<br>npm install hexo-renderer-kramed --save<br></code></pre></div></td></tr></table></figure></li><li>然后在node_modules\kramed\lib\rules\inline.js目录下把第11行的escape变量的值做相应的修改： <figure class="highlight"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//  escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br>escape: /^\\([`*\[\]()#$+\-.!_&gt;])/<br></code></pre></div></td></tr></table></figure> 这一步是在原基础上取消了对\,{,}的转义(escape)。<br> 同时把第20行的em变量也要做相应的修改。    <figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//  em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br>em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/<br></code></pre></div></td></tr></table></figure></li><li>执行<code>hexo clean</code>再用<code>hexo g</code>重新生成</li></ol><h3 id="页面中的LaTeX-MathJaX矩阵无法换行，只有一列"><a href="#页面中的LaTeX-MathJaX矩阵无法换行，只有一列" class="headerlink" title="页面中的LaTeX/MathJaX矩阵无法换行，只有一列"></a>页面中的LaTeX/MathJaX矩阵无法换行，只有一列</h3><ol><li>行内矩阵不能用<code>\begin&#123;matrix&#125;</code>，要用<code>\begin&#123;smallmatrix&#125;</code>，括号用<code>\left[</code>和<code>\right]</code>表示</li><li>由于渲染问题，换行符<code>\\</code>要改为<code>\\\</code> 同时注意前后都要空格</li></ol><h3 id="页面中的表格无法显示"><a href="#页面中的表格无法显示" class="headerlink" title="页面中的表格无法显示"></a>页面中的表格无法显示</h3><p>这是由于mathjax不支持markdown中的表格语法造成的，正确的写法是用数组array代替:<br><figure class="highlight livescript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs livescript">$$<br><span class="hljs-string">\begin&#123;array&#125;&#123;c|lcr&#125;</span><br>n &amp; <span class="hljs-string">\text&#123;Left&#125;</span> &amp; <span class="hljs-string">\text&#123;Center&#125;</span> &amp; <span class="hljs-string">\text&#123;Right&#125;</span> <span class="hljs-string">\\</span><br><span class="hljs-string">\hline</span><br><span class="hljs-number">1</span> &amp; <span class="hljs-number">0.24</span> &amp; <span class="hljs-number">1</span> &amp; <span class="hljs-number">125</span> <span class="hljs-string">\\</span><br><span class="hljs-number">2</span> &amp; -<span class="hljs-number">1</span> &amp; <span class="hljs-number">189</span> &amp; -<span class="hljs-number">8</span> <span class="hljs-string">\\</span><br><span class="hljs-number">3</span> &amp; -<span class="hljs-number">20</span> &amp; <span class="hljs-number">2000</span> &amp; <span class="hljs-number">1</span>+<span class="hljs-number">10i</span><br><span class="hljs-string">\end&#123;array&#125;</span><br>$$<br></code></pre></div></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>技术杂谈</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>✨【置顶】本站说明</title>
    <link href="/2021/02/20/%E7%BD%AE%E9%A1%B6/"/>
    <url>/2021/02/20/%E7%BD%AE%E9%A1%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>1️⃣ 建议点击右上角【📚分类】查看文章<br>2️⃣ 因为用的github做的图床，所以图片加载可能会有问题，请用更科学的方式访问本站⚡<br>3️⃣ 电脑端Chrome/Edge 等Chromium浏览器可以点击左下角的铃铛🔔订阅我的博客<br>4️⃣ 转载请著名来源，谢谢📖</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>4. 课后练习-MNIST 手写训练集</title>
    <link href="/2021/02/18/Machine%20Learning-NAU/4.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-MNIST/"/>
    <url>/2021/02/18/Machine%20Learning-NAU/4.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0-MNIST/</url>
    
    <content type="html"><![CDATA[<h1 id="课后练习3"><a href="#课后练习3" class="headerlink" title="课后练习3"></a>课后练习3</h1><h2 id="Tasks"><a href="#Tasks" class="headerlink" title="Tasks"></a>Tasks</h2><ol><li>Familiarize yourself with the MNIST dataset: MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges. [<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>]  </li><li>Familiarize yourself with sklearn package: scikit-learn: machine learning in Python — scikitlearn 0.24.1 documentation [scikit-learn.org]  </li></ol><h2 id="Programming-exercise"><a href="#Programming-exercise" class="headerlink" title="Programming exercise"></a>Programming exercise</h2><h3 id="Q1-Use-the-fetch-openml-function-found-in-sklearn-datasets-to-load-the-mnist-784-dataset-into-python-This-will-load-X-and-y-variables-for-you"><a href="#Q1-Use-the-fetch-openml-function-found-in-sklearn-datasets-to-load-the-mnist-784-dataset-into-python-This-will-load-X-and-y-variables-for-you" class="headerlink" title="Q1. Use the fetch_openml function found in sklearn.datasets to load the mnist_784 dataset into python. This will load X and y variables for you."></a>Q1. Use the fetch_openml function found in sklearn.datasets to load the mnist_784 dataset into python. This will load X and y variables for you.</h3><ul><li>Print the dimensions of the variables returned by the function.</li><li>Write a python script to find how many distinct values are present in y?</li><li>Select one sample from X for each distinct y value.</li><li>Resize each sample to represent the 28x28 pixel image.</li><li>Display all the selected images in one diagram using subplots in matplotlib. The following<br>code gives you an example of how to do this,<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">fig = plt.figure()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>):<br>fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, i)<br>plt.imshow(images[i])<br>plt.show()<br></code></pre></div></td></tr></table></figure>Solutions:<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> sklearn,datasets <span class="hljs-keyword">import</span> fetch_openml<br>images,labels = fetch_openml(<span class="hljs-string">&#x27;mnist_784&#x27;</span>,version=<span class="hljs-number">1</span>, return_x_y=true, as_frame=false)<br><span class="hljs-comment"># load 70000 28x28=784 handwriting images</span><br><span class="hljs-comment"># print(images.shape)</span><br><span class="hljs-comment">#&gt;&gt; (7000,784)</span><br></code></pre></div></td></tr></table></figure>or <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br>digits = datasets.load_digits() <span class="hljs-comment">#load the mnist dataset which already in sklearn</span><br>images = digits.images <span class="hljs-comment">#access  1797 8x8 images in mnist by print(images.shape)</span><br>labels = digits.target <span class="hljs-comment">#access 1797 labes </span><br><span class="hljs-comment"># print(images.shape)</span><br><span class="hljs-comment">#&gt;&gt; (1797,8,8)</span><br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>digits = datasets.load_digits() <span class="hljs-comment">#load the mnist dataset which already in sklearn</span><br>images = digits.images <span class="hljs-comment">#access  1797 8x8 images in mnist by print(images.shape)</span><br>labels = digits.target <span class="hljs-comment">#access labels</span><br><br>np.unique(labels) <span class="hljs-comment"># summerize the labels</span><br>print(np.unique(labels).shape)<br><br>fig = plt.figure()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>):<br>    fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, i+<span class="hljs-number">1</span>) <span class="hljs-comment"># creat a batch of subplot with 2 rows 5 columns</span><br>    <span class="hljs-comment"># i means the position in the subplot</span><br>    plt.imshow(images[i])<br>plt.show() <span class="hljs-comment"># display the subplot</span><br></code></pre></div></td></tr></table></figure><h3 id="Q2-Use-sklearn-to-train-a-digit-classifier"><a href="#Q2-Use-sklearn-to-train-a-digit-classifier" class="headerlink" title="Q2. Use sklearn to train a digit classifier."></a>Q2. Use sklearn to train a digit classifier.</h3></li><li>Split the X and y into a training set and testing set of 80-20 split.</li><li>Train a Support Vector Machin (SVM) for classification of the digits using the training set.<br>The following code shows how to train a model using sklearn.<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">clf = svm.SVC()<br>clf.fit(x_train, y_train)<br></code></pre></div></td></tr></table></figure></li><li>Test the model using the test set.</li><li>Experiment with different parameter values for the SVM and see how it performs. Try<br>changing the gamma value to be [0.0001, 0.0005, 0.001, 0.005, 0.01]</li><li>Plot the accuracy value with respect to the change in gamma above.</li><li>Plot the confusion matrix<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">import</span> matplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics<br><br>digits = datasets.load_digits() <span class="hljs-comment"># load the mnist dataset which already in sklearn</span><br>data = digits.images <span class="hljs-comment"># access  1797 8x8 images in mnist, print(images.shape)</span><br>labels = digits.target <span class="hljs-comment"># access 1797 labels</span><br><br>images = data.reshape((<span class="hljs-built_in">len</span>(data),-<span class="hljs-number">1</span>)) <span class="hljs-comment"># reshaape the 8x8 matrixes into 64x1 vectors</span><br><br>x_train,x_test,y_train,y_test = train_test_split(images,labels, test_size = <span class="hljs-number">0.2</span>, shuffle = false) <span class="hljs-comment"># 20% will be test set</span><br><span class="hljs-comment"># x:images y:labels</span><br><br>clf = svm.SVC() <span class="hljs-comment"># create the svm classifier</span><br>clf.fit(x_train, y_train) <span class="hljs-comment"># fit the data  within vectors</span><br><br>acc = clf.score(x_test, y_test) <span class="hljs-comment"># do the test and retrun the accurancy</span><br>disp = metrics.plot_confusion_matrix(clf,x_test,y_test) <span class="hljs-comment"># add into confusion matrix</span><br>print(acc) <span class="hljs-comment"># print the accurancy</span><br>sklearn.metrics.ConfusionMatrixDisplay(disp) <span class="hljs-comment"># display the confusion matrix</span><br><br>g_ = [<span class="hljs-number">0.0001</span>,<span class="hljs-number">0.0005</span>,<span class="hljs-number">0.001</span>,<span class="hljs-number">0.005</span>,<span class="hljs-number">0.01</span>] <span class="hljs-comment"># list of gamma</span><br>scores = [] <span class="hljs-comment"># list of accurancy</span><br><span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> g_:<br>    clf = svm.SVC(gamma = g) <span class="hljs-comment"># create the svm classifier,specify the gamma</span><br>    clf.fit(x_train, y_train) <span class="hljs-comment"># fit the data  within vectors</span><br><br>    acc = clf.score(x_test, y_test) <span class="hljs-comment"># do the test and retrun the accurancy</span><br>    scores.append(acc)<br><br><br><br>print(g_) <span class="hljs-comment"># print the accurancy</span><br>print(scores)<br><br>plt.plot(g_, scores)<br>plt.show()  <br></code></pre></div></td></tr></table></figure></li></ul><p>```</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>课后练习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2. 数学方法</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/2.%20%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/2.%20%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="数学方法"><a href="#数学方法" class="headerlink" title="数学方法"></a>数学方法</h1><h2 id="矩阵的运算"><a href="#矩阵的运算" class="headerlink" title="矩阵的运算"></a>矩阵的运算</h2><h3 id="矩阵的乘法"><a href="#矩阵的乘法" class="headerlink" title="矩阵的乘法"></a>矩阵的乘法</h3><p>矩阵的乘法规则：前一矩阵的行乘后一矩阵的纵列若A是一个$m \times n$的矩阵，B是一个$a \times b$的矩阵，那么矩阵乘法$A \times B$的结果将会是一个$n \times a$的矩阵</p><blockquote><p>要注意$A × B=0 ⇏A=0 ~or~ B=0$<br>$AB \not ={} BA$,但是$(AB)C=A(BC)$</p></blockquote><p>矩阵与向量的乘法可以改写,用如下例子来做表示：<br>$A=\left[\begin{smallmatrix}<br>1 &amp; 2 &amp; 1 \\\ -1 &amp; 0 &amp; 2<br>\end{smallmatrix}\right] B=\left[\begin{smallmatrix}1 \\\ -1 \\\ 1\end{smallmatrix}\right]$<br>有$A\times B=1×\left[\begin{smallmatrix}1 \\\ -1\end{smallmatrix}\right]+(-1)×\left[\begin{smallmatrix}2 \\\ 0\end{smallmatrix}\right]+1\times \left[\begin{smallmatrix}1 \\\ 2\end{smallmatrix}\right]$<br>称B是A的一组线性组合</p><h3 id="转置、对称"><a href="#转置、对称" class="headerlink" title="转置、对称"></a>转置、对称</h3><p>$A^T$表示A的转置，即A行列交换后的矩阵。<br>有$(AB)^{T}=B^{T}A^{T}$<br>方阵：A为一个正方形矩阵:$m×m$<br>单位矩阵：\对角线上的元素为1，其余元素为0的方阵，有$AI=IA=A$<br>若$A^T=A$,称A是一个对称矩阵(Symmetric matrix)，若$A^T=-A$，称A是一个交错矩阵(Skew-symmectric matrix)</p><h3 id="矩阵的逆"><a href="#矩阵的逆" class="headerlink" title="矩阵的逆"></a>矩阵的逆</h3><p>有矩阵$A^{-1}A=I$,称矩阵$A^{-1}$为矩阵A的逆。奇异矩阵不可逆。运算规律：</p><ol><li>$(AB)^{-1}=B^{-1}A^{-1}$</li><li>$(A^T)^{-1}=(A^{-1})^{T}$</li><li>$(A^{-1})^{-1}=A$</li></ol><h3 id="矩阵方程的解法"><a href="#矩阵方程的解法" class="headerlink" title="矩阵方程的解法"></a>矩阵方程的解法</h3><p>对于任何一个线性方程组可以改写成：</p><script type="math/tex; mode=display">Ax=b</script><p>A是系数矩阵，x是参数向量，b是方程组右边组成的常数向量<br>解方程只需要求出$x=Ab$</p><ul><li>线性无关<br>如果A的列向量$a_1,a_2,a_3…a_n$的线性组合为0：</li></ul><script type="math/tex; mode=display">\sum λ_ia_i=0</script><p>称这些向量是线性无关的。<br>A中线性无关列向量的最大数目表示A的秩(Rank)<br>$Nul(A)$表示线性齐次方程$Ax=0$的解集，它的维度称为零度(Nullity)<br>如果 $RanK(A)+Nullity(A)=columns~of~A$，可以判断A是可逆的。</p><h3 id="正交-Orthogonal-Perpenticular"><a href="#正交-Orthogonal-Perpenticular" class="headerlink" title="正交(Orthogonal/Perpenticular)"></a>正交(Orthogonal/Perpenticular)</h3><p>两个向量$x,y$,如果$x^Ty=0$，称这两个向量是正交的。<br>如果一个向量集$b_1,b_2,b_3…b_n$中的任意两个元素 $b_i^Tb_j=\begin{cases}<br>1, i=j \\ 0, i \not =j<br>\end{cases}$<br>称这个向量集是标准正交集(Orthonormal)<br>若矩阵Q，$Q^TQ=I$称Q是正交的，它所有的列向量都是正交的</p><h3 id="行列式计算"><a href="#行列式计算" class="headerlink" title="行列式计算"></a>行列式计算</h3><p>det(A)或者|A|记作A的行列式,在Python中可以用numpy库中的函数进行运算。计算性质：</p><ol><li>$det(AB)=det(A)det(B)$</li><li>$det(A^{-1})=\frac{1}{det(A)}$<br>所以det(A)=0时，A不可逆</li><li>$det(A^T)=det(A)$</li><li>$det(kA)=k^ndet(A)$,$A_{n \times n}$</li><li>$det(A)=Πλ_i$</li></ol><h3 id="特征值-特征向量"><a href="#特征值-特征向量" class="headerlink" title="特征值/特征向量"></a>特征值/特征向量</h3><p>若有$Ax=λx$,λ称为A的特征值(Eigenvalue)，x称为A的特征向量(Eigenvector)</p><blockquote><p>特征值可能是一个复数，矩阵的特征向量/特征值可能有几个是相同的<br>$kλ$和$kx$仍然是A的特征值和特征向量，所以默认解出的特征向量的模长(Norm)为1。</p></blockquote><ul><li>求解特征值/特征向量<br>通过$det(A-λI)=0$，求解$λ$,再代回$Ax-λx=0$求解x</li><li><p>谱分解(Spectral Theorem)<br>A所有的特征值和特征向量可以写成一个矩阵方程：</p><script type="math/tex; mode=display">A \begin{bmatrix}  x_1&x_2&...&x_n\end{bmatrix}=\begin{bmatrix}  x_1&x_2&...&x_n\end{bmatrix}\begin{bmatrix}  λ_1 &&&&\\ &λ_2\\&&λ_3\\&&&...\end{bmatrix}</script><p>$\begin{bmatrix}<br>λ_1 &amp;&amp;&amp;&amp;\\ &amp;λ_2\\&amp;&amp;λ_3\\&amp;&amp;&amp;…<br>\end{bmatrix}$ 称为A的对角矩阵(Diagonal matrix)$Λ$<br>记作$AE=EΛ$<br>如果E是可逆矩阵，$A=EΛE^{-1}$<br>如果A是对称矩阵，有$E^{-1}=E^T$,$A=EΛE^T$<br>在机器学习中，A常常是对称的，而且所有的特征值都是实数</p></li></ul><h3 id="迹-Trace"><a href="#迹-Trace" class="headerlink" title="迹(Trace)"></a>迹(Trace)</h3><p>矩阵A的\对角线元素的总和称为A的迹：</p><script type="math/tex; mode=display">tr(A)=\sum a_{ii}</script><p>它在数值上也等于所有特征值的和：</p><script type="math/tex; mode=display">tr(A)=\sum λ_{i}</script><p>计算性质：</p><ol><li>$tr(AB)=tr(BA)$</li><li>$tr(A+B)=tr(A)+tr(B)$</li></ol><h3 id="伪逆矩阵-Pseudo-inverse"><a href="#伪逆矩阵-Pseudo-inverse" class="headerlink" title="伪逆矩阵(Pseudo-inverse)"></a>伪逆矩阵(Pseudo-inverse)</h3><p>当A不可逆时，要解决$Ax=b$,转写为$x=A^{-1}b$的形式求解x看似不可能，因此构造矩阵$A^{+}$,使得$x=A^{+}b,Ax-b$的模长最小，$A^+$称为A的伪逆矩阵。</p><script type="math/tex; mode=display">A^+=(A^TA)^{-1}A^T</script><script type="math/tex; mode=display">A^+A=(A^TA)^{-1}A^TA=I</script><script type="math/tex; mode=display">AA^+=A (A^TA)^{-1}A^T\not=I</script><p>在Python中<code>pinv(A)</code>可以实现求解伪逆矩阵</p><h3 id="矩阵的导数"><a href="#矩阵的导数" class="headerlink" title="矩阵的导数"></a>矩阵的导数</h3><p>矩阵的导数满足如下性质：</p><ol><li>$\frac{d}{dx}Ax=A^T$</li><li>$\frac{dx}{dx}=I$</li><li>$\frac{y^Tx}{dx}=\frac{dx^Ty}{dx}=y$</li><li>$\frac{d(x^TAx)}{dx}=\begin{cases}<br>(A+A^T)x,\text{A is square}\\2Ax,\text{A is symmetrix}<br>\end{cases}$</li><li>$\frac{d(u^T(x)~v(x))}{dx}=[\frac{du^T}{dx}]v+[\frac{dv^T}{dx}]u$</li><li>$\frac{d~tr(A)}{dA}=I$</li><li>$\frac{det(A)}{dA}=det(A)(A^{-1})^T$</li></ol><ul><li><p>伪逆矩阵证明当A为奇异矩阵时，求解$Ax=b$:定义误差(error)$e=Ax-b$，要使得$|e|$尽可能小：设$y=|e|^2$，有：</p><script type="math/tex; mode=display">\begin{aligned}  y & =e^Te \\  & =(Ax-b)^T(Ax-b) \\  & =(Ax)^T(Ax)-(Ax)^Tb-b^T(Ax)+b^Tb \\  & =x^TA^TAx-2b^T(Ax)+b^Tb \\\end{aligned}</script><p>对y求导：$\frac{dy}{dx}=2A^TAx-2A^Tb+0$</p><blockquote><p>第一项，$A^TA$是一个对称矩阵，可以应用#4.<br>第二项，应用#3<br>第三项，$b^Tb$是一个常数</p></blockquote><p>令$\frac{dy}{dx}=2A^TAx-2A^Tb=0$：</p><script type="math/tex; mode=display">A^TAx=A^Tb</script><script type="math/tex; mode=display">(A^TA)^{-1}A^Tb=A^+b</script></li></ul><p>在Python中，<code>linalg.solve(A,B)</code>能够求解$x=A^+b$     </p><h2 id="概率论的基础概念"><a href="#概率论的基础概念" class="headerlink" title="概率论的基础概念"></a>概率论的基础概念</h2><ul><li>随机变量<br>随机实验是一种能够产生随机结果的可重复性实验，样本空间$S$表示随机实验中所有可能出现的结果构成的集合，事件(Event)是样本空间S的子集。<br>随机变量(Random variables)是将S对应到实数集的一种函数，概率分布函数(MPF)表示X在样本空间中所发生的概率与X之间的关系。概率密度函数(PDF)表示样本空间中概率分布的稠密程度。    </li><li><p>常见的随机分布<br>*具体翻阅概率论笔记，此处不再赘述。<br>两点分布(Bernouli)<br>几何分布(Geometric)<br>二项分布(Binomial)<br>泊松分布(Poisson)<br>均匀分布(Uniform)<br>指数分布(Exponential)<br>双指数分布(Laplace/Double Exponential): $F(x)=\frac{λ}{2}e^{-λ|x|},λ&gt;0$<br>正态分布(Gaussian/Normal)  </p></li><li><p>其他分布函数概念<br>联合概率密度/联合分布函数(Joint PDF)<br>边缘分布函数(Marginal PDF)<br>条件概率函数(Conditional PDF)   </p></li></ul><h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h2><p>  全概率公式的逆公式，表示已知B事件发生的概率下，在A中某一个划分下发生的概率：</p><script type="math/tex; mode=display">P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}=\frac{P(A_i)P(B|A_i)}{ΣP(A_i)P(B|A_i)}</script>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>讲义</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1. 课后练习-机器学习简介和Python的基本操作</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/1.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/1.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="课后练习1"><a href="#课后练习1" class="headerlink" title="课后练习1"></a>课后练习1</h1><p>Solve the questions below by writing a Python function or script.   </p><h2 id="Q1-Add-up-the-numbers-from-100-to-200-and-output-their-sum-using-while-and-for-loops"><a href="#Q1-Add-up-the-numbers-from-100-to-200-and-output-their-sum-using-while-and-for-loops" class="headerlink" title="Q1. Add up the numbers from 100 to 200 and output their sum, using while and for loops."></a>Q1. Add up the numbers from 100 to 200 and output their sum, using while and for loops.</h2><p>for loop:<br><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">total = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>,<span class="hljs-number">201</span>): <span class="hljs-comment">#注意range不包括右边项</span><br>    total+=i<br>print(<span class="hljs-string">&quot;for loop sum:&quot;</span>,total)    <br></code></pre></div></td></tr></table></figure><br>while loop:<br><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">total=<span class="hljs-number">0</span><br>counter = <span class="hljs-number">100</span><br><span class="hljs-keyword">while</span> counter &lt; <span class="hljs-number">201</span>:<br>    total= total+counter <span class="hljs-comment">#++不能在python当中使用</span><br>    counter+=<span class="hljs-number">1</span> <span class="hljs-comment">#while 循环里面没有自增加</span><br>print(<span class="hljs-string">&#x27;for loop sum:&#x27;</span>,total)<br></code></pre></div></td></tr></table></figure></p><h2 id="Q2-Read-a-string-from-console-and-output-its-length-swap-its-cases（转换大小写）-convert-it-to-lower-case-and-upper-case-and-reverse-it-Hint-try-string-slice-with-step-1"><a href="#Q2-Read-a-string-from-console-and-output-its-length-swap-its-cases（转换大小写）-convert-it-to-lower-case-and-upper-case-and-reverse-it-Hint-try-string-slice-with-step-1" class="headerlink" title="Q2. Read a string from console and output its length, swap its cases（转换大小写）, convert it to lower case and upper case, and reverse it. (Hint: try string slice with step -1)"></a>Q2. Read a string from console and output its length, swap its cases（转换大小写）, convert it to lower case and upper case, and reverse it. (Hint: try string slice with step -1)</h2><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">s= <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;enter the string:&quot;</span>)<br>print(<span class="hljs-string">&quot;the length of the string:&quot;</span>,<span class="hljs-built_in">len</span>(s))<br><br>swap=<span class="hljs-built_in">str</span>.swapcase(s) <span class="hljs-comment">#str.swapcase() 转换大小写</span><br><br>print(<span class="hljs-string">&quot;swapcase is :&quot;</span>,swap)<br>print(<span class="hljs-string">&quot;lowercase:&quot;</span>,<span class="hljs-built_in">str</span>.lower(s)) <span class="hljs-comment">#str.lower/upper()</span><br>print(<span class="hljs-string">&quot;uppercase:&quot;</span>,<span class="hljs-built_in">str</span>.upper(s)) <br><br>print(<span class="hljs-string">&quot;reverse order:&quot;</span>,s[::-<span class="hljs-number">1</span>]) <span class="hljs-comment">#[起点:终点：步长]</span><br></code></pre></div></td></tr></table></figure><h2 id="Q3-Read-a-string-from-console-Split-the-string-on-space-delimiter-”-”-and-join-using-a-hyphen-”-”"><a href="#Q3-Read-a-string-from-console-Split-the-string-on-space-delimiter-”-”-and-join-using-a-hyphen-”-”" class="headerlink" title="Q3. Read a string from console. Split the string on space delimiter (” ”) and join using a hyphen (”-”)."></a>Q3. Read a string from console. Split the string on space delimiter (” ”) and join using a hyphen (”-”).</h2><p>(Example: input the string, ”this-is a string” and output as ”this is-a-string”)<br><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">s=<span class="hljs-string">&quot;hello world&quot;</span><br>print(<span class="hljs-string">&quot;replace:&quot;</span>,<span class="hljs-built_in">str</span>.replace(s,<span class="hljs-string">&quot;&quot;</span>,<span class="hljs-string">&quot;-&quot;</span>)) <span class="hljs-comment">#str.replace(string,&quot;&quot;,&quot;&quot;)交换前后元素</span><br></code></pre></div></td></tr></table></figure></p><h2 id="Q4-Learn-the-Python-list-operations-and-follow-the-commands-below"><a href="#Q4-Learn-the-Python-list-operations-and-follow-the-commands-below" class="headerlink" title="Q4. Learn the Python list operations and follow the commands below:"></a>Q4. Learn the Python list operations and follow the commands below:</h2><ul><li>Initialize an empty list L.  </li><li>Add 12, 8, 9 to the list.  </li><li>Insert 9 to the head of the list;  </li><li>Double the list. (e.g. change L = [1, 2, 3] to L = [1, 2, 3, 1, 2, 3])  </li><li>Remove all 8 in the list.  </li><li>Reverse the list.  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">L=[]<br>print(L)<br><br>L.append(<span class="hljs-number">12</span>) <span class="hljs-comment">#List.append() 列表在末尾添加</span><br>L.append(<span class="hljs-number">8</span>)<br>L.append(<span class="hljs-number">9</span>)<br>print(L)<br><br>L.insert(<span class="hljs-number">0</span>,<span class="hljs-number">9</span>) <span class="hljs-comment">#list.insert(position,element) 在列表的指定位置添加一个元素</span><br>L=[<span class="hljs-number">9</span>]+L <span class="hljs-comment">#另一种方式</span><br><br>L=L+L<br>print(L)<br>L=L*<span class="hljs-number">2</span> <span class="hljs-comment">#另一种方法</span><br>L=L.extend(L) <span class="hljs-comment">#list.extend(list) 在列表的末尾添加一个列表</span><br>print(L)   <br><br>number_eights=L.count(<span class="hljs-number">8</span>) <br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,number_eight):<br>    L.remove(<span class="hljs-number">8</span>)  <span class="hljs-comment">#list.remove(element) 在列表中移除第一个【element】元素</span><br><span class="hljs-comment">#另一种解决办法</span><br><span class="hljs-keyword">while</span> <span class="hljs-number">8</span> <span class="hljs-keyword">in</span> L:<br>     L.remove(<span class="hljs-number">8</span>)<br><br>L.reverse() <span class="hljs-comment">#list.reverse() 列表内倒序</span><br>print(L)<br></code></pre></div></td></tr></table></figure><h1 id="Q5-Learn-Python-matrix-operations-by-completing-the-following-tasks"><a href="#Q5-Learn-Python-matrix-operations-by-completing-the-following-tasks" class="headerlink" title="Q5. Learn Python matrix operations by completing the following tasks:"></a>Q5. Learn Python matrix operations by completing the following tasks:</h1></li><li>Create a 3x2 matrix named A, with all ones.</li><li>Create a 3x2 matrix named B, where $𝐵 =\begin{bmatrix} 1&amp;2\\ 3&amp;4 \\ 5&amp;6 \end{bmatrix}$  </li><li>Print A and B.</li><li>Transpose A to be a 2x3 matrix.</li><li>Multiply matrix A with matrix B and store the output in matrix C.</li><li>Print the dimensions of C.<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <span class="hljs-comment">#import packagename 加载库  import packagename as nickname 加载并替换库的名字</span><br>A=np.ones(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>) <span class="hljs-comment">#numpy.ones(row,line)</span><br>print(A)  <br><br>B=np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]]) <span class="hljs-comment">#numpy.array() 创建矩阵，用法同左边</span><br>print(B)<br><br>print(A,B)<br><br>A=A.reshape((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)) <span class="hljs-comment">#numpy.reshape((row,line)) 重新改写矩阵的大小</span><br>A=np.transpose(A) <span class="hljs-comment">#numpy.transpose(matrix) 返回转置矩阵</span><br>print(A)<br><br>C=A @ B <span class="hljs-comment">#@ 矩阵叉乘</span><br>print(C.shape) <span class="hljs-comment">#matrix.shape 矩阵的大小</span><br></code></pre></div></td></tr></table></figure><h1 id="Q6-Use-𝑀-for-the-following-tasks"><a href="#Q6-Use-𝑀-for-the-following-tasks" class="headerlink" title="Q6. Use 𝑀 for the following tasks,"></a>Q6. Use 𝑀 for the following tasks,</h1><script type="math/tex; mode=display">𝑀 =\begin{bmatrix}−2&−4&2 \\ −2&1&2 \\ 4&2&5 \end{bmatrix}</script></li><li>Calculate the eigenvalues and eigenvectors for M. (hint: use numpy.linalg.eig)    </li><li>Use matplotlib to plot the eigenvalues in a graph.    </li><li>Save the eigenvalues into a file named “eig.npy” (hint: use numpy.save).   </li><li>Load the saved file into a new variable called load_eig and print the values.   <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> plot <span class="hljs-keyword">as</span> plt<br>M=[[-<span class="hljs-number">1</span>,-<span class="hljs-number">4</span>,<span class="hljs-number">2</span>],[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">5</span>]]<br>eignval,eignvect=np.linalg,eig(M) <span class="hljs-comment">#numpy.linalg,eig(Matrix) 返回两个值，第一个是特征值，第二个是特征向量</span><br>print(eignval)<br><br>plt.plot(eignval)<br><br>np.save(<span class="hljs-string">&quot;eig&quot;</span>,eigval) <span class="hljs-comment">#numpy.save(&quot;filename&quot;,value) 将值在当前目录下以“filename.npy”储存</span><br><br>load.eig=np.load(<span class="hljs-string">&quot;eigval.npy&quot;</span>) <span class="hljs-comment">#numpy.load(&quot;path&quot;) 返回加载的文件</span><br></code></pre></div></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>课后练习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1. 机器学习简介和Python的基本操作</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/1.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%92%8CPython%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/1.%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B%E5%92%8CPython%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="机器学习简介和Python的基本操作"><a href="#机器学习简介和Python的基本操作" class="headerlink" title="机器学习简介和Python的基本操作"></a>机器学习简介和Python的基本操作</h1><blockquote><p>新加坡国立大学(NAU)/OTH927/2021/1/23<br>Terence Sim (tsim@comp.nus.edu.sg) /Karen Boh/ Sanka Rasnayaka(Assistant)</p><h2 id="Before-the-course…"><a href="#Before-the-course…" class="headerlink" title="Before the course…"></a>Before the course…</h2><ul><li>Software and environment: Anaconda and Opencv  </li><li>Ultimate Project: Traffic Sign Recognition   </li><li>There’s a individual Quiz on lecture 5  </li></ul></blockquote><h2 id="人工智能的产生"><a href="#人工智能的产生" class="headerlink" title="人工智能的产生"></a>人工智能的产生</h2><p>lines of codes programming forces people to find a way to teach the program to do things.<br>example: makeup transfer<br>example: auto ping-pong machine</p><h2 id="计算机视觉简介"><a href="#计算机视觉简介" class="headerlink" title="计算机视觉简介"></a>计算机视觉简介</h2><p>计算机视觉可以大致的被分为三个大类：  </p><ul><li>3D建模(3D Construction)<br>例子：敦煌莫高窟的3D建模（来自武汉大学）  </li><li>图像渲染(Image Rendering)<br>例子：Google Pixel<br>其搭载的增强现实算法能够对周围的图像进行实时渲染  </li><li>图像检测(Pattern Recongnition)<br> 例子：都灵的图像识别装置<br> 人们穿戴对应的设备行走，设备能够识别他周围的物品   </li></ul><p>计算机视觉可以在各个领域帮助到人们，在医学领域帮助医生识别X光片，在自动驾驶领域，自动驾驶汽车依靠车身上的传感器和相机识别道路上的物体，在体育竞技领域，计算机视觉能够帮助人们更好的训练运动员的运动姿势。世界上第一张人脸检测的图片由Dr.Sung Kah Kay在1996年完成。    </p><p>计算机科学的知识架构：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210123230629.png" alt="">  </p><h2 id="Python-的基本操作"><a href="#Python-的基本操作" class="headerlink" title="Python 的基本操作"></a>Python 的基本操作</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>  Python 不需要编译，是机器学习的首选语言之一，有非常多的库能够被调用。<br>  Python支持超大的数字运算<br>  编译环境（IDE）：Anaconda， 适用于大数据环境    </p><blockquote><p>不要使用 Python 2.x   </p><ul><li>Python的IDE思路：REPL<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210123231447.png" alt=""><br>Reading： 读取来自键盘等的输入<br>Evalueate： 将输入进行Evaluate，其结果通常是一个数值，这个数值最终会被编译器输出（Print）<br>在输出后，这个程序将等待下一次的输入，形成一个循环  <h3 id="赋值和函数定义"><a href="#赋值和函数定义" class="headerlink" title="赋值和函数定义"></a>赋值和函数定义</h3>Python可以不用声明变量的类型，直接对其进行赋值，其变量的赋值类型取决于赋值<br>Python支持同时对多个变量进行赋值：  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">a,b=c,d <br>```  <br><br>定义函数的结构：  <br>``` Python<br>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">functionname</span>(<span class="hljs-params">variable</span>):</span>  <br>      <span class="hljs-keyword">return</span> value<br>    <span class="hljs-comment"># 也可以不需要返回值</span><br></code></pre></div></td></tr></table></figure> 另一种定义方式：  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">    function= <span class="hljs-keyword">lambda</span> return_variable: options  <br>```  <br>例如：  <br>  ``` Python<br>   S= <span class="hljs-keyword">lambda</span> x: x*x  <br></code></pre></div></td></tr></table></figure>在Python中，函数的定义和调用有如下特点：</li></ul><ol><li>嵌套调用：<br><code>funcationname(funcationname(variable))</code>  </li><li>在Python中，变量可以传递给函数，<strong>函数也可以传递给变量</strong>。<br>例如：  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">function</span> (<span class="hljs-params">n</span>):</span><br>    n*n<br>    <span class="hljs-keyword">return</span> n<br>foo=function(<span class="hljs-number">5</span>)<br><span class="hljs-comment"># 此时foo的类型是一个函数</span><br>foo(<span class="hljs-number">10</span>)<br><span class="hljs-comment"># &gt;&gt; 100</span><br></code></pre></div></td></tr></table></figure></li><li>可以在定义函数的部分嵌套定义其他函数：  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">function</span>(<span class="hljs-params">variable1</span>):</span><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">subfuntion</span>(<span class="hljs-params">variable2</span>):</span><br>     <span class="hljs-keyword">return</span> variable2<br> <span class="hljs-keyword">return</span> variable1<br></code></pre></div></td></tr></table></figure><h3 id="条件结构-if-else"><a href="#条件结构-if-else" class="headerlink" title="条件结构(if-else)"></a>条件结构(if-else)</h3>条件语句的基本结构： <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">if</span> conditon:<br>  options<br></code></pre></div></td></tr></table></figure>例如：<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compare</span>(<span class="hljs-params">a,b</span>):</span><br>  <span class="hljs-keyword">if</span> a&gt;b:<br>    <span class="hljs-keyword">return</span> a<br>  <span class="hljs-keyword">return</span> b<br>compare(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><span class="hljs-comment">#&gt;&gt; 4</span><br></code></pre></div></td></tr></table></figure>在条件语句中可以同时并存多个条件：<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">if</span> condition1:<br>  options<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> condition2:<br>  options<br><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>  condtion3:<br>  options<br><span class="hljs-keyword">else</span>:<br>  options<br></code></pre></div></td></tr></table></figure><h3 id="循环结构"><a href="#循环结构" class="headerlink" title="循环结构"></a>循环结构</h3></li><li>通过函数定义的返回值来进行循环 (recursion)<br>例如：  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">factorial</span>(<span class="hljs-params">n</span>):</span><br>  <span class="hljs-keyword">if</span> n==<span class="hljs-number">1</span>:<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>  <span class="hljs-keyword">return</span> n*factorial(n-<span class="hljs-number">1</span>)<br>factorial(<span class="hljs-number">5</span>)<br><span class="hljs-comment">#&gt;&gt; 120</span><br><span class="hljs-comment">#5*4*3*2*1</span><br></code></pre></div></td></tr></table></figure></li><li>通过for循环语句来进行循环(for-range) <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">for</span> innnervariable <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span> <span class="hljs-comment">#usually is range(a,b) a to b do n++</span><br>options<br></code></pre></div></td></tr></table></figure>例如：  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">factorial</span>(<span class="hljs-params">n</span>):</span><br> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,n+<span class="hljs-number">1</span>)<br>   result=result*n<br> <span class="hljs-keyword">return</span> result<br> factorial(<span class="hljs-number">5</span>) <br> <span class="hljs-comment">#&gt;&gt; 120  </span><br></code></pre></div></td></tr></table></figure>要注意 <code>range(a,b)</code> 是不包括b的：  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> (<span class="hljs-number">0</span>,<span class="hljs-number">4</span>)<br>   print(a) <br><span class="hljs-comment">#&gt;&gt; 3  </span><br></code></pre></div></td></tr></table></figure>这样的循环结构没有自增加（<code>x++</code>）的存在</li></ol></blockquote><ol><li>通过while语句进行循环<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">while</span> (condition):<br>  options<br></code></pre></div></td></tr></table></figure>例如：<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gcd</span>(<span class="hljs-params">a,b</span>):</span> <span class="hljs-comment">#最大公约数</span><br>  <span class="hljs-keyword">while</span> (b&gt;<span class="hljs-number">0</span>):<br>    r=a&amp;b<br>    a,b=b,r<br> <span class="hljs-keyword">return</span> a<br></code></pre></div></td></tr></table></figure><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3>在Pyhton中，字符串由双引号””或者单引号’’定义。<br>字符串支持加减法：<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-string">&#x27;hello&#x27;</span>+<span class="hljs-string">&#x27;world&#x27;</span><br><span class="hljs-comment">#&gt;&gt; &#x27;helloworld&#x27;</span><br></code></pre></div></td></tr></table></figure>也支持乘法（重复多次）：<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-string">&#x27;hello&#x27;</span>*<span class="hljs-number">3</span><br><span class="hljs-comment">#&gt;&gt; &#x27;hellohellohello&#x27; </span><br></code></pre></div></td></tr></table></figure><ul><li>字符串的传递<br>字符串可以传值给变量（类型是字符串），可以通过[起点：终点：步长]访问字符串中的特定位置的字符。<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">a=<span class="hljs-string">&#x27;helloworld&#x27;</span><br>print(a[<span class="hljs-number">2</span>])<br><span class="hljs-comment">#&gt;&gt;&#x27;l&#x27;</span><br>print(a[<span class="hljs-number">0</span>:<span class="hljs-number">4</span>])<br><span class="hljs-comment">#&gt;&gt;&#x27;hello&#x27;</span><br>&gt; Python 中的序号是从<span class="hljs-number">0</span>开始的<br>print(a[::-<span class="hljs-number">1</span>] )<br><span class="hljs-comment">#&gt;&gt;&#x27;dlrowolleh&#x27;</span><br>b=<span class="hljs-string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span><br>print(a[<span class="hljs-number">1</span>:<span class="hljs-number">15</span>:<span class="hljs-number">2</span>])<br><span class="hljs-comment">#&gt;&gt;&#x27;bdfhjln&#x27;</span><br></code></pre></div></td></tr></table></figure><code>len()</code>函数将返回字符串的长度<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-built_in">len</span>(hello)<br><span class="hljs-comment">#&gt;&gt; 5</span><br></code></pre></div></td></tr></table></figure>[]默认的访问顺序是从左到右，负号（-）表示从右到左的访问顺序。   <h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3>列表（list）是一种参数类型，例如：<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">x=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]<br><span class="hljs-built_in">type</span>(x)<br><span class="hljs-comment">#&gt;&gt; list</span><br></code></pre></div></td></tr></table></figure>列表用[]来表示，列表也可以嵌套。<br>列表中的元素可以是任何类型。<br>和字符串一样，可以用[]来访问列表中特定的某一个或者多个元素。    </li><li>列表的操作  </li></ul><ol><li>append()<br><code>append()</code>函数将在列表最后一位加上()内的字符串后，输出整个字符串<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">x=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]<br>x.append(<span class="hljs-number">2</span>)<br>print(x)<br><span class="hljs-comment">#&gt;&gt;[1,2,3,4,2]</span><br></code></pre></div></td></tr></table></figure></li><li>列表理解（list comprehension）<br>在列表的[]中填入生成列表的方法：<br>例如：<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python">x=[a <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">8</span>)]<br><span class="hljs-built_in">print</span> x<br><span class="hljs-comment">#&gt;&gt; [1,2,3,4,5,6,7]</span><br>y=[square(a) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> x]<br><span class="hljs-built_in">print</span> y<br><span class="hljs-comment">#&gt;&gt;[1,4,9,16,25,36,49]</span><br></code></pre></div></td></tr></table></figure>可以利用列表理解来过滤某些元素:<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">iseven</span>(<span class="hljs-params">n</span>):</span><br>    <span class="hljs-keyword">return</span> n%<span class="hljs-number">2</span>==<span class="hljs-number">0</span><br>x=[a <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-number">8</span>)]<br>y=[square(a) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> x <span class="hljs-keyword">if</span> iseven(a)]<br><span class="hljs-built_in">print</span> y<br><span class="hljs-comment">#&gt;&gt;[4,16,36]  </span><br></code></pre></div></td></tr></table></figure></li></ol></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>讲义</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3. 图像处理</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/3.%20%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/3.%20%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 60%;<br>    padding-left: 30%;<br>}</style></p><h1 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h1><h2 id="成像原理与数字化"><a href="#成像原理与数字化" class="headerlink" title="成像原理与数字化"></a>成像原理与数字化</h2><ul><li>小孔成像（Pinhole）<br>小孔成像的基本原理如下图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207142920.png" alt="">    </li><li>透镜成像<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207143053.png" alt="">  </li><li><p>CCD/CMOS（电荷耦合）成像<br>在CCD成像当中，通过透镜后的像会呈现在CMOS上，COMS会将呈现数字化，这一过程中有两个重要的步骤：  </p><ol><li><strong>抽样</strong>（Sampling）<br>将图像转化为有限的单位像素，如图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207144301.png" alt="">    </li><li><strong>量化</strong>（Quantization）<br>用整数表示单位像素的值，对于8bit而言，单位像素的明暗程度以0~255的灰度值来表示，0表示黑色，255表示白色。  </li></ol><p>现在，一幅黑白的图像中的每一个单位像素点都用一个整数来表示其黑白的程度，那么整张图片就可以用一个只有整数的矩阵来表示（单通道）。<br>对于彩色的图像，通常以RGB（红色、绿色、蓝色）的三种程度（三通道）来进行量化，因此彩色图片的一个单位像素点以一个三维的向量，通常是$[R,G,B]$来表示。最终三个矩阵表示一幅彩色图像，这个过程叫做张量（Tensor）。  </p><h2 id="点处理-Point-Processing"><a href="#点处理-Point-Processing" class="headerlink" title="点处理(Point Processing)"></a>点处理(Point Processing)</h2><p>图像的点处理是： 设定图像上一个像素值$r(x,y)$,经过处理$s(x,y)=T(r(x,y))$后，得到同一位置的像素$s(x,y)$。<br>注意：</p></li></ul><ol><li>$(x,y)$表示坐标。</li><li>不同的图像处理库其坐标系统的原点设置不同，y的取值设定也不同。  </li><li>$T$只能是单调（通常是单调递增）的函数。<h3 id="常见的点处理变换"><a href="#常见的点处理变换" class="headerlink" title="常见的点处理变换"></a>常见的点处理变换</h3></li></ol><ul><li><p>阈值变换(Thresholding)<br>函数图像<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207152536.png" alt=""><br>阈值函数可分为两种：软阈值函数（左）和硬阈值函数/二值化函数(Hard thresholding/Binarization，右),它们的作用都是将像素转换成黑白像素。<br>图像中的$m$点称为阈值，高于阈值的像素将会被强化为近黑色/黑色的像素值，低于阈值的像素值将被弱化为近白色/白色的像素值。  </p></li><li><p>像素反转</p><script type="math/tex; mode=display">s=L-1-r</script><p>$L$表示最大的像素值。<br>作用是将像素值进行翻转，白色变为黑色，黑色变为白色。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207154347.png" alt="">  </p></li><li><p>对数变换  </p><script type="math/tex; mode=display">s=clog(1+r)</script><p>对数变换能够扩展低灰度值（突出过曝区域的细节）而压缩高灰度值（突出过暗区域的细节），从而增强图像的清晰度。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207154929.png" alt="">   </p></li><li><p>幂变换</p><script type="math/tex; mode=display">s=cr^γ</script><p>$γ$是幂指数，显示器中的伽玛校正(Gamma Correction)即调整该值使得显示器整体偏亮或偏暗。 当$0&lt;γ&lt;1$时，显示器偏暗，$γ&lt;1$时，显示器偏亮。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207155330.png" alt=""><br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207155454.png" alt="">   </p></li><li><p>折线(Piecewise Linear Curves)<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207155745.png" alt="">   </p></li></ul><h3 id="亮度直方图-Historgram"><a href="#亮度直方图-Historgram" class="headerlink" title="亮度直方图(Historgram)"></a>亮度直方图(Historgram)</h3><p>亮度直方图的横轴是像素值，纵轴是该像素值内的像素点个数，它反映了黑白图像整体的像素分布情况。<br>如果直方图在白色区域内比较集中，图像偏亮，直方图窄黑色区域内比较集中，图像偏暗。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207160828.png" alt=""><br>图像在直方图上的最大分布范围（即横轴的宽度）称为对比度(Contrast)，直方图窄的图像对比度低。通常情况下，对比度越高图像越清晰。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207160857.png" alt="">    </p><ul><li>直方图均衡(Historgram equalization)<br>直方图均衡是一种用于增强图像对比度的同时均衡图像亮度的方法（即拉宽和拖平直方图）。<br>直方图可以视为反映了每一个像素值在整个图像的占比关系（$\frac{n_w}{n}$，n表示图像像素点总数，$n_w$表示像素值为w的像素点数量），因此整个直方图可以被概率分布函数化：  <script type="math/tex; mode=display">p_r(w)=p(r=w)=\frac{n_w}{n},w=0,1,...,255</script>应用变换：  <script type="math/tex; mode=display">s=T(r)=(L-1)\int_0^rp_r(w)dw</script>使得:  <script type="math/tex; mode=display">P_s(s)=P_r(r)|\frac{dr}{ds}|=\frac{1}{L-1}</script><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207162034.png" alt=""><br>由于w并不是连续的，因此最终的直方图并非是完全扁平的矩形。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207164743.png" alt=""></li><li>局部增强(Local enhancement)<br>对局部的一些像素群（例如以某个像素为中心$9 × 9$或$3 × 3$的像素）应用直方图均衡的方法称为局部增强。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210207163241.png" alt="">   <h2 id="相邻处理（Neighborhood-Processing）"><a href="#相邻处理（Neighborhood-Processing）" class="headerlink" title="相邻处理（Neighborhood Processing）"></a>相邻处理（Neighborhood Processing）</h2>与点处理比较，虽然相邻处理的输入值仍然是一个像素值，但是输出值确实一个围绕输入像素值的像素集。（如下图所示）<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208124611.png" alt=""><br>常见的一些相邻处理的方法：  </li><li>均值滤波  </li><li>最值滤波  </li><li>中值滤波（像素值按大小排列，取排序位于中间位置的像素值作为中值滤波后的像素值）<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208124649.png" alt=""><br>▲上面的三张图依次是：原图（有一些细小噪点）、求平均、求中位数后的输出。  </li></ul><h3 id="滤波（Filtering）"><a href="#滤波（Filtering）" class="headerlink" title="滤波（Filtering）"></a>滤波（Filtering）</h3><p>滤波是一种应用于相邻处理的常见方法。<br>设以某个像素为中心的像素方阵称为<strong>核矩阵</strong>(Kernel/Mask matrix),以$W$记，核矩阵中的每一个像素值以$w(u,v)$表示。 将核矩阵照射至图像的某一区域$r$，使被照射区域中的像素值与对应的核矩阵中同位置的像素值一一相乘后全部相加，最后用一常数$C$调整，该过程被称为滤波。    </p><script type="math/tex; mode=display">s(x,y)=C∑_{(u,v)∈W}w(u,v)r(x+u,y+v)</script><p>整个过程可以用下图来表示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208130628.png" alt=""><br>一次滤波结束后，核矩阵平移$α$个像素单位（称为<strong>步长</strong>(Padding)），照射图像的另一个区域，重复上述过程。    </p><ul><li><p>均值滤波<br>其核矩阵如下：   </p><script type="math/tex; mode=display">\frac{1}{n}×\begin{bmatrix} 1&1&...&1\\...&...&...&...\\ 1&1&...&1 \\ 1&1&...&1\end{bmatrix}</script><p>n表示方阵元素的数量。<br>这样最终的输出结果是取像素点周围领域的平均值作为响应输出，最终的图像会被模糊化。  </p></li><li><p>高斯滤波(Gaussian Filtering)<br>高斯滤波的核矩阵内的元素在三维上符合标准高斯/标准正态分布，且最高点在核矩阵中心处，如下图所示。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208131837.png" alt=""><br>其具体的计算公式为：  </p><script type="math/tex; mode=display">G(x,y)=\frac{1}{2πσ^2}e^{-\frac{x^2+y^2}{2σ^2}}</script><p>$σ$是标准高斯分布中的方差，$σ$较小时，图像的峰值窄且高。<br>例如当$σ=1.4$时，其核矩阵可以取：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208132110.png" alt=""><br>高斯滤波的作用是<strong>将图像模糊化</strong>，使图像呈现一种毛玻璃的质感。运用高斯滤波处理图像的方法又被称为<strong>高斯模糊</strong>(Gaussian Blurring)。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208132748.png" alt=""><br><strong>高斯模糊的重要作用是将图像中的噪点通过模糊化图像的方法移除。</strong></p></li></ul><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p><strong>卷积</strong>(Convolution)是一种常用于图像处理的方法。<br>设核矩阵(又被称为卷积核)的像素分布可以表示为$h$,原图的像素分布表示为$f$，卷积有如下公式：</p><script type="math/tex; mode=display">g(x)=\int_{-∞}^∞f(τ)h(x-τ)dτ</script><ul><li>卷积核的正则化<br>如果要使得图像的整体亮度在卷积前后不发生改变，卷积核必须被正则化，即卷积核内所有元素的和必须是1。</li></ul><h2 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h2><p>边缘（Edges）是图像中像素值变化急剧(Sharply)的部分，常见的边缘有如下图所示的四大类。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208134126.png" alt=""><br>边缘检测是以特殊的卷积核（称为算子（Operator））对图像进行处理。<br>常见的算子有如下几种：  </p><ul><li><p>Roberts算子<br>Roberts算子是两个能够强化图像的边缘部分的核矩阵：</p><script type="math/tex; mode=display">G_x=\begin{bmatrix}  +1&0\\0&-1   \end{bmatrix}</script><script type="math/tex; mode=display">G_y=\begin{bmatrix}  0&+1\\-1&0   \end{bmatrix}</script><p><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208133900.png" alt="">    </p></li><li><p>索伯算子(Sobel’s Operator)<br>Sobel算子的两个卷积核形式：  </p><script type="math/tex; mode=display">\begin{bmatrix} -1&0&+1\\-2&0&+2\\-1&0&+1 \end{bmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix}+1&+2&+1\\0&0&0\\-1&-2&-1\end{bmatrix}</script><p>两个卷积核的特征是卷积核正中的纵列或行列为0，用于检测图像的纵向/横向边缘。<br>PIL或者是OpenCV中有对应的库可以执行Sobel边缘检测。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208140605.png" alt="">    </p></li><li><p>拉普拉斯算子(Laplacian operator)<br>函数$F(x,y)$的梯度可由梯度公式得到：  </p><script type="math/tex; mode=display">G(x,y)=\frac{∂F(x,y)}{∂x}cos(σ)+\frac{∂F(x,y)}{∂y}sin(σ)</script><p>定义拉普拉斯算子（一阶）$▿f|(x_0,y_0)=(f_x(x_0,y_0),f_y(x_0,y_0))$，其二阶形式：</p><script type="math/tex; mode=display">▿^2f(x,y)=\frac{∂^2f(x,y)}{∂x^2}+\frac{∂^2f(x,y)}{∂y^2}</script><p>在x方向上可以近似由差分表示：  </p><script type="math/tex; mode=display">\frac{∂^2f(x,y)}{∂x^2}=f(x+1,y)+f(x-1,y)-2f(x,y)</script><p>在y方向上同理，最终得到二阶拉普拉斯算子的表达式：</p><script type="math/tex; mode=display">▿^2f(x,y)=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)</script><p>得到拉普拉斯算子的卷积核形式：  </p><script type="math/tex; mode=display">\begin{bmatrix}  0&1&0\\1&-4&1\\0&1&0\end{bmatrix}</script></li><li><p>拉普拉斯-高斯算子（LoG operator）<br>由表达式：  </p><script type="math/tex; mode=display">▽^2g(x,y)=-\frac{1}{2πσ^4}(2-\frac{x^2+y^2}{σ^2})e^{-\frac{x^2+y^2}{2σ^2}}</script><p>所得到的算子的卷积核形式：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208141000.png" alt="">   </p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>讲义</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3. 课后练习-图像处理</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/3.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/3.a%20%E8%AF%BE%E5%90%8E%E7%BB%83%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 40%;<br>    padding-left: 10%;<br>}</style></p><h1 id="课后练习2"><a href="#课后练习2" class="headerlink" title="课后练习2"></a>课后练习2</h1><p>Required libs:Numpy PIL Scipy Matplotlib cv2</p><h2 id="Q1-Write-a-python-script-to-open-the-“lena-png”-file-using-opencv"><a href="#Q1-Write-a-python-script-to-open-the-“lena-png”-file-using-opencv" class="headerlink" title="Q1. Write a python script to open the “lena.png” file using opencv."></a>Q1. Write a python script to open the “lena.png” file using opencv.</h2><ul><li>Display the opened image in a new window named “Display Lena”</li><li>Save the image to a new file named “lena_resaved.png”<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/lena.png" alt="">  <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> cv2 <span class="hljs-keyword">as</span> cv<br>img = cv.imread(<span class="hljs-string">&quot;lena.png&quot;</span>) <span class="hljs-comment"># cv2.imread(&#x27;path&#x27;)  read the img</span><br>cv.imshow(<span class="hljs-string">&quot;Display Lena&quot;</span>,img) <span class="hljs-comment">#cv2.imshow(windowname,path)</span><br>cv.waitkey(<span class="hljs-number">0</span>) <span class="hljs-comment">#to let the window display until clicking/pressing</span><br>cv.imwrite(<span class="hljs-string">&quot;lena_resaved.png&quot;</span>,img) <span class="hljs-comment">#cv2.imwrite(filename,path,params)</span><br></code></pre></div></td></tr></table></figure><h2 id="Q2-Use-PIL-and-Matplotlib-libraries-for-Q2"><a href="#Q2-Use-PIL-and-Matplotlib-libraries-for-Q2" class="headerlink" title="Q2. Use PIL and Matplotlib libraries for Q2."></a>Q2. Use PIL and Matplotlib libraries for Q2.</h2>Use “lena.png” to perform following operations and save the images:  </li><li>Crop a section from the image whose vertices are (100,100), (100,400), (400,100), (400,400).<br>(hint: convert the cv2 image into PIL Image)  </li><li>Rotate the cropped image by 45 degrees counter-clockwise.</li><li>Perform histogram equalization on lena.png. (hint: use ImageOps.equalize from PIL)</li><li>Use matplotlib to plot the histogram figure for both original image and processed image.<br>(hint: use histogram() function in PIL)  </li><li>Perform Max Filtering, Min Filtering, and Median Filter on lena.png. (hint: PIL.ImageFilter)  </li><li>Perform Gaussian Blur with sigma equal to 3 and 5. (hint: PIL.ImageFilter)<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/bee.png" alt=""> <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image <br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageOps <br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageFilter <span class="hljs-keyword">as</span> <span class="hljs-built_in">filter</span><br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt  <br><br>pil_img=Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;lena.png&quot;</span>) <span class="hljs-comment">#open img in pil </span><br><span class="hljs-comment">#(in cv2 lib, img is opened as array)</span><br><span class="hljs-comment"># load cv img: Image.fromarray()</span><br>pil_img.show() <span class="hljs-comment"># show the img</span><br>img_crop = pil_img.crop((<span class="hljs-number">100</span>,<span class="hljs-number">100</span>,<span class="hljs-number">300</span>,<span class="hljs-number">300</span>)) <span class="hljs-comment">#crop((start point,hight,width))</span><br>img_crop.show() <span class="hljs-comment">#show the img</span><br><br>img_rota = img_crop.rotate(<span class="hljs-number">45</span>) <span class="hljs-comment">#rotate(degree)</span><br>img_rota.show()<br><br>img_eql=ImageOps.equalize(pil_img) <br><span class="hljs-comment">#ImageOps.equalize(path) histogram equalize the imge</span><br><br>plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">256</span>),img_eql.histogram()) <br><span class="hljs-comment">#pyplot(aix,img) plot someting   </span><br><span class="hljs-comment">#img.histogram()  return the histogram</span><br>plt.show()<br>plt.show(rang(<span class="hljs-number">0</span>,<span class="hljs-number">256</span>),pil_img.histogram())<br>plt.show()  <br><br>img_max = pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.maxfilter()) <br><span class="hljs-comment">#filter.(Imagefilter.parm()) add filters</span><br>img_max.show()<br><br>img_min=pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.minfilter())<br>img_min.show()<br><br>img_mid=pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.medianfilter())<br>img_mid.show()<br><br><br>img_gus3=pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.gaussianblur(radius=<span class="hljs-number">3</span>))<br>img_gus3.show()<br><br>img_gus10=pil_img.<span class="hljs-built_in">filter</span>(<span class="hljs-built_in">filter</span>.gaussianblur(radius=<span class="hljs-number">10</span>))<br>img_gus10.show()<br><br></code></pre></div></td></tr></table></figure><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210210191614.png" alt=""></li></ul><h2 id="Q3-Colour-space-conversion-Use-Python-OpenCV-functions-to-perform-following-operations-on"><a href="#Q3-Colour-space-conversion-Use-Python-OpenCV-functions-to-perform-following-operations-on" class="headerlink" title="Q3. Colour space conversion. Use Python OpenCV functions to perform following operations on"></a>Q3. Colour space conversion. Use Python OpenCV functions to perform following operations on</h2><p>“bee.png” and save the images at each step.</p><ul><li>Read the image.</li><li>Convert the image to HSV(<strong>Hu Satuation Value:包含了三个通道：单色(H)，饱和度(S)，灰度(V)</strong>) color space.</li><li>Perform histogram equalization on V channel by cv2.equalizeHist().</li><li>Convert the result image to BGR color space.</li><li>Show the image by cv2.imshow() and save the image.<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageOps <br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageFilter <span class="hljs-keyword">as</span> <span class="hljs-built_in">filter</span><br><br>bee_img = cv2.imread(<span class="hljs-string">&quot;bee.png&quot;</span>)<br>bee_hsv = cv2.cvtColor(bee_img,cv2.COLOR_BGR2HSV)<br>bee_hsv.imshow()<br><br>bee_hsv[:,:,<span class="hljs-number">2</span>]= cv2.equalizeHist(bee_hsv[:,:,<span class="hljs-number">2</span>])<br><span class="hljs-comment"># 2 presents the channel 2: V</span><br>bee_rgb = cv2.cvtColor(bee_hsv,cv2.COLOR_HSV2BGR)<br>cv2.imshow(<span class="hljs-string">&quot;norm&quot;</span>,bee_rgb)<br><br>bee_img[:,:,<span class="hljs-number">2</span>]= cv2.equalizeHist(bee_rgb[:,:,<span class="hljs-number">2</span>])<br>bee_img = cv2.cvtColor(bee_hsv,cv2.COLOR_HSV2BGR)<br>cv2.imshow(<span class="hljs-string">&quot;rgb&quot;</span>,bee_img)<br></code></pre></div></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>课后练习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4. 分类器</title>
    <link href="/2021/02/16/Machine%20Learning-NAU/4.%20%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <url>/2021/02/16/Machine%20Learning-NAU/4.%20%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 60%;<br>    padding-left: 30%;<br>}</style></p><h1 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h1><h2 id="分类器概述"><a href="#分类器概述" class="headerlink" title="分类器概述"></a>分类器概述</h2><p>设$S=\{ω_1,ω_2,..,ω_c\}$是表示所有特征标签ω的集合，x表示数据集空间$R^n$中的特征向量，定义：<strong>分类器</strong>（Classifier）是一种能够使$R^n→S$的函数$f$。分类器能够将特征标签（labels）指定到特征向量。  </p><ul><li>图像识别的基本流程<br>输入图像—&gt;预处理-&gt;获取特征-&gt;分类<h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><strong>特征</strong>(Features)是不同类别的数据具有的用于识别其自身的属性。在机器学习中，要想对数据集进行识别和分类就必须要提取数据集的特征。<br>特征的提取并不是越多越好，不相关的特征（称为噪声（Noise features））会降低识别的准确度；具有高相关性的特征（比如：长发和女性）会让模型出现过拟合（Generalization）和模型冗余之类的其他问题。  </li><li><p>决策边界<br><strong>决策边界</strong>（Decision boundary）是二元分类中能够依据特征的分布来分出两类的边界。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208145142.png" alt="">   </p></li><li><p>过拟合问题（Generalization）<br>如果一个模型虽然可以穿过所有的数据点，但是其图像波动很大，其同样也不能描述数据的分布，（其数据的分布是无法被泛化处理），称为过拟合，或者说这个算法具有高方差的特性。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210208150529.png" alt=""></p></li></ul><h2 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h2><p><strong>贝叶斯分类器</strong>(Bayes’ classifier)被理论证明是目前最好的分类器。贝叶斯分类器依赖于模式识别(Pattern recognition)<br>想象如下的情形：我们已经测量了用于识别男女性别的特征，现在要根据这些特征来对一个未知的人判断其性别。如果你对其不做任何的测量，那么你是否还能进行分类？<br>₋答案是：如果我们知道男女性别的比例，比如男性/总的人群:$P{ω_1}=58.8%$。将频率视为概率，那么这个概率被称为<strong>先验概率</strong>（Prior probability），由于是男性的概率大于是女性的概率，因此我们将这个识别目标<strong>总是判断为概率最高的标签</strong>——即男性，那么我们判断其为男性的正确率为58.8%。<br>如何去优化这个正确率？——对识别目标进行观测：<br>假设已经观测到目标的特征$X$，对标签集$Ω$,计算所有的如果具有特性$x$,识别目标为标签$ω_i$的概率，并从中找到最大的条件概率，目标特征的标签$ω^*$即为最大的条件概率。用数学公式表达为：   </p><script type="math/tex; mode=display">ω^*=arg_{ω_i}~maxP(ω_i|x)</script><p>由计算得出的概率$P(ω_i|x)$称为<strong>后验概率</strong>(Posteriori)。<br>由贝叶斯公式：  </p><script type="math/tex; mode=display">P(A|B)=\frac{P(B|A)×P(A)}{P(B)}</script><p>那么后验概率可以转化为：</p><script type="math/tex; mode=display">P(ω_i|x)=\frac{P(x|ω_i)·P(ω_i)}{P(x)}</script><p>带入分类器公式：   </p><script type="math/tex; mode=display">ω^*=arg_{ω_i}~max\frac{P(x|ω_i)·P(ω_i)}{P(x)}</script><p>由于$P(x)$是一个常数，那么最大值函数可以被简化为求$P(x|ω_i)·P(ω_i)$的最大值：  </p><script type="math/tex; mode=display">ω^*=arg_{ω_i}~maxP(x|ω_i)·P(ω_i)</script><p>这就是贝叶斯分类器公式。    </p><ul><li><p>特殊情况<br>如果先验概率是均等的：</p><script type="math/tex; mode=display">P(ω_i)=P(ω_1)=...=P(ω_n)=C</script><p>那么分类器公式还能被简化为：  </p><script type="math/tex; mode=display">ω^*=arg_{ω_i}~maxP(x|ω_i)</script><p>称为最大可能公式（Maximunm Likelihood）。   </p><p>如果分类器中只有两个标签$ω_1,ω_2$:<br>那么设定：   </p><script type="math/tex; mode=display">g(x)=P(ω_1|x)-P(ω_2|x)</script><p>如果$g(x)&gt;0$则判断为$ω_1$,反之判断为另一类。$g(x)$称为判别函数（Discriminant function）。   </p></li><li><p>代价/损失（Cost）<br>设对于标签集$\{ω_1,ω_2,…,ω_c\}∈C$,$λ_{ij}$是分类器判断为$ω_i$但实际上的标签是$ω_j$所作出的<strong>代价</strong>（Cost）。<br>规定在$λ$中，当$i=j$时，$λ_{ij}=0$。<br>那么二元的贝叶斯分类器的代价函数为：   </p><script type="math/tex; mode=display">\frac{P(x|ω_1)}{P(x|ω_2)}>\frac{λ_{12}-λ_{22}}{λ_{21}-λ_{11}}·\frac{P(ω_2)}{P(ω_1)}.......ω_1</script><script type="math/tex; mode=display">\frac{P(x|ω_1)}{P(x|ω_2)}<\frac{λ_{12}-λ_{22}}{λ_{21}-λ_{11}}·\frac{P(ω_2)}{P(ω_1)}.......ω_2</script><p>如果$λ_{12}=λ_{21}=1$且$λ_{11}=λ_{22}=0$,称$P(x|ω_i)P(ω_i)$为MAP方程。</p></li></ul><blockquote><p>贝叶斯分类器被证明是理论上误差最小的分类器。     </p></blockquote><p>运用贝叶斯分类器需要知道在有特征$x$的条件下是分类标签$ω_i$的概率——$P(ω_i|x)$，称为可能性（Likelyhood）。在实际运用当中，一般是从数据集中估计这个概率（采用抽样检测等方法），这个估计出的概率通常是不准确的。<br>这个抽样检测的原则是：<strong>如果要创建一个D维（D是特征向量X的维度，即特征的数量）的直方图，一般而言至少需要$10^D$的训练样本。</strong></p><h2 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h2><p>解决贝叶斯分类器需要的训练样本数量大的问题的其中一个办法是假设所有的特征之间是独立的，根据概率论，有：</p><script type="math/tex; mode=display">P(XY)=P(X)P(Y)</script><p>假设特征向量$x=[x_1,x_2,…,x_D]^T$，有：</p><script type="math/tex; mode=display">ω^*=arg_{ω_j}~maxP(x|ω_j)P(ω_j)</script><script type="math/tex; mode=display">ω^*=arg_{ω_j}~maxP(ω_j)Π_{i=1}^DP(x_i|ω_j)</script><blockquote><p>在实际中，由于$P(x_i|ω_j)∈[0,1]$，因此$Π_{i=1}^DP(x_i|ω_j)$的乘积可能会下溢（非常趋近0）。因此对两边取log函数将乘法项目转为加法项防止下溢。   </p></blockquote><script type="math/tex; mode=display">ω^*=arg_{ω_j}~max~log[P(ω_j)]+∑_{i=1}^Dlog[P(x_i|ω_j)]</script><p>这种分类器公式称为朴素贝叶斯分类器（Naive Bayes）   </p><h2 id="K邻近算法"><a href="#K邻近算法" class="headerlink" title="K邻近算法"></a>K邻近算法</h2><p>K邻近算法（K-Nearest Neighbor,KNN）是一种不依赖概率而直接求得决策边界的办法。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210215095611.png" alt=""><br>如上图，设想现在样本空间内有两类样本，新加入一个x到样本空间内，设定$k=5$,计算x到样本空间内所有点的距离，最终取5个距离x最近的样本点，这五个样本点中哪一种类别的样本点多x就是哪一种类别。<br>通常情况下，标签数和K是都是奇数。<br>有数学证明在训练样本足够多的条件下， KNN的错误概率相比于贝叶斯更小。   </p><script type="math/tex; mode=display">P(error_{KNN})⪙P(error_{Bayes})</script><ul><li>距离度量（Distance Metrics）<br>设定距离度量函数$D(x,y)$,具有非负性、唯一性、和三角矢量性。   <script type="math/tex; mode=display">D_p(x,y)=(∑_{i=1}^n|x_i-y_i|^p)^{1/p}</script>x,y为向量。p称为范数（Norm）。<br>为了避免x，y的数值过于悬殊，人为地添加权重$w_i$，有：  <script type="math/tex; mode=display">D_p(x,y)=(∑_{i=1}^nw_i|x_i-y_i|^p)^{1/p}</script></li></ul><h2 id="其他分类器"><a href="#其他分类器" class="headerlink" title="其他分类器"></a>其他分类器</h2><ul><li>神经网络<br>神经网络是目前最热门的分类器方法，它模拟了神经元的传递过程，即输入信号——处理信号——接收信号。 </li><li>向量机（Support vector machine,SVM）<br>向量机的目的是为了找到一个线性的决策边界。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210215102440.png" alt="">  </li></ul><h2 id="性能评估"><a href="#性能评估" class="headerlink" title="性能评估"></a>性能评估</h2><p>现在要对一个分类器的效果进行评估，方法是用另一组数据集去测试分类器的性能。在实际运用中，通常把训练集划分为两部分：训练集和测试集。 测试集不会被训练。 将测试集放入分类器后，分类器得出的标签和测试集中的标签进行对比。   </p><h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p>有如下将测试结果可视化的方法，称为<strong>混淆矩阵</strong>(Confusion matrix)方法：<br>将横轴作为实际的标签，纵轴作为预测的标签，每一格表示“实际为标签i/但是预测为标签j”的频率，做出矩阵，如下图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210217114634.png" alt=""><br>对角线上频率的总和即为训练集的正确率。<br>混淆矩阵能够容易的表现出分类器错误的分类情况。  </p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p><strong>交叉验证</strong>（K-fold cross validation）能够最大程度的避免测试集发生的“偶然正确（称为福禄克测试，Fluke test）”，具体的做法是：<br>将数据集平均分为k份，取其中一份为测试集，剩下的为训练集。重复上述步骤直到每一份都被做过训练集。 最终分类器的准确率为所有测试的准确率的平均值。    </p><h3 id="错误类型与ROC曲线"><a href="#错误类型与ROC曲线" class="headerlink" title="错误类型与ROC曲线"></a>错误类型与ROC曲线</h3><ul><li><p>FRR<br>False Reject Rate， 表示目标正确却识别为错误的概率。   </p></li><li><p>FAR<br>False Accept Rate， 表示目标错误却识别为正确的概率。</p></li><li><p>FTE<br>Failure to Enroll Rate, 无法识别的概率。</p></li></ul><p>理想条件下，FRR和FAR都应该等于0。不断地改变分类器的阈值，将横轴为FAR,纵轴为1-FRR，作出<strong>ROC曲线</strong>(受试者工作特征曲线, Receiver operating characteristic curve)。这条曲线始终在y=x以上。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210217121830.png" alt=""><br>理想条件下，ROC曲线应该是一个L形状，即FAR=FRR=0。<br>ROC曲线围成的下夹面积，即AUC表示了系统的强壮性，AUC越大越好。<br>EER(Equal error rate)，也就是FPR=FNR的值，由于FNR=1-TPR，可以画一条从（0,1）到（1,0）的直线，找到直线与ROC曲线的交点。 交点越靠近(1,1)越好。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning-NUS 2021</category>
      
      <category>讲义</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.4. 多元分类</title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.4%20%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.4%20%E5%A4%9A%E5%85%83%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>一对多分类的实质是对一对一分类的拓展。<br>基本方法是建立一个多输出的神经网络，因此<strong>在多元分类中，最终的输出结果将是一个n维的向量</strong>，输出层的每一个输出单元用于判断是否是某一类（例如：是否是行人，是否是自行车），当判断为结果是某一类时，在理想情况下，这个网络会在这一输出单元输出1，其他的输出单元输出0，最终输出的结果是如：$h_Θ(x)≈\left[\begin{smallmatrix} 1 \\\ 0 \\\ 0 \\\ 0 \end{smallmatrix}\right]$之类的向量。<br>以前我们在训练集中用一个整数y来表示分类的标签，在多元分类中，我们使用如上所示的向量来表示分类的标签。<br>现在假设函数的模型应该为：</p><script type="math/tex; mode=display">h_Θ(x^{(i)})≈y^{(i)}</script><p>等式的左右两边输出的结果都是n维的向量。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>4. 神经网络简介</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.3. 神经单元的逻辑函数功能</title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.3%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.3%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 50%;<br>    padding-left: 20%;<br>}</style></p><h1 id="神经单元的逻辑函数功能"><a href="#神经单元的逻辑函数功能" class="headerlink" title="神经单元的逻辑函数功能"></a>神经单元的逻辑函数功能</h1><p>思考下面一个例子：<br>如果$x_1,x_2$都为二进制数，如图有四个样本分布在样本空间内，<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101151549.png" alt=""><br>那么假设函数可以写成：</p><script type="math/tex; mode=display">y=x_1 XNOR x_2</script><p>那么神经网络是否可以生成这样的函数呢？ </p><h2 id="简单逻辑函数的实现——AND-OR-NOT"><a href="#简单逻辑函数的实现——AND-OR-NOT" class="headerlink" title="简单逻辑函数的实现——AND,OR,NOT"></a>简单逻辑函数的实现——AND,OR,NOT</h2><p>为了解决这个问题，我们从AND函数入手：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101151722.png" alt=""><br>如图，输入层有两个特征$x_1$和$x_2$，他们是二进制数。目标函数为$y=x_1 AND x_2$.  </p><p><strong>观察激活函数$y=g(x)$,我们发现当$x=4$时，$y=0.99$,当$x=-4$时，$y=0.01$</strong><br>由上述激活函数的性质，为了计算这样的神经网络，首先先增加一个偏置单元 【+1】，对每个单元赋予权重：-30，20,20<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101152043.png" alt=""><br>列出真值表：  </p><script type="math/tex; mode=display">\begin{array}{lcr}x_1 & x_2 & h_θ(x) \\\    0 & 0 & g(-30)≈0 \\\   0 & 1 & g(-10)≈0  \\\  1 & 0 & g(-10)≈0  \\\  1 & 1 & g(10)≈1 \\\  \end{array}</script><p>观察最后一列，我们发现最后一列的输出事实上很接近与AND函数的结果，那么可以认为$h_θ(x) ≈ x_1 AND x_2$</p><p>那么同理，下图的神经网络最终可以生成一个类似于OR函数的假设函数。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101152850.png" alt="">  </p><p>下图的神经网络最终可以生成一个类似于NOT函数的假设函数。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101153145.png" alt=""><br>可以发现NOT是通过给对应的单元施加一个较大的负数来实现的。</p><h2 id="复杂逻辑函数的实现"><a href="#复杂逻辑函数的实现" class="headerlink" title="复杂逻辑函数的实现"></a>复杂逻辑函数的实现</h2><p>下面我们来试试生成如下的函数：  </p><script type="math/tex; mode=display">h_θ(x)=(NOT x_1)AND(NOT x_2)</script><p>分析：<br>要想使$h_θ(x)=1$,那么当且仅当$x_1=x_2=0$时成立，最终的神经网络如下图所示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101154246.png" alt="">   </p><p>现在我们可以应付本节一开头的问题了——如何使神经网络生成$y=x_1 XNOR x_2$？<br>分析： </p><script type="math/tex; mode=display">x_1XNORx_2=NOT(X_1 XOR X_2)=(x_1 AND x_2)OR((NOT x_1) AND (NOT x_2))</script><p>以逻辑表达式形式书写：</p><script type="math/tex; mode=display">h_θ(x)=(x_1.x_2)+\overline{x_1}.\overline{x_2}</script><p>将它分层：<br>第一层： 获取$x_1$和$x_2$<br>第二层：计算  $a_1^{(2)}=x_1.x_2$  和  $a_2^{(2)}=\overline{x_1}.\overline{x_2}$.<br>第三层：计算  $a_1^{(2)}+a_2^{(2)}$.<br>通过这一节的前半部分，我们已经知道了每一层所需要的函数的神经网络构建方法，最终的神经网络将是：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210101155720.png" alt="">   </p><blockquote><p>实例： 可视化Minst手写字符数据集识别</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>4. 神经网络简介</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.2. 神经网络的基本模型</title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 60%;<br>    padding-left: 20%;<br>}</style></p><h1 id="神经网络的基本模型"><a href="#神经网络的基本模型" class="headerlink" title="神经网络的基本模型"></a>神经网络的基本模型</h1><h2 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h2><ul><li>假设： 大脑对于不同功能（听觉，视觉，触觉的处理）的实现是依赖于同样的学习方法  </li><li>依据： 神经重接实验  </li><li>神经元模型<br>神经网络模拟了大脑中的神经元或者是神经网络。先来看大脑中的神经元构成，我们会发现神经元有很多的输入通道（树突），同时通过轴突给其他的神经元传递信号。  将神经元简单抽象：一个计算单元，它从输入端接收一定数目的信息，并作一些处理，并将结果传递给其他的神经元。</li></ul><p>在计算机中，我们构建一个逻辑单元，它从输入端接收数据集X，并作处理来生成一个激活函数$h_θ (x)=\frac{1}{1+e^{-θ^T X}}$<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201229183932.png" alt=""><br>在这个模型之上，输入端会额外增加一个$x_0=1$，称为偏置单元。<br>在神经网络中，$Θ$称为模型的权重，$g(z)=\frac{1}{1+e^{-z}}$称为激活函数。  </p><p>神经网络是一组神经元连接在一起的集合，如图所示<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201229184740.png" alt=""><br>第一层称为输入层，我们在这一层输入全部的特征，最后一层称为输出层，这一层的神经元输出假设的最终结果，中间的层称为隐藏层，隐藏层可能不止有一层。<br>统一地，$a_i^{(j)}$将表示第j层的第i个激活项（激活指计算并输出结果），同时，第j层到第j+1层之间的映射由参数矩阵$Θ^{(j)}$确定，那么上图就可以用公式表示为：</p><script type="math/tex; mode=display">a_1^{(2)}=g(Θ_{10}^{(1)}x_0+Θ_{11}^{(1)}x_1+Θ_{12}^{(1)}x_2+Θ_{13}^{(1)}x_3)</script><script type="math/tex; mode=display">a_2^{(2)}=g(Θ_{20}^{(1)}x_0+Θ_{21}^{(1)}x_1+Θ_{22}^{(1)}x_2+Θ_{23}^{(1)}x_3)</script><script type="math/tex; mode=display">a_3^{(2)}=g(Θ_{30}^{(1)}x_0+Θ_{31}^{(1)}x_1+Θ_{32}^{(1)}x_2+Θ_{33}^{(1)}x_3)</script><script type="math/tex; mode=display">h_{Θ}(x)=g(Θ_{10}^{(2)}a_0^{(2)}+Θ_{11}^{(2)}a_1^{(2)}+Θ_{12}^{(2)}a_2^{(2)}+Θ_{13}^{(2)}a_3^{(2)})</script><p>如果一个网络在第j层有$s_j$个单元，且在第j+1层有$s_j+1$个单元，那么矩阵$Θ^{(j)}$的维度为$s_{j+1} \times (s_j+1)$</p><h2 id="神经网络的向量化-前向传输-Forward-propagation"><a href="#神经网络的向量化-前向传输-Forward-propagation" class="headerlink" title="神经网络的向量化:前向传输(Forward propagation)"></a>神经网络的向量化:前向传输(Forward propagation)</h2><p>对如上的等式，现在将$g()$中的线性加权组合以$z^{(2)}_1,z^{(2)}_2,z^{(2)}_3$表示，那么就有：</p><script type="math/tex; mode=display">a_1^{(2)}=g(z_1^{(2)})</script><script type="math/tex; mode=display">a_2^{(2)}=g(z_2^{(2)})</script><script type="math/tex; mode=display">a_3^{(2)}=g(z_3^{(2)})</script><p>现在就能够定义三个向量使得上述等式转化为向量乘法：<br>$x= \left[\begin{smallmatrix} x_0 \\\ x_1 \\\ x_2 \\\ x_3 \end{smallmatrix}\right]$, $z^{(2)}=\left[\begin{smallmatrix} z_1^{(2)}\\\ z_2^{(2)}\\\ z_3^{(2)}\end{smallmatrix}\right]=Θ^{(1)}x$，$a^{(2)}=\left[\begin{smallmatrix} a_1^{(2)}\\\ a_2^{(2)}\\\ a_3^{(2)}\end{smallmatrix}\right]$  </p><p>那么上述等式最终就可以转化成：</p><script type="math/tex; mode=display">z^{(2)}=Θ^{(1)}x</script><script type="math/tex; mode=display">a^{(2)}=g(z^{(2)})</script><p>对于隐藏层的偏置单元，增加一项$a_0^{(2)}=1$.<br>最后计算$z^{(3)}=Θ^{(2)}a^{(2)}$,那么最终得到的假设模型将会是：    </p><script type="math/tex; mode=display">h_{Θ}(x)=a^{(3)}=g(z^{(3)})</script><p>单看layer2 和 layer3，事实上，这两层做的就是逻辑回归，但输入进逻辑回归的特征不再是原始的特征x，而是通过原始特征生成的特征$a$。<br>而$a$与$x$之间的关系通过θ来定义。 因此可以通过改变$θ$来改变输入层和隐藏层之间的关系。</p><blockquote><p>下一章将说明如何调整$θ$的值来优化假设函数。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>4. 神经网络简介</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.1. 神经网络的背景</title>
    <link href="/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%83%8C%E6%99%AF/"/>
    <url>/2021/02/14/4.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B/4.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%83%8C%E6%99%AF/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 60%;<br>    padding-left: 20%;<br>}</style></p><h1 id="神经网络的背景知识"><a href="#神经网络的背景知识" class="headerlink" title="神经网络的背景知识"></a>神经网络的背景知识</h1><h2 id="激活函数算法的局限性"><a href="#激活函数算法的局限性" class="headerlink" title="激活函数算法的局限性"></a>激活函数算法的局限性</h2><p>假设一个数据集拥有非常多的原始特征和数据量，执行激活函数算法，那么次方项、交叉项会非常的多，计算量非常的大，最终的拟合结果也不好。<br>计算机视觉中的例子：<br>计算机读取到的是图片所对应的像素强度的矩阵。 </p><blockquote><p>对于灰度图像来说，像素强度就是每一个像素的灰度值。<br>对于RGB彩色图像来说，图片上的一个像素以三个值（R,G,B）/三维向量 来进行表示  </p></blockquote><p>如果现在设计一个分类器，使得计算机能够区分一个图片的主体是否为汽车。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/472E9216086782CF8029F2818CA1027A.png" alt=""><br>以图中的pixel1 和 pixel2的位置为例，我们可以把所有数据集中pixel1和pixel2的像素强度投射到坐标轴上，如图使用一个非线性假设来对图像进行分类。<br>如果对于一个50*50像素的图片数据集，那么训练集中将包含至少2500个原始特征（7500 RGB），这时候用激活函数算法计算量会非常的大。  </p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>4. 神经网络简介</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.3. 正则化</title>
    <link href="/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.3.%20%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.3.%20%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h2><p>对于模型，如果一个模型对于数据的偏差很大，不能能够很好的拟合数据的分布，称为欠拟合，或者说这个算法具有高偏差的特性。 如果一个模型虽然可以穿过所有的数据点，但是其图像波动很大，其同样也不能描述数据的分布，（其数据的分布是无法被泛化处理），称为过拟合，或者说这个算法具有高方差的特性。 在这种情况下，模型的参数过于多（有可能代价函数正好为0），以至于可能没有足够多的数据去约束它来获得一个假设函数。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224205854.png" alt=""><br>过拟合现象往往会发生在<strong>参数过多，而训练样本过少的情况</strong>。减小过拟合现象的思路有两种： </p><ol><li>尽可能的去掉那些影响因素很小的变量，这种方法虽然解决了过拟合问题，但是损失了精度。  </li><li><strong>正则化</strong>（Regularization）  </li></ol><h2 id="代价函数的正则化"><a href="#代价函数的正则化" class="headerlink" title="代价函数的正则化"></a>代价函数的正则化</h2><p>对于代价函数：</p><script type="math/tex; mode=display">min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2</script><p>增加两个惩罚项$1000\theta^2_3$和$1000\theta^2_4$，代价函数变为：  </p><script type="math/tex; mode=display">min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+1000\theta^2_3+1000\theta^2_4</script><p>如果要最小化这个函数，那么$\theta_3$与$\theta_4$就要尽可能的接近0，那么最后拟合的结果（假设函数）：$\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$，仍然是一个类似的二次函数.<br>正则化的基本思想是<strong>如果所有的参数足够小，那么假设模型就更简单。</strong>  </p><blockquote><p>事实上，如果参数足够小，得到的函数就会越平滑，越简单，越不容易出现过拟合的问题  </p></blockquote><p>在实际上，对于大量的特征和大量的参数，比如$x_1..x_{100}$和$\theta_0…\theta_{100}$，我们无法确定哪些参数是高阶项的参数，这个时候采用的方法就是对代价函数进行修改，使得所有的参数都尽可能的小。<br>修改后的代价函数方程：  </p><script type="math/tex; mode=display">J_{\theta}=\frac{1}{2m}[\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+λ\Sigma_{j=1}^{m}\theta_j^2]</script><p>其中$λ\Sigma_{j=1}^{m}\theta_j^2$称为<strong>正则化项</strong>，它的目的是为了<strong>缩小每一项的参数</strong>。</p><blockquote><p>$\theta_0$是否正则化对结果影响不大<br>λ的作用是对“+”号的前后（前：更好的拟合训练集，后：假设函数足够简单）两项进行取舍平衡，称为正则化系数  </p></blockquote><p>如果λ被设置的太大，那么所有参数的惩罚力度被加大，这些参数最后的结构都将全部接近于0，那么最后的假设函数将会变成$h_\theta(x)=θ_0$,最终导致欠拟合。  </p><h2 id="线性回归的正则化"><a href="#线性回归的正则化" class="headerlink" title="线性回归的正则化"></a>线性回归的正则化</h2><p><strong>正则化的梯度下降算法</strong><br>在线性回归中，我们使用修改后的梯度下降算法：<br>Repeat {   </p><script type="math/tex; mode=display">θ_0:=θ_0-\alpha\frac{1}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(j)} \tag{1}</script><blockquote><p>$θ_0$  不需要正则化  </p><script type="math/tex; mode=display">θ_j:=θ_j-\alpha[\frac{1}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}θ_j] \tag{2}</script><script type="math/tex; mode=display">j=1,2,3,...,n</script><p>}<br>事实上  $\frac{1}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(j)}+\frac{λ}{m}θ_j=\frac{∂J(θ)}{∂θ_j}$<br>$\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}=\frac{∂J(θ)}{∂θ_0}$  </p></blockquote><p>如果将（2）中的$\theta_j$统一，那么就可以得到（3）：  </p><script type="math/tex; mode=display">θ_j:=θ_j（1-α\frac{λ}{m}）-\frac{α}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)} \tag{3}</script><p>由于$1-α\frac{λ}{m}&lt;1$,且只比1小一点点，也就是说，梯度下降算法每次更新的时候$θ_j$在一开始都会比原来小一点点，再进行原来的梯度下降更新  </p><p><strong>正则化的正规方程算法</strong><br>在之前的讲义中，探讨过设计两个矩阵：<br>$X=\begin{bmatrix} (x^{(1)})^T \\ …\\ (x^{(m)})^T \end{bmatrix}$ 代表有m个数据的数据集 和 $y=\begin{bmatrix} y^{(1)} \\ …\\ y^{(m)} \end{bmatrix}$ 代表训练集当中的所有的标签<br>通过：</p><script type="math/tex; mode=display">θ=(X^TX)^{-1}X^Ty</script><p>（相当于对$J(θ)$中的每一个θ求偏导数，并且使其等于0）<br>可以求出最适合的θ<br>现在改变在正规方程中加入一项：</p><script type="math/tex; mode=display">θ=(X^TX+λ\begin{bmatrix}0 & 0 & 0 & ...&0 \\   0 & 1 & 0& ...&0 \\ 0 & 0 & 1& ...&0 \\ ... & ... & ...& ...&... \\ 0 & 0 & 0& ...&1\end{bmatrix})^{-1}X^Ty</script><p>来达到同样的效果  </p><blockquote><p>$\begin{bmatrix}<br>0 &amp; 0 &amp; 0 &amp; …&amp;0 \\   0 &amp; 1 &amp; 0&amp; …&amp;0 \\ 0 &amp; 0 &amp; 1&amp; …&amp;0 \\ … &amp; … &amp; …&amp; …&amp;… \\ 0 &amp; 0 &amp; 0&amp; …&amp;1<br>\end{bmatrix}$是一个(n+1)的方阵  </p></blockquote><p>如果矩阵X不可逆$（m&lt;=n）$,那么$(X^TX)^{-1}$也同样不可逆,但是经过数学证明，无论如何$(X^TX+λ<br>\begin{bmatrix}<br>0 &amp; 0 &amp; 0 &amp; …&amp;0 \\   0 &amp; 1 &amp; 0&amp; …&amp;0 \\ 0 &amp; 0 &amp; 1&amp; …&amp;0 \\ … &amp; … &amp; …&amp; …&amp;…\\ 0 &amp; 0 &amp; 0&amp; …&amp;1<br>\end{bmatrix})^{-1}$ 都是可逆的。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>3. 逻辑回归与正则化</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.2. 分类算法的优化</title>
    <link href="/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.2.%20%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E5%8C%96/"/>
    <url>/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.2.%20%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="分类算法的优化"><a href="#分类算法的优化" class="headerlink" title="分类算法的优化"></a>分类算法的优化</h1><h2 id="其他代价函数优化算法"><a href="#其他代价函数优化算法" class="headerlink" title="其他代价函数优化算法"></a>其他代价函数优化算法</h2><p>对于代价函数，它可以被拆分成求$J(θ)$和求$\frac{∂J(θ)}{∂θ_j }$  两个基础的部分  </p><p>事实上，除了基础的梯度下降算法能够求到最小值之外，还有如下的集中基本方法：</p><ol><li>共轭梯度算法（Conjugate Gradient）</li><li>BFGS</li><li>L-BFGS</li></ol><p>这些算法有一些共同的特点：</p><ol><li>这些算法利用线搜索算法（一种智能内循环）不需要手动选择学习率 α。</li><li>收敛的速度高于梯度下降算法</li><li>复杂度高于梯度下降算法</li></ol><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>对于同一个数据集x，需要分类的目标不只有两种，意味着离散取值y的值不只有0，1两个值。<br>基本思想是<strong>将一个n元分类问题转化为n个二分类问题</strong>。<br>比如对y=1，2，3:<br>可以先将2，3 设置为负类，使用之前的逻辑分类方法就能够将1与（2，3）分开，重复三次，能得到三个逻辑斯蒂函数：$h_θ^i (x)i=1,2,3$。<br>由于$h_θ^i (x)=P(y=i│x; θ)$， 因此$h_θ^i (x)$表示将1设置为正类别，分类器中是1的概率。<br>最后给定函数$max(h_θ^i (x))$， 表示选择出三个当中概率最高的部分，作为判断的结果。</p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>3. 逻辑回归与正则化</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.1. 激活函数</title>
    <link href="/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.1.%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    <url>/2021/02/14/3.%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/3.1.%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 60%;<br>    padding-left: 20%;<br>}</style></p><h1 id="逻辑回归、激活函数及其代价函数"><a href="#逻辑回归、激活函数及其代价函数" class="headerlink" title="逻辑回归、激活函数及其代价函数"></a>逻辑回归、激活函数及其代价函数</h1><h2 id="线性回归的可行性"><a href="#线性回归的可行性" class="headerlink" title="线性回归的可行性"></a>线性回归的可行性</h2><p>对分类算法，其输出结果y只有两种结果{0，1}，分别表示负类和正类，代表没有目标和有目标。<br>在这种情况下，如果用传统的方法以线性拟合${h_θ (x)=θ^T X}$，对于得到的函数应当对y设置阈值a，高于a为一类，低于a为一类。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224223513.png" alt=""><br>对于分类方法，这种拟合的方式极易受到分散的数据集的影响而导致损失函数的变化，以至于对于特定的损失函数，其阈值的设定十分困难。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224223523.png" alt=""><br>除此之外，${h_θ(x)}$（在分类算法中称为分类器）的输出值很可能非常大或者非常小，并不与{0，1}完全相符</p><h2 id="假设表示"><a href="#假设表示" class="headerlink" title="假设表示"></a>假设表示</h2><p>基于上述情况，要使分类器的输出在[0,1]之间，可以采用假设表示的方法。<br>设$h_θ (x)=g(θ^T x)$，其中$g(z)=\frac{1}{(1+e^{−z} )}$, 称为<strong>逻辑斯蒂函数</strong>（Sigmoid function，又称为<strong>激活函数</strong>，生物学上的S型曲线），有：</p><script type="math/tex; mode=display">h_θ (x)=\frac{1}{(1+e^{−θ^T X} )}</script><p>其两条渐近线分别为h(x)=0和h(x)=1</p><p>在分类条件下，最终的输出结果是：</p><script type="math/tex; mode=display">h_θ (x)=P(y=1│x,θ)</script><p>其代表在给定x的条件下 其y=1的概率<br>有:  </p><script type="math/tex; mode=display">P(y=1│x,θ)+P(y=0│x,θ)=1</script><h2 id="决策边界-Decision-boundary"><a href="#决策边界-Decision-boundary" class="headerlink" title="决策边界(Decision boundary)"></a>决策边界(Decision boundary)</h2><p>对假设函数设定阈值$h(x)=0.5$，<br>当$h(x)≥0.5$ 时，输出结果y=1.<br>根据假设函数的性质，当 $x≥$0时，$h(x)≥0.5$<br>由于之前用$θ^T x$替换x，则当$θ^T x≥0$时，$h(x)≥0.5，y=1$<br>解出 $θ^T x≥0$，其答案将会是一个在每一个$x_i$轴上都有的不等式函数。<br>这个不等式函数将整个空间分成了y=1 和 y=0的两个部分，称之为决策边界。  </p><h2 id="激活函数的代价函数"><a href="#激活函数的代价函数" class="headerlink" title="激活函数的代价函数"></a>激活函数的代价函数</h2><p>在线性回归中的代价函数：</p><script type="math/tex; mode=display">J(θ)=\frac{1}{m}∑_{i=1}^m \frac{1}{2} (h_θ (x^{(i)} )−y^{(i)} )^2</script><p>令$Cost（hθ (x)，y）=\frac{1}{2}(h_θ (x^{(i)} )−y^{(i)} )^2$， Cost是一个非凹函数，有许多的局部最小值，不利于使用梯度下降法。对于分类算法，设置其代价函数为：</p><p>对其化简：</p><script type="math/tex; mode=display">Cost（h_θ (x),y）=−ylog(h_θ (x))−((1−y)log⁡(1−h_θ (x)))</script><p>检验：<br>当 $y=1$时，$−log⁡(h_θ (x))$<br>当 $y=0$时，$−log⁡(1−h_θ (x))$  </p><p>那么代价函数可以写成：  </p><script type="math/tex; mode=display">J(θ)=-\frac{1}{m}[∑_{i=1}^m y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))]</script><p>对于代价函数，采用梯度下降算法求θ的最小值：</p><script type="math/tex; mode=display">{θ_j≔θ_j−α\frac{∂J(θ)}{∂θ_j}}</script><p>代入梯度：    </p><script type="math/tex; mode=display">θ_j≔θ_j−α∑_{i=1}^m(h_θ (x^{(i)} )−y^{(i)} ) x_j^i</script>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>3. 逻辑回归与正则化</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2.4. 向量化</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.4.%20%E5%90%91%E9%87%8F%E5%8C%96/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.4.%20%E5%90%91%E9%87%8F%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h1><p>对于求和的算法，有时可以转换为矩阵的乘法来进行计算<br>E.g.  </p><script type="math/tex; mode=display">H(θ)(x)=∑_{j=0}^nθ_j x_j</script><p>如果直接求和，求和的过程会非常冗长<br>而设计两个向量$θ$ $x$<br>则有线性回归假设函数的向量形式：</p><script type="math/tex; mode=display">h(x)=θ^T x</script><p>对更新函数：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20201224232957.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>2. Octave语言初步</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2.3. 控制和定义语句</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.3.%20%E6%8E%A7%E5%88%B6%E5%92%8C%E5%AE%9A%E4%B9%89%E8%AF%AD%E5%8F%A5/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.3.%20%E6%8E%A7%E5%88%B6%E5%92%8C%E5%AE%9A%E4%B9%89%E8%AF%AD%E5%8F%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="控制和定义语句"><a href="#控制和定义语句" class="headerlink" title="控制和定义语句"></a>控制和定义语句</h1><p>for i=1:10,<br>Indices=a : b 从a到b的索引<br>Break Continue 与C语言相同<br>While, end 结构体 同C语言  </p><p>选择结构：<br><figure class="highlight cpp"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> condition,    <br>command   <br>end   <br></code></pre></div></td></tr></table></figure><br>分支选择结构：<br><figure class="highlight cpp"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> condition,   <br>command;   <br>elseif condition,   <br>Command;  <br></code></pre></div></td></tr></table></figure><br>循环结构：<br><figure class="highlight cpp"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs cpp"><span class="hljs-keyword">while</span> condition，  <br>Conmand;  <br> end  <br></code></pre></div></td></tr></table></figure><br>定义函数：<br><figure class="highlight cpp"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs cpp">function y=function(x)  <br>command with Y,x;  <br></code></pre></div></td></tr></table></figure><br>保存为function.m文件  </p><p>返回多个值的函数：<br><figure class="highlight cpp"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs cpp">function [y1,y2]=function(x)  <br>Command with y1,y2,x;  <br></code></pre></div></td></tr></table></figure><br>加载函数：<br>定位到m文件目录下  </p><p><code>addpath（&#39;path&#39;）</code>: 加入Octave路径  </p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>2. Octave语言初步</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2.2. 数据计算和绘制</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.2.%20%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%92%8C%E7%BB%98%E5%88%B6%E5%9B%BE%E5%83%8F/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.2.%20%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%92%8C%E7%BB%98%E5%88%B6%E5%9B%BE%E5%83%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="数据计算和绘制"><a href="#数据计算和绘制" class="headerlink" title="数据计算和绘制"></a>数据计算和绘制</h1><h2 id="对元素的操作"><a href="#对元素的操作" class="headerlink" title="对元素的操作"></a>对元素的操作</h2><p><code>A.\*B</code> A矩阵和B矩阵的每一个元素对应相乘<br><code>.</code> 对每一个元素进行运算操作<br><code>abs(A)</code> 对A每一个元素取绝对值<br><code>v+1</code> 对向量v里面的每一个元素+1<br><code>A’</code> 矩阵A的转置<br><code>pinv(A)</code>对A求逆矩阵，不可逆时即为伪逆矩阵<br><code>max(A)</code> A中最大的元素的值<br><code>max(A,[], DI)</code> A中DI维度下元素最大的值（1 列 2 行）<br><code>ind()</code> 某个元素的位置<br><code>magic(n)</code> 返回n*n的幻方<br><code>find(condition)</code>查找对应条件的元素，并返回一个向量<br><code>sum(A)</code>A所有元素的和<br><code>prod(A)</code>A所有元素的乘积<br><code>ceil(A)</code> 对A向上取整<br><code>floor(A)</code>对A每个元素向下取整<br><code>flipud(A)</code>对A上下翻转  </p><h2 id="绘制图像"><a href="#绘制图像" class="headerlink" title="绘制图像"></a>绘制图像</h2><p><code>Plot(x,y,&#39;r&#39;)</code> 绘制关于x，y的图像 r表示y的函数是红色的（默认为蓝色）<br><code>hold on</code> 保存octave内存中的旧函数图像<br><code>xlabel(&#39;&#39;)</code>添加横轴标签<br><code>ylabel（‘’）</code>添加纵轴标签<br><code>legend(&#39;&#39;,&#39;&#39;)</code> 图例<br><code>title（‘’）</code>添加标题<br><code>print -dpng &#39;xx.png&#39;</code> 在当前路径下以png保存当前图像<br><code>close</code> 关闭当前图像<br><code>figure(1)</code>; 标记图像（多开图像窗口）<br><code>subplot(1,2,1)</code> 把图像分成1x2的网格 从第一个格图开始画图<br><code>axis([0.5 1 -1 1])</code>横轴0.5~1 纵轴-1~1<br><code>clf</code> 清除一幅图像<br><code>imagesc(A)</code>可视化矩阵<br><code>colorbar</code> 添加颜色条<br><code>colormap gray</code>  生成黑白图像<br><code>,</code>依次执行每一个命令  </p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>2. Octave语言初步</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2.1. 基本命令</title>
    <link href="/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.1.%20%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"/>
    <url>/2021/02/14/2.%20Octave%E8%AF%AD%E8%A8%80%E5%88%9D%E6%AD%A5/2.1.%20%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</url>
    
    <content type="html"><![CDATA[<h1 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h1><h2 id="基本运算"><a href="#基本运算" class="headerlink" title="基本运算"></a>基本运算</h2><p>代数运算：+ - * / sqrt（）<br>布尔运算：且：&amp;&amp;  或：||  非：！<br>赋值：=  </p><h2 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h2><p><code>disp()</code> 显示（）内的命令到屏幕<br><code>sprintf()</code>用法同c语言中的printf<br><code>format lone</code> 显示变量的更多小数位数<br><code>formate short</code> 显示变量的更少小数位数（4位）<br><code>help fuction</code> 显示function 函数的帮助文档  </p><h2 id="矩阵的快速操作"><a href="#矩阵的快速操作" class="headerlink" title="矩阵的快速操作"></a>矩阵的快速操作</h2><p><code>[a b; c d;]</code> 2x2矩阵<br>[]矩阵符号<br>；换行<br>快速建立步长相等的行向量：  起始参数：步长（默认为1）：终止参数<br><code>ones(a,b)</code> 快速生成axb的矩阵，且所有元素为1<br><code>zeros(a,b)</code>快速生成axb的矩阵，且所有元素为0<br><code>rand(a,b)</code>快速生成axb的矩阵，且所有元素的值为在（0，1）内的随机数<br><code>randn(a,b)</code>快速生成axb的矩阵，且所有元素的值为服从正态分布的随机数<br><code>hist()</code> 快速绘制变量的直方图<br><code>eye(a)</code> 快速生成axa的单位矩阵<br><code>size(row,column)</code>返回矩阵的大小，并将大小存入一个1x2的矩阵中<br><code>length(A)</code>返回向量A的最大维度的值<br><code>rref(A)</code> 求解矩阵A的阶梯型</p><h2 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h2><p><code>pwd</code> 返回Octave当前指向的路径<br><code>cd &#39;path&#39;</code> 使Octave指向path路径<br><code>ls</code> 返回Octave当前指向的路径下所有的文件名称  </p><p><code>Load (&#39;file.dat&#39;)</code>  加载file.dat文件<br>*file.dat 是一个编写好的仅有数据（用固定格式aaa bbb ccc）的文件<br><code>Who</code> 返回Octave当前内存中所有的变量<br><code>Whos</code> 返回Octave当前内存中所有的变量和对应的维度、数据类型、数据大小<br><code>Clear varible</code> 清除varible变量<br><code>Clear</code> 清除内存中所有的变量<br><code>Varible1=varible2(a:b)</code>将varible2中的a到b位数据赋给varible1<br><code>Save file.mat varible</code> 将varible存入file.mat中<br><code>Save file.txt varible ascii</code> 将varible存入file.txt中 编码为ascii  </p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p><code>varible(a,b)</code>定位到varible中的（a，b）变量<br><code>C=[A B]</code> 生成[A B]矩阵（B在A右边）<br><code>C=[A;B]</code> 生成[A B]矩阵（B在A下边）  </p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>2. Octave语言初步</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.5. 多项式拟合和正规方程</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.5.%20%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.5.%20%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="多项式拟合和正规方程"><a href="#多项式拟合和正规方程" class="headerlink" title="多项式拟合和正规方程"></a>多项式拟合和正规方程</h1><h2 id="特征点的创建和合并"><a href="#特征点的创建和合并" class="headerlink" title="特征点的创建和合并"></a>特征点的创建和合并</h2><p>对于一个特定的问题，可以产生不同的特征点，<strong>通过对问题参数的重新定义和对原有特征点的数学处理合并拆分，能够得到更加优秀的特征点。</strong></p><h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>对于更多更加常见的数学模型，其拟合往往是非线性关系的，这时候就需要考虑引用多项式来进行拟合，如：$h(x)=θ_0+θ_1 x+θ_2 x^2+θ_3 x^3$  </p><h2 id="正规方程算法"><a href="#正规方程算法" class="headerlink" title="正规方程算法"></a>正规方程算法</h2><p>在微积分中，对于函数$f(x,y)$，其局部最值往往是在$f_x=0$ 且$f_y=0$处取得。<br>因此，对于代价函数$J(θ)$，求$J(θ)$对每一个$θ_i$的偏导数，令它们都为0，即：</p><script type="math/tex; mode=display">\frac{∂J(θ)}{∂θ_i}=0~for~i=0,1,2,…,n</script><p>称为<strong>正规方程</strong>(Regular expression)。正规方程提供了一种直接求出最小值的方法，而不需要依赖迭代进行一步一步地运算。 </p><h3 id="正规方程的矩阵形式"><a href="#正规方程的矩阵形式" class="headerlink" title="正规方程的矩阵形式"></a>正规方程的矩阵形式</h3><p>对于数据集$\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})\}$,  其中每一个$x^{(i)}$都是一个向量：$x^{(i)}=\left[\begin{smallmatrix}x_0^{(i)} \\\ x_1^{(i)} \\\ …\\\ x_n^{(i)}\end{smallmatrix}\right]$ 构建<strong>设计矩阵</strong>（Design matrix）$X=\left[\begin{smallmatrix}<br>(x^{(1)})^T \\\ (x^{(2)})^T \\\ … \\\ (x^{(m)})^T<br>\end{smallmatrix}\right]$  和值向量 $y=\left[\begin{smallmatrix}  y^{(1)} \\\ y^{(2)} \\\ … \\\ y^{(m)}  \end{smallmatrix}\right]$<br>将代价函数转化为矩阵方程的形式，再对其求导，令其等于0，得到代价函数取得最小值时的$θ$：</p><script type="math/tex; mode=display">θ=(X^TX)^{-1}X^Ty</script><p>对比梯度下降算法：<br>正规方程算法不需要学习率和迭代，但<strong>对大规模数量（万数量级以上）的特征点（n），工作效率十分低下</strong>。对于一些如分类算法等等更加复杂的算法，正规方程法并不适用于求它们在极值处的θ值。  </p><h3 id="正规方程的不可逆性"><a href="#正规方程的不可逆性" class="headerlink" title="正规方程的不可逆性"></a>正规方程的不可逆性</h3><p>在使用正规方程时，要注意的问题是，<strong>如果设计矩阵X不可逆（为奇异矩阵），正规方程会无法使用。</strong>  </p><p>设计矩阵为奇异矩阵的常见情况：</p><ol><li>x-I 不满足线性关系  </li><li>正在运行的学习算法中，特征点的数量大于样本点的数量（使得$m≤n$）  </li></ol><p>当设计矩阵X不可逆时，应当尝试删除一些特征点，或者考虑正规化（Regularation）。<br>但是总体而言，<strong>矩阵X不可逆的情况是极少数的。</strong></p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.4. 调试方法</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.4.%20%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.4.%20%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="调试方法"><a href="#调试方法" class="headerlink" title="调试方法"></a>调试方法</h1><h2 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h2><p>对于某些不具有比较性的样本特征$x_i$ （比如对其他的x来说$x_i$ 相当大或者相当小），梯度下降的过程可能会非常漫长，并且可能来回波动才能最后收敛到全局的最小值。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144432.png" alt=""><br>在这样的情况下，可以对$x_i$ 进行缩放（如 $x_i≔αx_i$  或者 $x_i=x_i/α$），使得$x_i$ 与其他的$x$具有可比性，以增加梯度下降的效率。<br><strong>通常将$x$缩放至⟦-1,1⟧</strong>的区间内。（只表示一个大致的范围，这不是绝对的。）</p><h2 id="均值归一"><a href="#均值归一" class="headerlink" title="均值归一"></a>均值归一</h2><p>将$x_i$  替换为$x_i−μ_i$ 使得特征值具有为0的平均值（对$x_0$ 不适用）</p><script type="math/tex; mode=display">x_i:=(x_i−μ_i)/s_i</script><p>定义$μ_i$  为训练集$X$ 的平均值，$s_i=|x_imax−x_imin |$, 表示$x_i$ 的取值范围（近似值），或者直接设置为$s_i$ 的标准差。</p><h2 id="学习率-Learning-rate"><a href="#学习率-Learning-rate" class="headerlink" title="学习率(Learning rate)"></a>学习率(Learning rate)</h2><p>梯度下降调试的方法：</p><ol><li><p>绘制$minJ(θ)-batch$的图像<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144740.png" alt=""><br>原则：每一个batch之后 θ 的值都应该减小，这样的图像能够通过直观地表现变化率来表现梯度下降是否收敛（变化率为0）。  </p></li><li><p>自动收敛测试<br>如果$J(θ)$在某一次迭代之后的下降值小于某个值$ε$后，就能够判断算法已经达到了收敛。<br>$ε$的值比较难取，所以通常采取1.中的方法进行观测。</p></li></ol><p>常见的α过大的$minJ(θ)-batch$的图像：<br>α过大,导致代价函数无法收敛<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131144816.png" alt="">   </p><p>α过小，导致代价函数收敛速度过慢</p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.3. 多变量预测</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.3.%20%E5%A4%9A%E5%8F%98%E9%87%8F%E9%A2%84%E6%B5%8B/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.3.%20%E5%A4%9A%E5%8F%98%E9%87%8F%E9%A2%84%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 30%;<br>    padding-left: 20%;<br>}</style></p><h1 id="多变量预测"><a href="#多变量预测" class="headerlink" title="多变量预测"></a>多变量预测</h1><h2 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h2><p>对于多个特征量(Features)，规定符号表示：<br>$n$ 特征的总数量<br>$x^{(i)}$  第i个训练样本的输入特征向量，$i$表示的是一个索引(Index)<br>$x_j^i$  第i个训练样本中特征向量的第j个值  </p><p>此时的假设函数不再是单纯的 $h_θ (x)=θ_0+θ_1 x$ 的形式。<br>对于多个特征量，此时的假设函数为：   </p><script type="math/tex; mode=display">h_θ (x)=θ^T x=θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)}</script><p>对这个样本进行简化：<br>定义$x_0^i=1$, 定义参数向量：$x=\left[\begin{smallmatrix} x_0 \\\ x_1 \\\ … \\\ x_n \end{smallmatrix}\right]n$，系数向量：$θ=\left[\begin{smallmatrix}θ_0 \\\ θ_1 \\\ … \\\ θ_n \end{smallmatrix}\right]$<br>有：   </p><script type="math/tex; mode=display">h_θ (x)=θ^T x</script><p>这就是假设函数的向量形式。   </p><h2 id="梯度下降算法在多元线性回归中的应用"><a href="#梯度下降算法在多元线性回归中的应用" class="headerlink" title="梯度下降算法在多元线性回归中的应用"></a>梯度下降算法在多元线性回归中的应用</h2><p>对于假设函数：</p><script type="math/tex; mode=display">\begin{aligned}h_θ(x)  =  θ^T x \\\  & =θ_0+θ_1 x^{(1)}+θ_2 x^{(2)}+…+θ_n x^{(n)} \\\       \end{aligned}</script><p>和损失函数：   </p><script type="math/tex; mode=display">J(θ_0,θ_1,…,θ_n)=\frac{1}{2m} ∑_{i=1}^m(h_θ (x^{(i)} )−y^{(i)} )^2</script><p>此时的梯度下降算法：<br>Repeat\{</p><script type="math/tex; mode=display">θ_j≔θ_j−α\frac{∂J(θ)}{∂θ_j}</script><p>\}<br>对$\frac{∂J(θ)}{∂θ_j}$进行等价变形：<br>Repeat\{</p><script type="math/tex; mode=display">θ_j≔θ_j−α\frac{1}{m}∑_{i=1}^m(h_θ (x^{(i)} )−y^{(i)})  x_j^i</script><p>\}</p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.2. 梯度下降算法</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.2.%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.2.%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p><style><br>img{<br>    width: 40%;<br>    padding-left: 20%;<br>}</style></p><h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><p>在开始之前，为了方便解释，首先规定几个符号所代表的意义：<br>$m$ 训练集中训练样本的数量<br>$X$  输入变量<br>$Y$  输出变量<br>$(x,y)$ 训练样本<br>$(x^i,y^i)$第i个训练样本（i表示一个索引）  </p><h2 id="监督学习算法的流程"><a href="#监督学习算法的流程" class="headerlink" title="监督学习算法的流程"></a>监督学习算法的流程</h2><p>提供训练集&gt;学习算法得到$h$（假设函数：用于描绘x与y的关系）&gt;预测y 的值  </p><h2 id="代价-损失函数（Cost-function）"><a href="#代价-损失函数（Cost-function）" class="headerlink" title="代价/损失函数（Cost function）"></a>代价/损失函数（Cost function）</h2><p><strong>假设函数(Hypothesis function)</strong>——$h$是用来表示某一个数据集可能存在的线性/非线性关系的函数。对于线性拟合，其假设函数为：  </p><script type="math/tex; mode=display">h_θ(x)=θ_1x+θ_0</script><p>这其中的$θ$是假设函数当中的参数。<br>也可以简化为：</p><script type="math/tex; mode=display">h_θ(x)=θ_1x</script><p><strong>代价函数</strong>，在统计学上称为均方根误差函数。当假设函数中的系数$θ$取不同的值时，$\frac{1}{2m}$倍假设函数预测值$h_θ(x^{(i)})$和真实值$y^{(i)}$的差的平方的和之间的函数关系表示为代价函数$J$。</p><script type="math/tex; mode=display">\begin{aligned}J(θ_0,θ_1)= \\\  & \frac{1}{2m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})^2 \\\      \end{aligned}</script><blockquote><p>在这里取1/2的原因是便于消除求导之后产生的2倍,同时也可以进一步缩小$θ$  </p><p>该函数的自变量是$θ_1$和$θ_0$，因此该函数是三维的函数（如图所示）。<br> <img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131130651.png" alt="">  </p></blockquote><p><strong>代价函数在几何上表示为数据集空间内的各点到假设函数的距离的平方的平均值的一半。</strong><br>要想使得数据能够被假设函数很好地拟合，那么代价函数要尽量地小。<strong>当代价函数取到它的最小值即</strong>$J(θ_1)_{min}$<strong>时，此时的填入假设函数的</strong>$θ$<strong>对数据的拟合程度是最好的</strong><br>对于线性的代价函数，假设函数对数据集的拟合程度越高，对应的$(θ_0,θ_1)$越接近代价函数图像等高线的中心。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131132852.png" alt="">   </p><h2 id="梯度下降算法（Gradient-Descent）"><a href="#梯度下降算法（Gradient-Descent）" class="headerlink" title="梯度下降算法（Gradient Descent）"></a>梯度下降算法（Gradient Descent）</h2><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>在微积分中，函数$f(x,y)$在$(x_0,y_0)$处是函数值增加最快的方向是<strong>梯度（Gradient）</strong>的方向，<strong>梯度的反方向是函数值减小最快的方向。</strong><br>梯度的计算公式：  </p><script type="math/tex; mode=display">▿f|_{(x_0,y_0)}=(f_x(x_0,y_0),f_y(x_0,y_0))</script><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>梯度下降算法是一种求解代价函数最小值的方法，它可以用在<strong>多维任意的假设函数</strong>当中。<br>简而言之，梯度下降算法求得$J(θ_1)_{min}$的主要思路是：   </p><ol><li>给定$θ_0$和$θ_1$的初始值，通常令$θ_0=0$，$θ_1=0$。</li><li>不断改变$θ_0$和$θ_1$的值使得$J(θ_0,θ_1)$的值逐渐变小，直到找到$J(θ_0,θ_1)$的最小值或者局部最小值。<br>如果从一个初始值出发，寻找附近的最小值，重复该过程，得到上图，最后得到的值为局部最优解。  <blockquote><p>将梯度下降算法类比为爬山，从一个点开始，不断寻找“下山”的路线，最后找到一个“下山”的出口。——吴恩达  </p></blockquote></li></ol><p>当改变初始值时，会找到另一条“下山”的路径，找到第二个局部最优解（局部最小值）。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131134419.png" alt=""><br>对于线性回归的代价函数而言，只存在一个局部最小值。（见代价函数的图像）  </p><h3 id="表示"><a href="#表示" class="headerlink" title="表示"></a>表示</h3><p>梯度下降算法可以表示为：<br>Repeat untill convergence\{  </p><script type="math/tex; mode=display">θ_j:=θ_j-α\frac{∂J(θ_0,θ_1)}{∂θ_j},j=0~and~j=1</script><p>\}<br>解释：    </p><ol><li>:=  表示赋值运算符</li><li>α称为<strong>学习率</strong>，用来控制下降的<strong>步长</strong>（Padding），即更新的幅度：  <ul><li>α太小，同步更新的速率会非常的慢     </li><li>α过大，同步更新时可能会越过最小值点   </li></ul></li><li>$\frac{∂J(θ_0,θ_1)}{∂θ_j}$是代价函数的梯度：<script type="math/tex; mode=display">\frac{∂J(θ_0,θ_1)}{∂θ_0}=\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})</script><script type="math/tex; mode=display">\frac{∂J(θ_0,θ_1)}{∂θ_1}=\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}</script><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131135144.png" alt=""><br>△在代价函数中（以简化的代价函数为例），无论初始值在最小值点的左侧还是右侧，通过同步更新都能够使该点被“移动（Update）”到最小值，在最小值点，由于导数值为0，最终同步更新停止在了$θ_j=θ_j$，如前面所说，$θ_j$即为极小值点。  </li></ol><h3 id="同步更新"><a href="#同步更新" class="headerlink" title="同步更新"></a>同步更新</h3><p><strong>同步更新</strong>（Simulaneous update）是实现梯度下降算法的最有效方式。  </p><script type="math/tex; mode=display">temp0:θ_0:=θ_0-α\frac{∂J(θ_0,θ_1)}{∂θ_0}</script><script type="math/tex; mode=display">temp1:θ_1:=θ_1-α\frac{∂J(θ_0,θ_1)}{∂θ_1}</script><script type="math/tex; mode=display">θ_0:=temp0</script><script type="math/tex; mode=display">θ_1:=temp1</script><p>这个更新方程能够同时更新$θ_0$和$θ_1$：$θ_0$更新为$θ_0-J’(θ)$，对$θ_1$同理。<br>更新的方法是计算赋值号右边带入$θ_1$和$θ_2$的值进行计算，得到的两个值分别储存在temp0和temp1中，从上到下进行赋值。  </p><p>对于简化的代价函数：  </p><script type="math/tex; mode=display">θ_1：=θ_1-αJ'(θ_1)</script><script type="math/tex; mode=display">\frac{dJ(θ_1,θ_0)}{dθ_j} =d(\frac{1}{2m}Σ(h_θ(x_i)-y_i))^2)</script><p>将梯度代回代价函数中就得到了<strong>Batch梯度下降法</strong>的基本形式：<br>Repeat untill convergence\{  </p><script type="math/tex; mode=display">θ_0:=θ_0-α\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})</script><script type="math/tex; mode=display">θ_1:=θ_1-α\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x^{(i)}</script><p>\}    </p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1.1. 什么是机器学习</title>
    <link href="/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.1.%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <url>/2021/02/14/1.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1.1.%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h1><h2 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><blockquote><p>A computer  program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.    ——Tom Mitchelle  </p></blockquote><p>简言之，机器学习通过完成任务（T）得到经验（E），进而提升性能（P）。<br>例如：一个自我对弈的跳棋学习机器：E ：自我对弈的棋局 T：下跳棋 P：与新对手玩跳棋时的获胜概率  </p><h2 id="机器学习的主要算法类型"><a href="#机器学习的主要算法类型" class="headerlink" title="机器学习的主要算法类型"></a>机器学习的主要算法类型</h2><ul><li><strong>监督学习</strong>（Supervised）<br>人教会计算机完成任务。<br>根据统计数据做直线或曲线拟合/分离数据，来预测结果。<br>其中包括了两大问题：  <ul><li><strong>回归</strong>（Regression）<br>给算法做一个数据集，包含正确答案，（比如房价-年），用线性/非线性回归方程拟合数据,预测数据。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124639.png" alt=""></li><li><strong>分类问题</strong>（<strong>逻辑回归</strong>问题）（Classification/Logical regression）<br>用实数对出现的可能状况分类<br>（比如：1和0表示患乳腺癌/不患乳腺癌 ；1表示患乳腺癌A，2表示患乳腺癌B，0表示不患乳腺癌），在多维坐标系中（每一个维度表示不同的属性），然后用线性或非线性的函数将不同类的数据分开。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124726.png" alt=""></li></ul></li><li><strong>无监督学习</strong>（Unsupervised）<br>计算机自己学习，经典的算法分为两大类：    <ul><li><strong>聚类算法</strong><br>对并不明确分类的数据集，计算机根据数据特征自动将数据分为几个簇<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210131124803.png" alt=""></li><li><strong>鸡尾酒会算法</strong>（Cocktail party）<br>略，这里只对鸡尾酒会问题和解决方法作一个概述：<br>鸡尾酒会问题是在计算机语音识别 领域的一个问题。<br>当前语音识别技术已经可以以较高精度识别一个人所讲的话，但是当说话的人数为两人或者多人时，语音识别率就会极大的降低，这一难题被称为鸡尾酒会问题。<br>对于的给定混合信号，分离出鸡尾酒会中 同时说话的每个人的独立信号。<br>鸡尾酒问题的解决方法是把两个收音器分别放在两个人的附近，每个收音器且与两个人的距离是不等距的，如此来分离两个人的声音。</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>1. 线性回归</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>5.2. 神经网络的优化</title>
    <link href="/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/"/>
    <url>/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.2.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="算法优化"><a href="#算法优化" class="headerlink" title="算法优化"></a>算法优化</h1><h2 id="参数展开"><a href="#参数展开" class="headerlink" title="参数展开"></a>参数展开</h2><p>参数展开是一种将矩阵展开为向量的方法，常用于很多高级优化中。<br>例如如下的高级优化：<br><figure class="highlight cpp"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs cpp">function[jVal,gradient]=costFunction(theta)<br>...<br>optTheta=fminunc(@costFunction,initialTheta,options)<br></code></pre></div></td></tr></table></figure><br>fminuc是一种高级的优化算法。这些高级优化算法的输入值的形式都是参数向量。<br>在神经网络中，很多参数并非是向量的形式，而是完整的矩阵，比如第l层的参数矩阵$Θ^{(l)}$和梯度矩阵$D^{(l)}$(见5.1.)，这时就需要应用参数展开将这些矩阵展开为向量，方法是在Octave中应用<code>[;]</code>表达将所有的元素从矩阵中取出，展开成一个长向量，并应用<code>reshape</code>语法重新合成矩阵。<br>比如如下的10层神经网络：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210204151039.png" alt="">   </p><h2 id="梯度检测"><a href="#梯度检测" class="headerlink" title="梯度检测"></a>梯度检测</h2><p>反向传播算法的实现过程非常的繁琐，因此在与其他算法一同工作的时候可能会产生一些bug，这些bug可能本身不会影响程序的运行，但是最终输出的模型准确度可能会非常低。 因此需要引入梯度检测(Gredient Check)来解决反向传播算法或类梯度下降算法中出现的这类问题。  </p><ul><li>从数值上近似梯度<br>要想求出代价函数$J(\Theta)$在某一点$\theta$的梯度（在二维内反映为该点的斜率），可以在$\theta$的两边取$\theta \plusmn \epsilon, \epsilon \rightarrow 0$,$\theta$处的梯度可以近似的表示为（实数形式）：<script type="math/tex; mode=display">\frac{dJ(θ )}{dθ }≈  \frac{J(θ +ε )-J(θ -ε )}{2ε }</script>称为双侧差分（Two-side difference）。<br>当$\theta$是$\Theta^{(i)}$的展开时，可以用双侧差分来估计所有的偏导数项：<script type="math/tex; mode=display">\frac{∂ J(θ)}{∂ θ_k}≈ \frac{J(θ _k+ε ,θ_1,...,θ_n )-J(θ _k-ε ,\theta_1,...,θ_n  )}{2ε }</script>在Octave中用如下的代码实现：<figure class="highlight cpp"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Cpp"><span class="hljs-keyword">for</span> i=<span class="hljs-number">1</span>:n,<br>  thetaPlus=theta;<br>  thetaPlus=thetaPlus(i)+epsilon;<br>  thetaMinus=theta;<br>  thetaMinus(i)=thetaMinus(i)-epsilon;<br>  gradApprox(i)=(J(thetaPlus)-J(thetaMinus))/(<span class="hljs-number">2</span>*epsilon);<br>end;<br>Check gradApprox≈DVec<br></code></pre></div></td></tr></table></figure></li><li>总结流程<ol><li>利用反向传播算法算出$D^{(i)}$的展开向量DVec</li><li>利用双侧差分计算gradApprox</li><li>DVec和gradApprox作比较    </li><li>关闭双侧差分，利用反向传播进行训练（以提高训练时的算法效率）</li></ol></li></ul><h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>在最开始执行高级优化或者是神经网络的梯度下降时，应当对$Θ$设置一些初始值，即初始化$Θ$。<br>在逻辑回归中，将参数全部初始化为0的做法会导致神经网络中一个单元出发的所有的参数都相等，导致神经网络中所有的隐藏单元都在计算相同的特征。正确的做法是对$Θ$随机地设定一些值来初始化它，具体的做法是：<br>设置某个区间$[-ϵ,ϵ]$，使得所有$θ$都在这个区间内随机取到，用Octave代码实现：<br><figure class="highlight cpp"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs cpp">theta1 = rand(<span class="hljs-number">10</span>,<span class="hljs-number">11</span>)*(<span class="hljs-number">2</span>*init_epsilon)-init_epsilon; <br><span class="hljs-meta">#rand()的作用是随机生成一个mxn的矩阵，矩阵里面所有的元素值都介于0,1之间。</span><br></code></pre></div></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>5. 神经网络拟合</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>5.1 神经网络的代价函数</title>
    <link href="/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0/"/>
    <url>/2021/02/14/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8B%9F%E5%90%88/5.1.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络的代价函数"><a href="#神经网络的代价函数" class="headerlink" title="神经网络的代价函数"></a>神经网络的代价函数</h1><h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p>接下来的讲义主要考虑两种分类问题：第一种是二元分类，如之前的讲义所述，y的取值只能是0或者1，输出层只有一个输出单元，假设函数的输出值是一个实数；第二种是多元分类，y的取值是一个k维的向量，输出层有k个输出单元。</p><h2 id="神经网络的代价函数形式"><a href="#神经网络的代价函数形式" class="headerlink" title="神经网络的代价函数形式"></a>神经网络的代价函数形式</h2><p>假设一个神经网络训练集有m个训练样本：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$<br>$L$表示神经网络的总层数，$s_l$表示$l$层中神经元的数量（不包括偏置神经元）。<br>在神经网络中使用的代价函数是在逻辑回归中使用的正则化代价函数：  </p><script type="math/tex; mode=display">J(θ)=-\frac{1}{m}[∑_{i=1}^m y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))]+\frac{λ}{2m}∑_{j=1}^n θ_j^2</script><p>略微不同的是，在神经网络中分类标签和假设函数的输出值都变成了k维的向量，因此神经网络中的代价函数变成了：  </p><script type="math/tex; mode=display">J(θ)=-\frac{1}{m}[∑_{i=1}^m ∑_{k=1}^Ky_k^{(i)} log⁡(h_θ(x^{(i)} )_k)+(1−y_k^{(i)}) log(1−h_θ (x^{(i)})_k)]+\frac{λ}{2m}∑_{l=1}^{L-1}∑_{j=1}^{s_l}∑_{j=1}^{s_l+1} (Θ_{ji}^{(l)})_j^2</script><p>解释：  </p><ol><li>用$(h_Θ(x))_i$来表示第i个输出  </li><li>这个代价函数中$∑_{k=1}^K$表示所有的输出单元之和，这里主要是将$y_k$的值与$(h_Θ(x))_k$的大小作比较   </li><li>正则项的作用是去除那些对应于偏置单元的项，具体而言就是不对$i=0$的项进行求和和正则化。  </li></ol><h2 id="代价函数最小化：反向传播算法"><a href="#代价函数最小化：反向传播算法" class="headerlink" title="代价函数最小化：反向传播算法"></a>代价函数最小化：反向传播算法</h2><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>同之前的线性回归和逻辑回归一样，接下来要求得代价函数的最小值$J(Θ)min$并求出$Θ$。主要的步骤是写出$J(Θ)$并求关于每一个$Θ_{ij}^{(l)}$的偏导项$\frac{∂}{∂Θ_{ij}^{(l)}}J(Θ)$。<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210201185117.png" alt=""><br>现在先来讨论如上图所示的神经网络中，只有一个训练样本$(x,y)$的情况：<br>首先先用前向传播算法（见讲义4.2）验证假设函数是否会真的输出结果:    </p><script type="math/tex; mode=display">a^{(1)}=x</script><script type="math/tex; mode=display">z^{(2)}=Θ^{(1)}a^{(1)},并增加一个偏置单元</script><script type="math/tex; mode=display">a^{(2)}=g(z^{2})</script><script type="math/tex; mode=display">z^{(3)}=Θ^{(2)}a^{(2)},并增加一个偏置单元</script><script type="math/tex; mode=display">a^{(3)}=g(z^{3})</script><script type="math/tex; mode=display">z^{(4)}=Θ^{(3)}a^{(3)}</script><script type="math/tex; mode=display">a^{(4)}=g(z^{4})=h_Θ(x)</script><p>接下来，为了计算关于每一个$Θ_{ij}^{(l)}$的偏导项$\frac{∂}{∂Θ_{ij}^{(l)}}J(Θ)$，就要用到<strong>反向传播算法</strong>（Backpropagation）。<br>从直观上说，对于每一个节点，都要计算每个节点的误差：$δ^{(l)}_j$,表示第l层第j个节点的误差。  </p><script type="math/tex; mode=display">δ^{(l)}_j=a_j^{(l)}-y_j=(h_Θ(x))_j-y_j</script><p>y表示训练集中y向量里的第j个元素的值。<br>其向量形式：  </p><script type="math/tex; mode=display">δ^{(l)}=a^{(l)}-y</script><p>这里的$δ^{(l)}$和$a^{(l)}$都是一层每一个误差/输出所构成的向量。<br>具体而言，对于上图所示的4层（$L=4$）神经网络,第四层的误差项：  </p><script type="math/tex; mode=display">δ^{(4)}=a^{(4)}-y</script><p>照例写出前面两层的误差：  </p><script type="math/tex; mode=display">\delta^{(3)}=(Θ^{(3)})^Tδ^{(4)}⋅g'(z^{(3)})</script><script type="math/tex; mode=display">\delta^{(2)}=(Θ^{(2)})^Tδ^{(3)}⋅g'(z^{(2)})</script><p>事实上应用微积分的链式法则，$g’(z^{(3)})=a^{(3)}⋅(1-a^{(3)})$,1是一个每项都为1的向量。<br>反向传播的步骤相当于是从最后一层开始求误差，然后将最后一层的误差传给前一层，反向依次传播。<br>最终将会有： </p><script type="math/tex; mode=display">\frac{∂}{∂Θ_{ij}^{(l)}}J(Θ)=a^{(l)}_iδ^{(l+1)}_i</script><p>此处忽略了正则化项：$λ$。<br>现在将反向传播算法从一个训练样本拓展到一个有m个训练样本：${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$,$L$层的神经网络训练集：      </p><p>定义$Δ_{ij}^{(l)}=0$用于计算$\frac{∂}{∂Θ_{ij}^{(l)}}J(Θ)$,接下来遍历整个训练集：<br>For $i=1$ to $m$:<br>  set $a^{(1)}=x^{(i)}$ #用于将所有的x输入到输入层的激活函数中<br>  用正向传播算法计算$a^{(l)}~for~l=2,3,…,L$<br>  $δ^{(L)}=a^{(L)}-y^{i}$ #计算最后一层的误差<br>  用反向传播算法计算$\delta^{(L-1)}$到$δ^{(2)}$,<br>  $Δ^{(l)}_{ij}:=Δ^{(l)}_{ij}+a_j^{(l)}δ^{(l+1)}_i$<br>  (写成向量的形式：$Δ^{(l)}:=Δ^{(l)}+δ^{(l+1)}(a_j^{(l)})^T$)<br>结束循环后，令  </p><script type="math/tex; mode=display">D^{(l)}_{ij}:=\begin{cases}    \frac{1}{m}Δ^{(l)}_{ij},j=0       \frac{1}{m}Δ^{(l)}_{ij}+λΘ^{(l)}_{ij},j \not=0 \end{cases}</script><p>那么最终：  </p><script type="math/tex; mode=display">\frac{∂}{∂Θ_{ij}^{(l)}}J(Θ)=D^{(l)}_{ij}</script><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><ul><li>回顾：前向传播模型<br>前向传播的整个过程可以用下图表示：<br><img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210204143238.png" alt=""><br>比如：如果洋红色的部分其权重为$Θ_{10}^{(2)}$,红色的权重值为$Θ_{11}^{(2)}$，青色的权重值是$Θ_{12}^{(2)}$， 那么$z_1^{(3)}=Θ_{10}^{(2)} \times 1+Θ_{11}^{(2)} × a_1^{(2)}+Θ_{12}^{(2)}×a_1^{(2)}$。<br>反向传播的过程和前向传播非常类似，只是传播的方向不同。  </li><li>反向传播的理解<br>关注反向传播的代价函数：   <script type="math/tex; mode=display">J(θ)=-\frac{1}{m}[∑_{i=1}^m ∑_{k=1}^Ky_k^{(i)} log⁡(h_θ(x^{(i)} )_k)+(1−y_k^{(i)}) log(1−h_θ (x^{(i)})_k)]+\frac{λ}{2m}∑_{l=1}^{L-1}∑_{j=1}^{s_l}∑_{j=1}^{s_l+1} (Θ_{ji}^{(l)})_j^2</script>对于单个的样本:$(x^{(i)},y^{(i)})$，只有一个输出单元并且忽略正则化，那么这个样本的代价函数：  <script type="math/tex; mode=display">Cost(i)=y^{(i)} log⁡(h_θ(x^{(i)} ))+(1−y^{(i)}) log(1−h_θ (x^{(i)}))</script>这个代价函数的功能类似于计算方差，可以近似的看做是方差函数：  <script type="math/tex; mode=display">Cost(i)≈(h_\Theta(x^{(i)})-y^{(i)})^2</script>它反应了样本模型输出值和样本值的接近程度。<br>反向传播中每个节点的误差：$δ^{(l)}_j$,表示第l层第j个节点的误差。有：   <script type="math/tex; mode=display">δ^{(l)}_j=\frac{\partial}{∂z_j^{(l)}}Cost(i)</script>$z_j^{(l)}$与$h_\Theta(x^{(i)})$相关。  </li></ul><p>  反向传播的整个过程可以用下图表示：<br>  <img src="https://raw.githubusercontent.com/l61012345/Pic/master/img/20210204145244.png" alt=""><br>  例如对$δ^{(2)}_2$，洋红色和红色箭头分别表示两个权重值$Θ_{12}^{(2)}$和$Θ_{22}^{(2)}$，有  </p><script type="math/tex; mode=display">δ^{(2)}_2=Θ_{12}^{(2)} ×δ^{(3)}_1 +Θ_{22}^{(2)} ×δ^{(3)}_2</script>]]></content>
    
    
    <categories>
      
      <category>机器学习基础课程——吴恩达</category>
      
      <category>5. 神经网络拟合</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
