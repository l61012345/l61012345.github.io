{"title":"4.3. 线性回归的正则化","id":"2021/08/21/机器学习——吴恩达/4. 正则化/4.3. 线性回归的正则化/","date_published":"08/21/2021","summary":"线性回归的正则化正则化的梯度下降算法在线性回归中，我们使用修改后的梯度下降算法：Repeat {   \nθ_0:=θ_0-\\alpha\\frac{1}{m}\\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(j)} \\tag{1}\n$θ_0$  不需要正则化  \n\nθ_j:=θ_j-\\alpha[\\frac{1}{m}\\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\\frac{λ}{m}θ_j] \\tag{2}j=1,2,3,...,n}事实上： $\\frac{1}{m}\\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(j)}+\\frac{λ}{m}θ_j=\\frac{∂J(θ)}{∂θ_j}$$\\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}=\\frac{∂J(θ)}{∂θ_0}$  \n如果将（2）中的$\\theta_j$统一，那么就可以得到（3）：  \nθ_j:=θ_j（1-α\\frac{λ}{m}）-\\frac{α}{m}\\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)} \\tag{3}由于$1-α\\frac{λ}{m}&lt;1$,且只比1小一点点，也就是说，梯度下降算法每次更新的时候$θ_j$在一开始都会比原来小一点点，再进行原来的梯度下降更新  \n正规方程在之前的讲义中，探讨过设计两个矩阵：$X=\\begin{bmatrix} (x^{(1)})^T \\\\ …\\\\ (x^{(m)})^T \\end{bmatrix}$ 代表有m个数据的数据集 和 $y=\\begin{bmatrix} y^{(1)} \\\\ …\\\\ y^{(m)} \\end{bmatrix}$ 代表训练集当中的所有的标签通过：\nθ=(X^TX)^{-1}X^Ty（相当于对$J(θ)$中的每一个θ求偏导数，并且使其等于0）可以求出最适合的θ现在改变在正规方程中加入一项：","url":"https://l61012345.top/2021/08/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%90%B4%E6%81%A9%E8%BE%BE/4.%20%E6%AD%A3%E5%88%99%E5%8C%96/4.3.%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/","tags":[],"categories":["机器学习基础课程——吴恩达","04. 正则化"]}