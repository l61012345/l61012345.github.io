{"title":"13.2. 随机梯度下降算法","id":"2021/08/22/机器学习——吴恩达/13. 大规模学习/13.2. 随机梯度下降算法/","date_published":"08/22/2021","summary":"随机梯度下降算法在13.1. 中提到使用传统的梯度下降算法来最小化大数据集的代价函数计算量非常大，因此需要找到一种方式来改进现有的梯度下降算法。一种可行的方式是随机梯度下降算法（Stochastic gradient desent）。  \n回顾：线性回归的梯度下降算法对于假设函数：$h_θ(x)=∑_{j=0}^mθ_jx_j$其训练集的代价函数为：  \nJ_{train}(θ)=\\frac{1}{2m}(h_θ(x^{(i)})-y^{(i)})^2使用梯度下降算法找到最小化$J_{train}(θ)$的参数$θ$，其内循环为：  \nθ_j:=θ_j-α\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}其中$\\frac{∂}{∂θ}J_{train}(θ)=\\frac{1}{m}∑_{i=1}^m(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}$梯度下降算法通过不断地迭代求梯度找寻局部最小值，最终达到算法收敛。上述的梯度下降算法对整个数据集中的所有项求和，在一次下降迭代中需要同时考虑整个数据集中所有的数据，称为批量梯度下降算法（Batch gradient desent）。  \n随机梯度下降的思路随机梯度下降算法在每一次迭代时不需要考虑所有的数据。观察批量梯度下降算法，可以发现代价函数的本质实际上是衡量参数$θ$对每一个某个样本$(x^{(i)},y^{(i)})$的拟合程度，再取平均值。因此代价函数$J_{train}(θ)$可以被分解为：  \ncost(θ,(x^{(i)},y^{(i)}))=\\frac{1}{2}(h_θ(x^{(i)})-y^{(i)})^2如之前所说，这一部分衡量的是参数$θ$在某个样本$(x^{(i)},y^{(i)})$上的具体表现情况。原来的代价函数可以改写为：  \nJ_{train}(θ)=\\frac{1}{m}cost(θ,(x^{(i)},y^{(i)}))按照上文的理解方式，$J_{train}(θ)$可以看做是衡量参数$θ$对数据集整体的平均表现。与批量梯度下降算法不同的是，随机梯度下降算法的每一次迭代只观察数据集中的一个样本$(x^{(i)},y^{(i)})$，根据这一个样本的评价来缩小$θ_j$直到遍历完整个数据集。然后重复这一遍历数据集，分别以每个样本的评价来缩小$θ_j$的过程，直到$θ_j$达到收敛。  ","url":"https://l61012345.top/2021/08/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%90%B4%E6%81%A9%E8%BE%BE/13.%20%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%AD%A6%E4%B9%A0/13.2.%20%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/","tags":[],"categories":["机器学习基础课程——吴恩达","13. 大规模机器学习"]}