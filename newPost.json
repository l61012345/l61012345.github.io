{"title":"9.6. PCA算法优化","id":"2021/08/03/机器学习——吴恩达/9. 非监督学习/9.6. PCA优化/","date_published":"08/03/2021","summary":"PCA算法优化主成分数量的选取$K$称作主成分的数量，通常$K$的选取与如下的两个参数有关：平均投影误差的平方：  \n\\frac{1}{m}∑_{i=1}^m|x^{(i)}-x^{(i)}_{approx}|^2$x_approx=U_{reduce}z$，是通过$z$复原后得到的向量。反应每一个数据到投影的距离之和。数据的方差：  \n\\frac{1}{m}∑_{i=1}^m|x^{(i)}|^2反应每一个数据到原点的距离之和。$K$的选取通常遵循以下规律：满足最小的$K$,使得：  \n\\frac{\\frac{1}{m}∑_{i=1}^m|x^{(i)}-x^{(i)_{approx}}|^2}{\\frac{1}{m}∑_{i=1}^m|x^{(i)}|^2}≤ 0.1(1\\%)表示99%的数据中的特征被保留，仅有1%的数据特征被压缩。如果希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。  \n\n理解： \n\n投影误差对应的是被放弃的$n-k$个特征值$λ$之和，而均方值对应的是所有的$n$之和，因此从这一点来看，$\\frac{∑_{i=1}^k λ_i}{∑_{i=1}^n λ_i}≥99\\%$的说法与此处的解释并无冲突。  \n1%其实是一个容错的区间，从5%到15%的区间内选取都是比较合理的，取决于具体问题。  \n\n\n实现过程从$k=1$开始应用PCA，计算出$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k=2$，如此类推，直到找到可以使得比例小于1%的最小$k$值（原因是各个特征之间通常情况存在某种相关性）。这样做的问题在于奇异值分解的过程计算量非常的大。实际上，在Octave中应用时，奇异值分解：svd()函数会返回三个矩阵U,S,V，其中U即为前文中提到的一个含有$n$个方向向量（也是特征向量）的矩阵（所有的列向量是方向向量/特征向量）。S是一个对角矩阵，对角线上的元素$s_{ii}$为特征值。$\\frac{\\frac{1}{m}∑_{i=1}^m|x^{(i)}-z^{(i)}|^2}{\\frac{1}{m}∑_{i=1}^m|x^{(i)}|^2}≤ 0.1(1\\%)$可以等价为：  \n1-\\frac{∑_{i=1}^ks_{ii}}{∑_{i=1}^ns_{ii}}≤0.01如此则只需要应用一次svd()函数通过返回的特征值矩阵$S$得到所有的特征值，尝试前$k$个特征值，直到找到符合条件的最小$k$即可。  ","url":"https://l61012345.top/2021/08/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%90%B4%E6%81%A9%E8%BE%BE/9.%20%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/9.6.%20PCA%E4%BC%98%E5%8C%96/","tags":[],"categories":["机器学习基础课程——吴恩达","9. 非监督学习"]}