{"title":"13.3. 小批量梯度下降算法","id":"2021/08/22/机器学习——吴恩达/13. 大规模学习/13.3. 小批量梯度下降/","date_published":"08/22/2021","summary":"小批量梯度下降算法小批量梯度下降算法（Mini-bath gradient descent）是另一种改善由大数据造成的计算量问题的梯度下降算法。小批量梯度下降算法的思路介于批量梯度下降算法和随机梯度下降算法之间，在一些情况下的表现比随机梯度下降算法更为出色。  \n小批量梯度下降算法的思路回顾之前的梯度下降算法：  \n\n批量梯度下降算法在一次迭代中使用了数据集中所有的样本  \n随机梯度下降算法在一次迭代中使用了数据集中的一个样本  \n\n而小批量梯度下降算法介于这两者之间，在一次迭代中选择使用$b$个样本，称为数据集中的一批(Batch)数据。$b$是每一批数据的批量大小（Batch-size）。通常$b$在2-100之间，常取$b=10$作为一次迭代所使用的数据量。内循环的更新方程：使用一个Batch的数据，求出平均梯度以更新$θ_j$。  \nθ_j:=θ_j-α\\frac{1}{b}∑_{k=i}^{i+b-1}(h_θ(x^{(k)})-y^{(k)})x_j^{(k)}更新指针$i$以切换到下一个batch：  \ni:=i+b直到所有batch的数据都执行完这一流程，重复遍历直到找到使得$J_{train}(θ)$最小化的$θ_j$。  \n隐式并行性如果有合适的向量化工具，小批量梯度下降算法在内循环的部分拥有并行性，意味着能够在一次迭代内能够并行计算多个batch的梯度并更新$θ_j$。虽然随机梯度下降也具有并行性，但是如果将随机梯度下降做并行处理，同时计算每一个样本的梯度，这样消耗的并行计算资源要比小批量梯度下降算法大得多。因此如果将小批量算法利用合适的向量化工具并行化，其计算速度会比批量梯度下降和随机梯度下降要快的多。  ","url":"https://l61012345.top/2021/08/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%90%B4%E6%81%A9%E8%BE%BE/13.%20%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%AD%A6%E4%B9%A0/13.3.%20%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/","tags":[],"categories":["机器学习基础课程——吴恩达","13. 大规模机器学习"]}