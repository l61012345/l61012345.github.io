---
title: 基于互信息的语义模式定理和遗传编程
date: 2024/03/2
category_bar: true
categories: 
- 论文
- 进化计算
- 遗传编程
---
# 基于互信息的语义模式定理和遗传编程
## 背景
### 遗传算法中的模式定理
所谓模式(schema)是指在搜索空间中一组具有某些共同特征的点的集合。模式的定义最早在遗传算法领域由Holland提出，他认为遗传算法中的模式是带有Don't care (\*)的位串(string)，Don't care部分的比特既可以是0也可以是1，因此一个模式实际上代表了若干种不同的个体。进一步地，Holland提出了模式定理(schema theory)，这个定理给出了某个特定的模式群体$H$在下一代$t+1$中被采样概率的下限，也就是这个子种群中的个体存活到下一代后的概率，这个概率表示为：  
$$P(H,t+1)≥P(H,t)\frac{f(H,t)}{\overline{f}}\left[1-p_c\frac{Δ(H)}{L-1}\left(1-P(H,t)\frac{f(H,t)}{\overline{f}}\right)\right](1-p_m)^{o(H)}$$
其中$p_c$和$p_m$分别是交叉和突变的概率；$Δ(H)$为$H$的定义距；$o(H)$为模式$H$中有意义的比特数量，称为阶数;$L$为个体位串长度；$f(H,t)$为第$t$代中模式$H$的平均适应度，$\overline{f}$是第$t$代所有个体的平均适应度。  
关于遗传算法中的模式定理此处并不展开讨论。  
需要知道的是，Holland的模式定理考虑了交叉和突变操作对于模式中有意义部分(也就是非Don't care部分)的结构上的破坏效应(disruptive effect)。  
根据Holland的模式理论，定义距小、平均适应度高、阶数更低的模式更容易在进化中被保留和存活，Goldberg将这样的模式称为积木块(Building block)。  
在后续的研究中，大部分的研究人员都同意，遗传算法等等一系列的进化计算算法是通过在进化的过程中让个体逐渐积累其承载的building block的个数来让个体的结构逐渐靠近目标解的结构，从而找到全局最优解。   

<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20240302103941.png width=50%>  

{% note info %}  
在后面的介绍中，由于习惯，Schema和Building block将不做翻译。  
{% endnote %}

### 遗传编程中的结构模式及其缺陷
#### 遗传编程中模式定义的发展
遗传编程中对于Schema最为广泛的定义仍然借用了遗传算法中对于模式的定义思路：遗传编程中的模式是搜索空间中一组结构(syntax)特征相似的点的集合。(a set of points in the search space that share some syntactic characteristics.)  
类比于遗传算法中带有Don't care的位串，遗传编程中的schema也是一组带有Don't care的树形结构。  
最早Altenberg定义遗传编程中的schema为一组含有同一个子树的树形个体，并且给出了在相当多条件限定下的遗传编程的模式定理。之后O'Reilly和Oppacher将Don't care的概念引入了遗传编程的schema，将schema定义为一个含有泛化子树(即用Don't care 作为代替子树，用“\#”表示)，Don't care可以表示此处任何有效的子树。每个schema包括两个部分，子树的结构以及相应的最低出现在schema采样(也就是这个schema对应的一群个体)中的出现次数。在之后Poli和Langdon将Don't care的定义延展到了函数集和端点集，并用“=”表示：Don't care可以是任何有效的端点或者函数；他们定义schema是一颗完整的含有“=”的树。之后Poli将两种不同的Don't care结合，定义出Hyper schema：构成schema的函数集中包括=，端点集中则包括了\#和=.    
关于遗传编程中schema定义的发展在[^1]中有更详细的介绍。  

[^1]: Zahra et al, Semantic Schema theory for genetic programming, Apply Intelligence, 2016.  

总而言之，这些对模式的定义都认为，**结构是承载遗传信息的关键**，如果个体的结构越靠近目标解的结构，那么这个个体的表现就越好。因此，遗传编程中设计了以自动定义函数(automatically defined function, ADF)为代表的结构保护机制。以保护有用的结构片段不受到交叉和突变带来的印象。  

#### 结构schema的缺陷
上述对于schema的定义都围绕着Don't care，也就是一组结构相似的个体的集合展开。  
但是，遗传编程和遗传算法不同的地方来自，在遗传算法的线性结构中，语义的变化与结构的变化是一一对应的。这是Holland的模式定理有效的条件，即搜索空间均匀，变异操作在任意比特上改变所对应的在适应度函数的空间中对应点搜索步长应当是一致的。

<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20240302122733.png width=50%>  

而在遗传编程中，由于函数集的出现导致个体的结构很难做到信息上的的均匀分布，也就是说，位于不同位置上进行的变异操作结果的得到语义可能存在相当巨大的差异。  
最终导致的结果是，一个结构的schema，也就是一组结构相似的个体中对应个体的适应度表现差异可能相当巨大。基于结构schema得到的模式定理不能很好的预测某个schema在未来的表现。  

## 基于互信息的语义模式
因此，一部分研究人员认为，相比于归纳个体的结构，更应该归纳个体的表现：**将那些表现相似的个体归纳为一个schema，变异和交叉会有大概率在那些表现好的schema中进行，如此可以加快遗传编程的搜索速度**。这是基于语义的遗传编程的基本思想。  

### 语义
个体的表现称为语义(semantic)。传统的研究认为，适应度(fitness)可以作为衡量个体语义的标准。但是遗传编程个体结构信息分布的不均匀导致个体或者某些子种群的适应度无法在进化过程中展现连续的变化。此外，适应度对于个体的结构过于敏感以至于无法正确的识别出具有高贡献的building blocks。**也就是说具有高贡献的building blocks的个体未必具有高适应度**。某些适应度高的个体可能会因为在遗传操作中出现微小的变化而导致适应度急剧下降，反之依然。  
因此，除了使用适应度之外，还需要使用别的特征来正确反映种群在进化过程中的进展。  

#### 互信息
[^1]中使用了个体输出与真实输出之间的互信息(mutual information)来衡量个体的语义。一个个体$t$的语义具体表示为：  
$$S(t)=\frac{I(\hat{Y},Y)}{H(Y)}$$
其中$Y$是真实数据集$\{(x_i,y_i)\}$中的输出，$\hat{Y}$是这棵树在给定数据集的输入时的输出：$\hat{y}_i=t(x_i)$；$I(\hat{Y},Y)$是两者的互信息，表示为：  
$$I(\hat{Y};Y)=\sum_{\hat{y}\in\hat{Y}}\sum_{y\in Y}p(y,\hat{y})log(\frac{p(y,\hat{y})}{p(y)p(\hat{y})})$$
其中$p(y,\hat{y})$是$Y$和$\hat{Y}$的联合概率分布；$p(y)$和$p(\hat{y})$分别是$Y$和$\hat{Y}$的边缘概率分布。  

$H(Y)$是目标输出的信息熵，此处用于进行归一化，表示为：  
$$H(Y)=-\sum_{y\in Y}p(y)log(p(y))$$

{% note info %}  
对两个随机变量$X$，$Y$，其互信息$I(X;Y)$衡量了两者的相互依赖关系，即在一方部分不确定的情况下，另一方的不确定程度。因此$I(X;X)=H(X)$.其本质上是对联合概率分布定义的拓展，如下图所示：   
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20240302140346.png width=60%>  

其中$H(X,Y)$为$X$与$Y$的联合熵，有：  
$$\begin{aligned}
    H(X,Y)&=-\sum_{x\in X}\sum_{y \in Y}log(p(x,y))\\
    &=H(X|Y)+H(Y|X)+I(X;Y)
\end{aligned}$$
{% endnote %}

在[^2]中作者使用了直方图抽样（即用抽样频率替代总体概率）来估计$\hat{Y}$和$Y$的互信息，因此上述式子可以改写为：  
$$I(\hat{Y};Y)=\sum_{i=1}^{k_{\hat{y}}}\sum_{j=1}^{k_y}log\left(\frac{L_{ij}N}{L_iL_j}\right)$$
$$H(Y)=-\sum_{j=1}^{k_y}log\left(\frac{L_j}{N}\right)$$
其中$k_{\hat{y}}$和$k_y$分别是$\hat{Y}$和$Y$样本统计直方图中的样条(bins)总数；$N$是样本数；$L_i$是$\hat{Y}$的直方图中第$i$个样条的纵坐标，代表落入该范围的$\hat{y}$的样本的频率；同理$L_j$是$Y$的直方图中第$j$个样条的纵坐标；$L_{ij}$代表样本$(\hat{y},y)$同时落入$\hat{Y}$直方图中第$i$个样条和$Y$直方图中落入第$j$个样条的概率。    

[^2]: Zahra et al, Semantic Schema modeling for genetic programming using clustering of building blocks, Apply Intelligence, 2018.  

同时定义，如果两个树$t_1,t_2$的输出$T_1=t_1(X)$,$T_2=t_2(X)$存在如下关系：  
$$\frac{I(T_1;T_2)}{H(T_1,T_2)}=1$$
称$t_1$和$t_2$在语义上是相等的。  

使用互信息最大的特点是：**对随机变量进行可逆的（加和倍数乘积）操作后，互信息保持不变**。   

下面折叠的部分中展示了一个用matlab编写的关于互信息这一特性的小实验。  
<details>
    <summary>关于互信息的小实验</summary>
在MATLAB中，两个随机变量的互信息可以通过直方图抽样估计得到：  

```matlab
function MI = mutual_information(X,Y)
    % 计算联合概率
    jointXY = histcounts2(X,Y,'Normalization','probability');
    % 计算边缘概率
    marginalX = sum(jointXY,2);
    marginalY = sum(jointXY,1);

    % 计算互信息
    MI = 0;
    for i = 1:length(marginalX)
        for j = 1:length(marginalY)
            if jointXY(i,j) ~= 0
                MI = MI + jointXY(i,j)/numel(jointXY) * log2(jointXY(i,j) * numel(jointXY)/(marginalX(i)*marginalY(j)));
            end
        end
    end
end
```

假设数据集$\{X,Y\}$符合规律$y=e^{2x}+x$，现在检验各种不同的个体结构与真实规律的互信息：  
```matlab
x = [0:0.01:1000];x = [0:0.01:1000];
y_target = exp(2*x)+x;
y_tree_1 = exp(2*x);
y_tree_2 = exp(2*x)+1;
y_tree_3 = exp(x);
y_tree_4 = x;
y_tree_5 = 2*x+1;
y_tree_6 = 2*exp(x);
y_tree_7 = power(exp(x),2);
y_tree_8 = power(exp(x)+x,100);
y_tree_9 = sin(x);
y_tree_90 = power(x,2);
y_tree_rand = randi(12308,1,100001);
y_tree_de = 2*exp(2*x)+1;

mi_tree_target = mutual_information(y_target,y_target); % 输出的信息熵
mi_tree_rand = mutual_information(y_tree_rand,y_target)/mi_tree_target;
mi_tree_1 = mutual_information(y_tree_1,y_target)/mi_tree_target;
mi_tree_2 = mutual_information(y_tree_2,y_target)/mi_tree_target;
mi_tree_3 = mutual_information(y_tree_3,y_target)/mi_tree_target;
mi_tree_4 = mutual_information(y_tree_4,y_target)/mi_tree_target;
mi_tree_5 = mutual_information(y_tree_5,y_target)/mi_tree_target;
mi_tree_6 = mutual_information(y_tree_6,y_target)/mi_tree_target;
mi_tree_7 = mutual_information(y_tree_7,y_target)/mi_tree_target;
mi_tree_8 = mutual_information(y_tree_8,y_target)/mi_tree_target;
mi_tree_9 = mutual_information(y_tree_9,y_target)/mi_tree_target;
mi_tree_90 = mutual_information(y_tree_90,y_target)/mi_tree_target;
mi_tree_de = mutual_information(y_tree_de,y_target)/mi_tree_target;
```
结果如下：  

|编号|归一化互信息的值|
|:-:|:-:|
|1|1|
|2|0.2674|
|3|1|
|4|0.2832|
|5|0.2409|
|6|1|
|7|1|
|8|0.017|
|9|0.3223|
|90|0.2674|
|de|1|
|rand|0.2674|


</details>

也就是说，只要个体的结构不发生较大的变化，在进化的早期互信息就可以持续追踪那些高语义值的个体。  

### 语义的building blocks和语义模式
由于将结构的空间投射到了新的语义空间，因此作者重新定义了building blocks和schema。  
在语义的空间中，结构相似的个体的语义未必相近，反之亦然。无法再像之前的定义通过结构的相似直接提取schema，因此此处作者采用的方法是首先将语义相同的子树归纳为building blocks，再从building blocks构建schema。  

#### 语义的building blocks
作者认为，语义的building blocks(semantic building blocks)是满足下列三个条件的子树：  
- 高语义值
- 该语义值在所有子树的语义值分布中出现频率高
- 子树的大小不能超过某个阈值


#### 语义模式
Schema可以看做是若干building blocks按照某种规律进行特定组合构成的building blocks的集合。因此，此处作者定义一个语义schema由两部分组成：  
- building blocks的集合  
- 描述building blocks在schema内部分布特征的函数，以便判断哪些个体属于这个schema  
  如果一个个体的building blocks的分布特征符合这个函数，那么这个个体就属于对应的这个schema.  

## SBGP
### 概览
根据语义building block和语义schema的定义，作者在[^3]中提出了SBGP(semantic Building block GP)。  

[^3]: Zahra et al., Semantic schema based genetic programming for symbolic regression, Applied Soft Computing, 2022.  

SBGP的核心思想是从种群中正确识别出building blocks，再将building blocks组建为schema，然后搜索空间中的搜索进程将主要在那些语义值高的schema子空间中进行。另外，为了保证算法不会陷入局部最优，SBGP中还设计了一些机制用于保护种群中的基因多样性。  

<img src=https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20240303171759.png width=90%>   

SBGP的算法流程由上图所示，对传统遗传编程的改进如下：对语义schema的提取和实例化、局部的遗传操作。  

### 语义building blocks的提取
语义的schema无法被直接通过结构上的相似性进行提取，而是通过语义的building blocks进行构建。作者认为语义的building blocks是满足 @[TOC](语义的building blocks)之前提到的定义的子树。  
提取语义的building blocks的方法如下：  
1. 计算整个种群中所有个体的语义值和平均语义值，筛选出语义值高于平均语义值的个体  
2. 为了筛选符合条件的子树，需要计算符合大小限制的当前种群中所有子树的语义值和相同语义值的子树的出现频率。因此，每一个（大小在限定范围内的）子树都记录了两个信息：其语义值和以及出现频率  
3. 具有高语义值和高出现频率的个体将会被视作语义的building blocks  
  
#### 最小语义树
最小语义树(semantic representative tree)指的是在语义相同的树当中大小最小的一棵树。最小语义树用于衡量某个语义值下的子树是否可以满足大小条件成为building block。  


全过程伪代码如下：  
```
C: cluster of Trees
minSS: minimum acceptable semantic similarity for building blocks
maxFrequencyRank: maximum acceptable frequency rank for building blocks
minBlockSize: minimum acceptable size for building blocks

for each semantic value ss in C:
    freq = number of occurrences of all subtrees in C having semantic value = ss
    srt = the smallest subtree having semantic value of ss # 最小树的定义在[3]中未被使用
    add <ss,freq,srt> to ssList
end
sort ssList according to freq
rank = 0
for each semantic value ss in ssList:
    if ss > minSS && size(ssList(ss).srt)>minBlockSize && rank < maxFrequencyRank
        add srt to blockList
        rank++
    end
end
return blockList
```


### 语义Schema的提取
按照定义，语义的schema
