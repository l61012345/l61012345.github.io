---
title: 生成对抗网络
date: 2022/09/04
categories: 
- 论文
- 神经网络

category_bar: true
---
# 生成对抗网络
> *Generative Adversarial Nets*, Ian J et al, 2014.

## 简介
### 问题动机
深度学习的意义是挖掘人工智能应用领域不同种类的数据集的丰富的、层次化的概率统计分布模型。目前，深度学习领域最成功的研究已经包括了鉴别模型(discriminative model)——通常是将高维度，丰富的感知输入映射到类别标签的模型。判别模型中常常使用反向传播和dropout算法以及分段线性单元（piecewise linear unit），它们具有特别良好的梯度。   
人工智能的另一种模型是生成模型(generative model)，即根据数据集分布特征生成新数据的人工智能模型。深度的生成模型(generative model)在研究领域的影响不大的原因有二：  
- 应用最大似然估计和相关策略时出现了的概率计算困难  
- 在生成上下文中难以应用利用分段线性单元的优势  
{% note info %}  
使用ReLU函数作为激活函数的神经元是一种典型的分段线性单元。ReLU函数可以表示为：  
$$f(x)=max(0,x)$$
ReLU函数的优点是计算量小，收敛速度快。缺点是当较大梯度通过ReLU神经元，更新参数之后，这个神经元不会对任何数据有激活现象，神经元梯度将永远为0.  
{% endnote %}  

### 网络框架
本文提出了一种新的可以避免这两种问题的新的生成模型估计程序，称为生成对抗网络(GAN, generative adversarial nets)。这种对抗网络中，除了生成模型外，还有一个与之对抗的鉴别模型：鉴别模型，又称为鉴别器(distriminator)学习鉴别某个样本来自生成模型，又称为生成器(generator)的概率分布（是生成模型所生成的样本），还是来自原数据集的概率分布（是原数据集中真实存在的样本）。   
上述过程可以将生成器比作生产假钞的造假者(counterfeiter)、鉴别器比作识别假钞的警察。造假者需要尽可能的将假钞制作得逼真到使警察无法识别，而警察则需要不断练习以鉴别真钞和假钞。  
最终，如果造假者造出了警察无法识别的假钞，那么造假者将赢得这场竞争。即当鉴别模型无法鉴别样本源头时，算法终止。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220904161642.png width=60%>  

整个网络框架可以用多种模型和优化算法实现。本文中生成器和鉴别器的结构都是多层感知机(multi-layer perceptron)，其中生成器的输入是随机噪声。两者的训练方式均采用反向传播和dropout算法，并仅使用正向传播从生成模型中进行采样。训练过程中不需要任何近似推断(approximate inference)和马尔科夫链(Markov chain)。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220904164810.png width=60%>  

## 既有方法
大多数有关深度生成模型的工作都集中在提供一个概率分布函数的参数规范的模型上。然后可以通过最大化对数似然来训练模型。文章中列举出了几种比较有代表性的生成模型，如下表所示：  

| 名称 | 概要 | 缺点 |
|:-|:-|:-|
|玻尔兹曼机<br>Boltzmann machine| 模型的相互作用表示为没有归一化的势函数的乘积，通过对随机变量的所有状态进行求和或者定积分进行归一化| 梯度不可追踪<br>混合对依赖于马尔科夫蒙特卡洛方法的学习算法造成了极大的问题 |
|深度信念网络<br>DBN, deep belief networks| 含有一个无向层和几个有向层的混合模型 | 无向模型和有向模型都存在计算困难 |  
|噪声对比估计<br>NCE,noise-contrastive estimation | 需要设定一个判别训练标准以拟合生成模型，生成模型本身用于辨别从固定分布的噪声中生成的样本| 由于使用了固定分布的噪声，学习速率在学习模型（甚至是相当一小部分数据）之后显著变慢|
|生成随机模型<br>GSN, generative stochastic network| 从所需的分布中抽取样本训练生成器，可以看做是参数化的马尔科夫链 | 需要计算马尔科夫链 |

而对抗网络框架不需要马尔科夫链进行采样。由于对抗网络在训练生成器的过程中不需要循环反馈，生成器的训练只是单次的，梯度下降的更新速率较慢，一定程度上避免了分段线性单元出现过早收敛的问题。从而可以更好的利用分段线性单元、提高反向传播的算法性能。  
{% note info %}  
但是这样的梯度下降方式也会导致GAN的收敛非常不稳定。  
{% endnote %}  


## 数学方法

