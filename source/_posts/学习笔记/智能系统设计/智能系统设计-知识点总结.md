---
title: 智能系统和数字图像处理应用-知识点总结
category_bar: true
date: 2022/12/27
categories: 
- 学习笔记
- 智能系统和数字图像处理应用
---
# 智能系统和数字图像处理应用-知识点总结
# 智能系统部分
机器学习包括了四种类型：  

| | |
|:-:|:-|
|监督学习<br>(supervised learning)|机器学习数据及其对应的标签/输出，从而生成模型。|  
|无监督学习<br>(unsupervised learning)|机器根据输入的数据自动挖掘数据特征，以此建模，不需要输入数据对应的标签。|
|半监督学习<br>(semi-supervised learning)|监督学习与无监督学习相结合的一种学习方法。半监督学习使用大量的未标记数据，以及同时使用标记数据，来进行模式识别工作。  |
|增强学习<br>(reinforcement learning)|一种反馈学习系统。由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。|

## 神经网络
### 感知机
下图展示了神经网络中一个神经元的结构：  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915145155.png width=50%>  
如上图所示，假设对一个神经元，其输入为来自若干其他神经元的输出$x_i$，那么该神经元的输出$y$可以用数学公式表达为：  
$$y=f\left[(∑_{i=0}w_ix_i)+b_i\right]$$
其中$b_i$表示该神经元的偏置(offset)，用于线性修正；$f[·]$是该神经元的激活函数。每一个神经元中的激活函数承担了对数据进行简单处理的任务。激活函数可以是线性的，也可以是非线性的。  

这样的由一个神经元构成的神经网络称为感知机(perceptron)。通过选择合适的激活函数和权重，这样的单层神经网络可以实现一些基本的逻辑函数功能、例如逻辑与(AND)、逻辑或(OR)、逻辑非(NOT)等。  
感知机的问题是**无法提取数据的非线性特征**。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220912172724.png width=60%>   
**可以发现AND和OR在样本空间中的表示都可以用一条直线对其进行划分，而对XNOR无法找到一条直线将其进行区分。对于感知机而言，由于其输出表示为多个神经元的线性组合，通过Sigmoid函数，只能找到数据的线性边界。**
对于这样的非线性逻辑函数，需要添加多层神经元才能完成逻辑函数的功能。


### 多层神经网络
整个神经网络的学习过程包括：  
- 随机初始化连接权重  
- 当均方差非常大，或者数代均方差变化很大时，执行如下的循环： 
  - 前向传播：带入每一个数据$(x_i,y_i)$中的$x_i$到神经网络中，计算神经网络中每一个神经元对每一个数据的输出$a_{jk}$和神经网络对每一个数据的预测值$\hat{y}$，并整理为向量。  
  - 计算真实值$y$与预测值$\hat{y}$之间的差距。  
  - 使用验证集对神经网络的准确率进行测试。
  - 反向传播：计算$δ$并更新每一层每一条连接的权重。  
- 使用测试集对训练好的神经网络的准确率进行测试。  

#### 前向传播
简单来说，前向传播的过程即将数据$x$带入到神经网络的表示中，得到输出的过程：  
$$\hat{y}=a^{(N)}=f(z^{(N)})$$  
$N$表示神经网络最后一层的标识。  
在前向传播的过程中，会得到每一个神经元的输出$a_{jk}$

#### 反向传播
反向传播的过程是根据实际值和预测值之间的误差，从输出层开始，逐层调整各层神经元连接权重的过程。  
在输出层第$N$层，根据得到的预测结果$\hat{y}$，使用如下式子来衡量与实际结果$y$之间的差距：  
$$δ^N_{jk}=(y_j-\hat{y}_j)y_j(1-y_j)$$
（$\hat{y}_j$和$y_j$都是向量，其维度等于数据集大小）  
对于隐藏层的神经元，如下式子表示了其神经网络单元输出的修正：  
$$δ^i_{jk}=a^i_j(1-a^i_j)∑_{m}δ^{i+1}_{mj}w^{i+1}_{mj}$$
简单来说即$a^i_j(1-a^i_j)$后一层中与该神经元相连的神经元的权重和修正的乘积。  
权值的修正过程表示为：  
$$Δw_{jk}=ηδ_{jk}a^i_j$$
$$w_{jk}:=w_{jk}+Δw_{jk}$$
其中，$η$是一个可以调整的参数，称为学习率(learning rate)。通过学习率可以控制权值一次性更新的幅度，换言之，即学习的快慢。学习率越大，权重更新的幅度越大，学习速度越快。  
上面所示的这个权重修正的方法称为梯度下降算法(gradient desent)。  
如此，神经网络在反复的前向传播和反向传播迭代（每一次迭代称为一轮，epoch）中不断地修正各连接的权重，直到使得真实值$y$与预测值$\hat{y}$之间的差距小到可以接受或者一直不变。这种情况称算法运行达到了收敛(convergence)。
通常，真实值$y$与预测值$\hat{y}$之间的差距是通过均方差(MSE,Mean Square Error)进行衡量的:  
$$MSE=\frac{1}{n}∑_{i=0}^{n-1}(\hat{y}_i-y_i)^2$$

### 调试
#### 动量
为了避免算法陷入局部最小值，目标函数中使用了动量(momentum)，该动量项$α$是介于0和1之间的值，该值通过尝试从局部最小值跳到最小值而增加了步长。如此，修正后的权重应当为：  
$$Δw_{ij}:=ηδ_ja^i_j+αΔw_{ij}$$
如果动量项较大，则学习率应该保持较小。动量值很大也意味着收敛将很快发生。但是如果将动量和学习率都保持在较高的值，那么算法可能会大步跳过最小值。  
较小的动量值不能可靠地避免局部最小值，并且还可能减慢系统的训练速度。如果梯度不断改变方向，动量也有助于平滑变化。  
正确的动量值可以通过命中和试验来学习，也可以通过交叉验证来学习。  

#### 过拟合
过拟合(overfitting)指神经网络的模型对数据的拟合的程度过高，过拟合意味着模型泛化能力低。模型能够很好的拟合当前的数据集，但是并不适应新的数据。  
<img src= https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220917134821.png width=40%>  

过拟合的模型波动较大、具有高方差的性质。  

#### 超参数
像学习率和动量这样的并非通过反向传播调整、而是在训练的开始就设置好的参数称为超参数(hyper-parameter)。对超参数进行调整的目的是为了改进学习过程的准确率和收敛速度。  

| 参数 | 过大结果 | 过小结果 |
|:-:|:-:|:-:|
|训练轮次|过拟合|欠拟合|
|学习率|学习过程不稳定|收敛缓慢|
|动量系数|过拟合|陷入局部最小值|
|神经元数量|过拟合|无法充分提取特征|

#### 迁移学习
迁移学习(transfer learning)是一种学习方式，其使用用某个数据集训练好的权重神经网络，冻结训练好的某些层中的权重，再用这个网络训练另一组全新类型的数据集，这次训练中冻结的权重将不会发生任何改变。  
迁移学习可以改进数据集数据量小导致的问题。  

### 其他类型的神经网络
除了基于反向传播的神经网络外，神经网络还有其他的几种类型。  

#### 自组织映射
自组织映射(SOM,self-organizing map)是一种只有两层的无监督学习神经网络。通过学习输入空间中的数据，生成一个低维、离散的映射(Map)，从某种程度上也可看成一种降维算法。它最重要的应用是用于聚类(clustering)。  
不同于一般神经网络基于损失函数的反向传播来训练，它运用竞争学习(competitive learning)策略，依靠神经元之间互相竞争逐步优化网络。且使用近邻关系函数(neighborhood function)来维持输入空间的拓扑结构。  
SOM的结构如下：  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220812094845.png width=50%>  

SOM只有两层，第一层为输入层，第二层为输出层，也称为竞争层(computational layer)。  
输入层神经元的数量是由输入向量的维度决定的，一个神经元对应一个特征。  

##### 竞争学习策略
对于SOM而言，神经网络中的权重仍然要进行随机初始化。其后权重的更新仍然基于权重更新算法：  
$$w:=w+ηδa$$
然而，由于其是一个无监督学习算法，无法对网络输入数据集的标签，因此此处的$δ$采用数据点$\boldsymbol{X}$到每一个神经元的权重之间的欧氏距离进行衡量：  
$$d=||\boldsymbol{X}-\boldsymbol{W}||=\sqrt{∑(x_i-w_i)^2}$$
与基于反向传播的神经网络不同的是，此处需要计算数据点到所有神经元的欧氏距离，并且找到到该数据点欧氏距离最短的神经元$\boldsymbol{W_{win}}$。然后使用该输入对该神经元的权重进行更新：  
$$\boldsymbol{W_{win}}:=\boldsymbol{W}+η(\boldsymbol{X}-\boldsymbol{W_{win}})$$
此外，这个神经元的权重$\boldsymbol{W_{win}}$还会对周围的神经元的权重造成影响，影响的大小服从近邻关系函数$θ(N)$，其中$N$表示的是影响范围内某个神经元距离赢家的神经元距离，下图表示了$N=1$和$N=2$时赢家（编号为13的神经元）对周围神经元的影响；  
<img src= https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220917143055.png width=50%>  

$θ(N)∈[0,1]$，表示该距离所对应的对其权重的影响与原来的百分比。赢家周围的神经元的权重更新表示为：  
$$\boldsymbol{W_{neigh}}:=\boldsymbol{W}+η(\boldsymbol{X}-\boldsymbol{W_{neigh}})θ(N)$$

训练完成后，所有的数据点都被映射到竞争层的神经元，在竞争层观察到竞争层的神经元移动到输入层数据密集的区域，从而自发地形成数据簇，完成聚类。  

#### 循环神经网络
循环神经网络(Recurrent Neural Network, RNN)是一种专用于设计处理时序数据的神经网络，其结构上与普通的多层神经网络不同的是，它的隐含层具有自循环的结构，通过这样的自循环，当前的隐含层神经元的输出与上一个时刻神经元的输出建立联系。  
<img src= https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20221226154349.png width=50%>  

循环神经网络的前向传播过程类似于普通的神经网络，它每个神经元的输出可以表示为：  
$$y(t)=f\left[(∑_{i=0}w_ix_i(t))+b_i+y(t-1)\right]$$
反向传播过程和普通的神经网络相同，但是需要更新自循环的权重。  

#### 卷积神经网络·深度学习
卷积神经网络(Convolutional Neural Network, CNN)是一种最初设计用于处理图像的神经网络。它可以看做是一组自适应图像滤波器，其每一层的神经元是用于处理图像的卷积核，卷积核内的参数通过反向传播进行修正。  

卷积神经网络属于深度学习(deep learning)的范畴，相比于一般的神经网络，它们具有如下特点：  
- 通过设置一系列的、更加复杂的级联网络结构可以深度挖掘数据的非线性特征。  
- 深度学习网络通常具有分层级的表示和结构。  
- 可以是监督学习也可以是非监督学习。  
