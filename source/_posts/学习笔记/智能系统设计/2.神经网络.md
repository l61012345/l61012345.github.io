---
title: 02.神经网络
category_bar: true
date: 2022/09/15
categories: 
- 学习笔记
- 智能系统设计
---
# 神经网络
## 介绍
神经网络(ANN，Artificial Neural Network)是人工智能中重要的一个分支领域，它指基于类脑结构的非严格建模系统。一个神经网络中通常含有非常多的处理单元，称为神经元(Neron)。神经元之间有着多个单向的连接。这些连接负责将数据从一个神经元的输出加权并传递到另一个神经元的输入。每个神经元只能对到达这个神经元的数据进行一些简单的处理，可能会有本地存储。  
像其他的人工智能算法，神经网络也具有学习功能。在学习过程中，神经网络根据所输入的数据及其标签（对于监督学习）自动调整网络中每两个神经元之间连接的权重。  

### 神经元的生理学结构
神经网络的结构灵感来自于大脑中神经元之间的信息传递过程。下图展示了大脑中一个神经元的结构：  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915145422.png width=50%>   
如图所示，神经元有很多的输入通道（树突），同时通过轴突给其他的神经元传递信号。大脑中的每一个神经元的树突接收来自多个其他神经元的输入，同时该神经元的轴突向其他神经元输出。  
对于神经网络中的一个神经元，其输入为来自多个其他神经元的输出数据，这样的连接类似于树突的结构；其内部通过一种简单的函数：激活函数(activation function)实现对数据的简单处理。和轴突一样，每个神经元的激活函数只有一个输出结果。下图展示了神经网络中一个神经元的结构：  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915145155.png width=50%>  

如上图所示，假设对一个神经元，其输入为来自若干其他神经元的输出$x_i$，那么该神经元的输出$y$可以用数学公式表达为：  
$$y=f\left[(∑_{i=0}w_ix_i)+b_i\right]$$
其中$b_i$表示该神经元的偏置(bias)，用于线性修正；$f[·]$是该神经元的激活函数。每一个神经元中的激活函数承担了对数据进行简单处理的任务。激活函数可以是线性的，也可以是非线性的。  
{% note info %}  
激活函数的选择依赖于数据集的分布特征。  
目前常用的激活函数包括：  
- Sigmoid函数:$y=\frac{1}{1+e^{-θ^T X}}$
- 线性函数:$y=θx$ 
- 分段线性函数/线性整流单元/ReLU函数：$y=\begin{cases} 0,x<0\\x,x≥0\end{cases}$ 
- 高斯函数：$y=exp(-x^2)$
- 双曲正切函数：$y=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$  
- 对数函数：$y=log(x)$
{% endnote %}  

### 感知机
这样的一个神经元也可以构成单层的神经网络，如图所示：  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915151203.png width=50%>  
如果这个神经网络中的输入和输出都是二进制数0或1，通过选择合适的激活函数和权重，这样的单层神经网络可以实现一些基本的逻辑函数功能、例如逻辑与(AND)、逻辑或(OR)、逻辑非等。这样的单层神经网络被称为感知机(perceptron)。  
感知机实现逻辑功能的过程及其由于只能找到数据的线性边界而无法进行非线性决策的问题（又称为异或问题，因为无法实现异或逻辑功能）请参见：[机器学习-吴恩达 5.3 感知机](https://l61012345.top/2021/03/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%90%B4%E6%81%A9%E8%BE%BE/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5.3.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0/)一讲。  


### 多层神经网络的结构
感知机的局限性最终由多层神经网络的发明而化解，多层神经网络可以实现非线性的决策边界。  
多层神经网络是由多个神经元层构成的集合，每一个神经元层中含有多个神经元。位于本层的神经元接收来自上一层神经元的输出，并用激活函数处理，然后将处理结构输出到下一层的神经元中。整个多层神经网络的结构如下图所示：  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915152124.png width=50%>  

如图所示：多层神经网络的结构主要可以分为三层：  
**输入层**(input layer)用于接收和处理数据。输入层的神经元个数等于输入变量的个数。传统的神经网络中，输入层的神经元是冗余的(dammy)，它们一般不会对输入数据做任何处理。  
**隐含层**(hidden layer)用于对数据进行进一步的处理和特征提取。隐含层可以不止有一层，其每一层神经元的个数和隐含层的层数由数据的复杂程度以及建模方法决定。数据越复杂，所使用的隐含层数越多。如果选择构建多个隐含层，通常情况下每一个隐含层中的单元数都是相同的。隐含层的单元数越多越好，但是隐藏单元数的增加会导致计算量的增大。因此每一个隐含层中隐藏单元的数目通常与输入层的维度，即特征的数目相匹配（是其整数倍）此外，每一层隐含层所使用的激活函数必须是相同的。  
**输出层**(output layer)用于输出处理结果。输出层的神经元个数等于输出变量的个数。  
需要注意的是，图中每一层的每一个神经元都与下一层的每一个神经元相连，这种拓扑结构称为“全连接”(full-connected)，事实上非全连接的神经网络，即每一层的单个神经元之与下一层的某些特定神经元相连的拓扑结构也是存在的。  
为了方便表述，令$w^i_{j,k}$表示神经网络中第$i$层，前一层第$k$个神经元到本层第$j$个神经元的连接的权重；以$a^i_j$表示第$i$层第$j$个神经元的输出；$b^i_j$表示第$i$层第$j$个神经元的偏置。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915173842.png width=50%>  

## 神经网络的学习
对于神经网络的训练过程，在每次迭代中，首先需要得到数据集的特征$x$输入到神经网络模型后的输出结果，即模型的预测值$\hat{y}$，然后根据其真实标签$y$返回来调整神经网络中各神经元连接的权重。因此将神经网络的每一次迭代分为前向传播(Forward Propagation/Feedforward Propagation)和反向传播两个过程：前向传播是将数据输入神经网络，然后得到预测值的过程。反向传播则是根据真实值和预测值之间的差距，来调整各连接权重的过程。  

### 数据集划分
对于学习所需要的数据集$(X,Y)=\{(x_0,y_0),(x_1,y_1),...,(x_n,y_n))\}$，一般将其划分为三部分：用于训练调整神经网络权重的训练集(training set)；用于测试训练好的神经网络效果的测试集(testing set)；用于在训练过程中观察神经网络性能，以便于及时调整训练参数的验证集(validation set)。通常三者占比在70%、15%、15%左右。  
对于数据集的大小，机器学习遵循“数据饥饿”（data hungry）准则，即数据集越多，模型的训练效果越好。  

### 初始化权重
需要注意的是训练的最初期，所有的权重通过一定的方式进行初始化。常见的初始化方法是在高斯分布中进行随机抽样，此外还有深度学习使用的何恺明初始化方法(Kaiming-He initialization)等等。但是一般随机初始化的权重都会进行归一化操作，使它们的范围分布在$[0,1]$之间，便于学习和调整。  

### 前向传播
简单来说，前向传播的过程即将数据$x$带入到神经网络的表示中，得到输出的过程：  
$$\hat{y}=a^{(N)}=f(z^{(N)})$$  
$N$表示神经网络最后一层的标识。  
在前向传播的过程中，会得到每一个神经元的输出$a_{jk}$

### 反向传播
反向传播的过程是根据实际值和预测值之间的误差，从输出层开始，逐层调整各层神经元连接权重的过程。  
在输出层第$N$层，根据得到的预测结果$\hat{y}$，使用如下式子来衡量与实际结果$y$之间的差距：  
$$δ^N_{jk}=(y_j-\hat{y}_j)y_j(1-y_j)$$
（$\hat{y}_j$和$y_j$都是向量，其维度等于数据集大小）  
对于隐藏层的神经元，如下式子表示了其神经网络单元输出的修正：  
$$δ^i_{jk}=a^i_j(1-a^i_j)∑_{m}δ^{i+1}_{mk}Wδ^{i+1}_{mj}$$
简单来说即$a^i_j(1-a^i_j)$后一层中与该神经元相连的神经元的权重和修正的乘积。  
权值的修正过程表示为：  
$$Δw_{jk}=ηδ_{jk}a^i_j$$
$$w_{jk}:=w_{jk}+Δw_{jk}$$
其中，$η$是一个可以调整的参数，称为学习率(learning rate)。通过学习率可以控制权值一次性更新的幅度，换言之，即学习的快慢。学习率越大，权重更新的幅度越大，学习速度越快。如今的很多机器学习平台(比如MATLAB)都支持自适应的学习率调整，无需进行人为设置。  
上面所示的这个权重修正的方法称为梯度下降算法(gradient desent)。这个过程只是一个简化版本，具体的反向传播的过程参考：[机器学习-吴恩达 5.5. 神经网络的代价函数·反向传播](https://l61012345.top/2021/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%90%B4%E6%81%A9%E8%BE%BE/5.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5.5.%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/)


如此，神经网络在反复的前向传播和反向传播迭代（每一次迭代称为一轮，epoch）中不断地修正各连接的权重，直到使得真实值$y$与预测值$\hat{y}$之间的差距小到可以接受或者一直不变。这种情况称算法运行达到了收敛(convergence)。
通常，真实值$y$与预测值$\hat{y}$之间的差距是通过均方差(MSE,Mean Square Error)进行衡量的:  
$$MSE=\frac{1}{n}∑_{i=0}^{n-1}(\hat{y}_i-y_i)^2$$

整个神经网络的学习过程包括：  
- 随机初始化连接权重  
- 当均方差非常大，或者数代均方差变化很大时，执行如下的循环： 
  - 前向传播：带入每一个数据$(x_i,y_i)$中的$x_i$到神经网络中，计算神经网络中每一个神经元对每一个数据的输出$a_{jk}$和神经网络对每一个数据的预测值$\hat{y}$，并整理为向量。  
  - 计算真实值$y$与预测值$\hat{y}$之间的差距。  
  - 使用验证集对神经网络的准确率进行测试。
  - 反向传播：计算$δ$并更新每一层每一条连接的权重。  
- 使用测试集对训练好的神经网络的准确率进行测试。  

## 实验：神经网络拓扑结构的设置
根据如上的介绍，可以知道在设计神经网络时，可以调整的参数包括：  
- 神经网络的层数
- 隐藏层神经元的个数
- 神经元使用的激活函数的类型  
- 学习率

其他一些可以调整的参数诸如是否进行归一化/正则化(regulation)，以及使用哪些数据特征等等在此不做讨论。  
借助谷歌面向神经网络初学者的tensorflow playground平台：http://playground.tensorflow.org/ 可以发现这些设置对于设置不同数据分布类型的神经网络的影响。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915184919.png width=50%>  

### 学习率对训练结果的影响
下表展示了在使用基础输入特征、2个隐藏层、其中第一个隐藏层含有4个神经元，第二个隐藏层含有4个神经元、激活函数为Tanh、无正规化设置时不同学习率对神经网络训练聚类结果的影响。  

| 学习率 | 聚类结果 | 收敛轮数 | 收敛过程 |
|:-:|:-:|:-:|:-:|
|0.01|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915190652.png width=20%>|303|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915185806.png width=30%>|
|0.1|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915190048.png width=20%>|42|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915190132.png width=30%>|
|1|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915190349.png width=20%>|124|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915190428.png width=30%>|

如果增加实验的次数，可以更加直观地发现：**学习率设置如果过低，达到收敛的速度将会越慢。而学习率如果设置的过高，则会导致训练过程的loss（或者MSE）不断振荡，出现不稳定的情况，并且最终的效果更加粗糙**。  

### 隐藏层数量对训练结果的影响
下表展示了在使用基础输入特征、学习率为0.1、激活函数为Tanh、无正规化设置时不同学习率对神经网络训练聚类结果的影响。  

| 隐藏层数<br>每一层隐藏层的神经元数量为4个 | 聚类结果 | 收敛轮数 | 收敛过程 |
|:-:|:-:|:-:|:-:|
|1|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915192844.png width=20%>|391|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915193306.png width=30%>|
|2|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915190048.png width=20%>|42|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915190132.png width=30%>|
|4|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915192049.png width=20%>|243|<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915192250.png width=30%>|