---
title: 01.智能系统介绍
category_bar: true
date: 2022/09/14
categories: 
- 学习笔记
- 智能系统和数字图像处理应用
---
# 智能系统介绍
## 智能系统概要
在了解智能系统之前，首先先要明确系统智能(intelligence)的定义：通常，如果一个系统是智能的，这意味着这个系统具有如下特性：  
- 推理(reasoning)：该系统可以针对特定的问题给出解决方法，或者答案。  
- 学习(learning)：该系统可以对输入的信息进行建模并存储。  
- 自适应(adaptivity)：该系统可以根据不同的目标问题自动调整模型的参数。目前只有极少数的系统可以完成这一点。  

机器智能(machine intelligence)是指计算机可以以一种类似于人的方式处理和解决问题。智能系统(intelligent system)是一种可以实现机器级别的智能的系统。它一定具有推理和学习功能，但不一定可以实现自适应。  

### 计算智能
计算智能(computational intelligence)是一种基于软件的智能系统，因此也被称为软计算(softcomputing)。计算智能具有如下特性：  
- 可以容忍不准确、不完整、具有某些干扰的数据。  
- 解决问题时使用的步骤通常是隐形(不为外部所知)的。  
- 通过反复观察和适应来学习解决方法。  
- 可以处理用模糊语言表达的信息。  

人工智能就是这样一种计算智能系统，发明人工智能的目的是为了使得机器可以像人一样进行推理。这些系统通常是基于对知识建立模型来实现的。这样的知识模型中包括了：基本的规则、数据之间的关系、数学等式等等，从外部观察可以认为含有这种模型的系统是一个黑箱(black box)：箱子的一侧输入数据，通过箱子内的某种机制在箱子的另一侧得到期望的输出。  
本课程中介绍的计算智能系统包括了：  
- 神经网络(ANN, Artificial Neural Networks)  
- 模糊系统(Fuzzy Systems)  
- 遗传算法(GA,Genetic Algorithm)

## 机器学习
Tom Mitchel于1998年定义了机器学习：机器学习通过完成任务（T）得到经验（E），进而提升性能（P）。  
> A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.    

传统的学习方式，比如线性回归，所输入的数据是数学模型和数据，通过计算机计算得出输出结果。而机器学习则是通过学习数据及其对应的输出（甚至不需要学习输出），挖掘潜在的数学模型并输出。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220915105708.png width=50%>  

机器学习可以解决的四种类型的问题为：  
- 模式识别(pattern recognition)：根据特征识别特定的目标。例如对X光图像的乳腺肿瘤识别。  
- 模式生成(pattern generation)：根据所学习的数据特征生成新的数据点。例如使用生成对抗网络增强数据集。  
- 异常检测(anomalies detection)：根据所学习的数据特征识别异常的数据点。例如识别伪造的信用卡签名图像。  
- 预测(prediction)：根据所学习的数据趋势预测数据未来的走向。例如根据历年解放碑人流量数据预测今年新年解放碑的拥堵情况。  

### 机器学习的类型
机器学习包括了四种类型：  
- 监督学习(supervised learning)  
  机器学习数据及其对应的标签/输出，从而生成模型。  
  例如：线性回归、逻辑斯蒂回归、支持向量机以及核函数、大部分神经网络和深度学习等等。  
- 无监督学习(unsupervised learning)   
  机器根据输入的数据自动挖掘数据特征，以此建模，不需要输入数据对应的标签。  
  例如：无监督聚类、主成分分析等降维方法。  
- 半监督学习(semi-supervised learning)  
  监督学习与无监督学习相结合的一种学习方法。半监督学习使用大量的未标记数据，以及同时使用标记数据，来进行模式识别工作。  
  例如：自训练算法、基于图的半监督算法、半监督支持向量机等等。  
- 增强学习(reinforcement learning)  
  一种反馈学习系统。由智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）组成。智能体执行了某个动作后，环境将会转换到一个新的状态，对于该新的状态环境会给出奖励信号（正奖励或者负奖励）。随后，智能体根据新的状态和环境反馈的奖励，按照一定的策略执行新的动作。
  例如：马尔科夫决策过程、Q学习等等。  

### 机器学习的发展历史
- 1940年代  
  1943年-Warren McCulloch和Wallter Pitts发表论文《神经活动中内在思想的逻辑演算》，提出了神经网络层次结构模型。  
- 1950年代  
  1950年-图灵提出了“图灵测试”理论。  
  1957年-康奈尔大学教授Frank Rosenblatt提出了感知机(Perceptron)的概念，并且首次用算法精确定义了自组织自学习的神经网络数学模型，设计出了第一个计算机神经网络。  
  1958年-Cox为逻辑斯蒂回归法正式命名，用于解决美国人口普查任务。  
  1959年-IBM公司的A.M.Samuel设计了一个具有学习能力的跳棋程序，曾经战胜了美国保持8年不败的冠军。Samuel将机器学习定义为无需明确编程即可为计算机提供能力的研究领域。  
- 1960年代  
  1960年-Stratonovich提出了隐式马尔科夫模型。  
  1962年-Hubel和Wiesel发现了猫脑皮层中独特的神经网络结构可以有效降低学习的复杂性，从而提出Hubel-Wiese生物视觉模型。  
  1963年-Ward提出了层次聚类算法。  
  1965年-斯坦福大学Feigenbaum开发了第一个专家系统Dendral，这个系统可以根据化学仪器的读数自动鉴定化学成分。  
  1967年-James MacQueen提出了K均值算法。  
  1969年-Marvin Minsky和Seymour Papert出版了著作《Perceptron》。其中提出了感知机的局限性（异或问题）。神经网络进入第一个寒冬期。  

- 1970年代  
  1970年-Linnainmaa首次完整地叙述了反向模式自动微积分算法（反向传播算法 BP 的雏形），但在当时并没有引起重视。  
  1972年-Edward H. Shortliffe研制的用于诊断和治疗感染性疾病的医疗专家系统MYCIN，第一次使用了知识库的概念，并使用了似然推理技术。  
  1975年-密歇根大学John holland发表论文《Adaptation in Natural and Artificial Systems》，提出了遗传算法(GA)。  

- 1980年代  
  1980年-卡内基梅隆大学举行了第一届机器学习国际研讨会，标志着机器学习研究在世界范围内兴起。  
  1980年-卡内基梅隆大学研发的专家系统XCON正式投入使用。  
  1981年-Werbos提出多层感知机，解决了神经网络线性模型无法解决的异或问题。  
  1983年-J. J. Hopfield利用神经网络求解“流动推销员问题”NP!Hard问题。  
  1985年-Judea Pearl提出了贝叶斯网络。  
  1986年-Rumelhart，Hinton和Williams提出了反向传播算法（BP），并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。  
  1989年-贝尔实验室Yann和LeCun教授提出了卷积神经网络（CNN）计算模型。  
- 1990年代  
  1995年-Corinna Cortes和Vapnik提出支持向量机算法。  
  1995年-Hochreiter的工作证明了神经网络的一个严重缺陷-梯度爆炸和梯度消失问题。神经网络进入第二个寒冬期。  
  1997年-Freund和Schapire提出了Adaboost方法。  
- 2000年代  
  2001年-Breiman发表随机森林方法（Random forest），Adaboost在对过拟合问题和奇异数据容忍上存在缺陷，而随机森林在这两个问题上更加鲁棒。  
  2006年-Geoffrey Hinton和Ruslan Salakhutdinov提出了深度学习模型。该文章提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。  
  2009年-Geoffrey Hinton提出了深度信念网络(DBN)。  
- 2010年后  
  2011年-多伦多大学Vinod Nair与Geoffrey Hinton在Kaggle全美数据科学大赛（NDSB）中首先提出并使用ReLU激活函数，该激活函数能够有效的抑制梯度消失问题。  
  2012年-Alex Krizhevsky提出了AlexNet网络。  
  2012年-Geoffrey Hinton研究团队采用深度学习模型AlexNet并第一次使用GPU加速模型计算，赢得了ImageNet比赛冠军。  
  2014年-Ian J Goodfellow提出了生成对抗网络。  
  2016年-由谷歌旗下DeepMind公司开发的基于深度学习的围棋模型AlphaGo与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜。 
  2017年-谷歌的机器翻译团队在NIPS上发表论文：《Attention is all you need》，开创性地提出了在序列转录领域，完全抛弃CNN和RNN，只依赖attention-注意力结构的简单的网络架构，名为Transformer。  
  2020年-OpenAl研究人员发表的论文《Language Models are few Shot Learners》，该论文介绍了GPT-3系列模型。  

  