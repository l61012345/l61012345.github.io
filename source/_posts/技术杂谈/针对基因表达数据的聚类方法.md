---
title: 针对基因表达数据的聚类方法
categories: 技术杂谈
tags:
- 复杂系统分析
excerpt: 通过cDNA技术检测出来的基因数据称为基因表达数据(gene expression data)。基因表达数据反映的是直接或间接测量得到的基因转录产物mRNA在细胞中的丰度，这些数据可以用于分析哪些基因的表达发生了改变，基因之间有何相关性，在不同条件下基因的活动是如何受影响的。它们在医学临床诊断、药物疗效判断、揭示疾病发生机制等方面有重要的应用。随着 cDNA 微阵列和寡核苷酸芯片等高通量检测技术的发展，生物学家可以从全基因组水平定量或定性检测基因转录产物 mRNA。由于生物体中的细胞种类繁多，同时基因表达具有时空特异性，因此，基因表达数据与基因组数据相比，要更为复杂，数据量更大，数据的增长速度更快。
---
# 针对基因表达数据的聚类方法

> *Cluster Analysis for Gene Expression Data: A Survey.* Daxin Jiang et al, 2004.  
> DNA微阵列（基因芯片）简介:https://zhuanlan.zhihu.com/p/46017763  
> 基因表达数据：https://baike.baidu.com/item/%E5%9F%BA%E5%9B%A0%E8%A1%A8%E8%BE%BE%E6%95%B0%E6%8D%AE/347523  
> SOM(自组织映射神经网络)——理论篇：https://zhuanlan.zhihu.com/p/73534694

## 背景
### DNA微阵列技术
随着人类基因组（测序）计划（Human genome project）的逐步实施以及分子生物学相关学科的迅猛发展，越来越多的动植物、微生物基因组序列得以测定，基因序列数据正在以前所未有的速度迅速增长，传统的基因检测方式效率低下，为此，建立新型杂交和测序方法以对大量的遗传信息进行高效、快速的检测、分析显得格外重要。   
cDNA微阵列技术/基因芯片技术（cDNA Microarrays/gene chip）是解决这一问题的重要技术：通过微加工技术将数以万计、乃至百万计的特定序列的DNA片段（基因探针），有规律地排列固定于2平方厘米左右的硅片、玻片等支持物上，构成的一个二维DNA探针阵列，其结构与计算机的电子芯片十分相似，所以被称为基因芯片。基因芯片主要用于基因检测工作。  

### 基因表达数据
通过cDNA技术检测出来的基因数据称为基因表达数据(gene expression data)。基因表达数据反映的是直接或间接测量得到的基因转录产物mRNA在细胞中的丰度，这些数据可以用于分析哪些基因的表达发生了改变，基因之间有何相关性，在不同条件下基因的活动是如何受影响的。它们在医学临床诊断、药物疗效判断、揭示疾病发生机制等方面有重要的应用。随着 cDNA 微阵列和寡核苷酸芯片等高通量检测技术的发展，生物学家可以从全基因组水平定量或定性检测基因转录产物 mRNA。由于生物体中的细胞种类繁多，同时基因表达具有**时空特异性**，因此，基因表达数据与基因组数据相比，要更为复杂，数据量更大，数据的增长速度更快。  
在本论文中，基因表达数据以矩阵的形式出现：矩阵的每一行$\boldsymbol{g_i}$表示每一个基因，矩阵的每一列$\boldsymbol{s_j}$表示一次在某一特定条件下对基因的表达实验，称为一次采样（sample）。矩阵的每一个元素$w_{i,j}$表示基因$g_i$在实验$s_j$下的表达水平(expression level)，表达水平是一个实数值。基因表达数据的组织形式如下图所示：  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220811144554.png width=60%>  


#### 数据特征
- 矩阵的维度是及其不对等的，通常在微矩阵实验中的基因数量级在$10^3$到$10^4$左右，甚至可以达到$10^6$，而参与的采样次数通常小于100，矩阵的长宽数量级差别非常大。  
- 有些基因可能参与的共调控过程不止一个，因此聚类结果中的簇之间可能相互重叠。  
- 通常，对于某个表现型，其参与该过程的基因数量占总参与实验的基因个数比例非常小，因此对于该表现型，无关基因（噪声）的数量非常大，信噪比非常低。只有一小部分基因参与任何感兴趣的细胞过程，而一个细胞过程只发生在一部分样本中。  

#### 数据处理
在实际的实验中，原始的矩阵中往往包含大量的噪声和缺失数据，因此需要对原始矩阵进行数据处理，常见的数据处理方式包括数据归一化和对缺失数据的预测。经过这些数据处理过程后才能探究基因表达相关的性质，本论文中假设这个原始矩阵已经被正确地数据处理过。  

### 聚类方法
聚类是对基因表达进行分析的重要手段。聚类可以发现具有相同表达模式(expression pattern)的基因，称为共表达基因(coexpress gene)。从生物学的角度来说，共表达基因极有可能参与同一细胞过程，表示了基因的共调控机制(coregulation)，并且为新的关于转录调控网络机制的假说提供参考依据。此外，聚类也可能发现新的一簇共表达基因类型。  
机器学习中的聚类方法可以大致分为两类：监督的(supervised)和非监督的(unsupervised)，二者的区别在于前者依赖于训练和提前指定好的类别，而后者仅根据数据自身的特征、不依赖于任何预先指定的数据类别进行分类。  
在对基因表达数据的研究中，无论是对基因进行聚类还是采样进行聚类都是有意义的：对基因进行聚类可以找出共表达基因簇；对采样进行聚类，每个样本簇可能对应于一些特定的宏观表现型（例如在多个不同的采样中，实验条件中都有同一种癌症），比如癌症类型或临床综合征。  
因此，本论文中按照对基因还是对样本分类将各种常见的聚类方法分类为三种：  
- 基于基因的聚类(gene-based clustering)：在这样的聚类中，基因被视为个体，采样被视为基因的特征。  
- 基于采样的聚类(sample-based clustering)：在这样的聚类中，采样被视为个体，基因被视为采样的特征。  

有一些算法可以同时运用于这两种聚类。  
- 子空间聚类(subspace clustering)：在这样的聚类中，基因和采样被同等地视为个体和特征。  

## 相似度度量
相似度(proximity)表示了两个数据的相似程度，在采样空间中表示为两个数据点的空间距离：两个数据点距离越近，两个数据点越相似。在基因表达数据中相似度度量有两种：欧氏距离(Eculidean distance)和皮尔森相关系数(Pearson correlation coeffitient)。  

### 欧氏距离
欧氏距离是最常见的一种距离度量，其为高维空间中两点$O_i=(o_{i1},...,o_{ip}),O_j=(o_{j1},...,o_{jp})$之间的距离公式：  
$$Eculidean(O_i,O_j)=\sqrt{\sum_{d=1}^p(o_{id}-o_{jd})^2}$$
如果某些维度上的数值数量级可能过大也可能过小，因此有必要在各维度上进行归一化处理。此外，经验证明欧氏距离对标准化$O'_{id}=\frac{O_{id}-μ_{O_i}}{σ_{O_i}}$后的基因表达数据的相似度度量要比非标准化的数据好得多。  

### 皮尔森相关系数
另一种方法是皮尔森系数，皮尔森系数通过数据的统计特征来衡量两个数据之间的相似度，高维空间中两点$O_i=(o_{i1},...,o_{ip}),O_j=(o_{j1},...,o_{jp})$的皮尔森相关系数表示为：  
$$Pearson(O_i,O_j)=ρ_{ij}=\frac{\sum_{d=1}^p(o_{id}-μ_{o_i})(o_{jd}-μ_{o_j})}{\sqrt{\sum_{d=1}^p(o_{id}-μ_{o_i})^2}\sqrt{\sum_{d=1}^p(o_{jd}-μ_{o_j})^2}}$$
其中$μ_{o_i}$和$μ_{o_j}$分别为$O_i$和$O_j$的均值。  

由于皮尔森相关系数只根据数据的统计特性（均值和均方值）来衡量相似度，因此极有可能出现两个数据分布完全不同，但是在近似的地方出现最值被皮尔森相关系数判定为相似的情况。  
改进这个问题的方法是刀切法(Jackknife)，刀切法的相关系数表示为：  
$$Jackknife(O_i,O_j)=min\{ρ_{ij}^{(1)},ρ_{ij}^{(2)},...,ρ_{ij}^{(l)}\}$$
其中$ρ_{ij}^{(l)}$指$O_i,O_j$都去掉第$l$个特征后的皮尔森相关系数。  
从定义式中不难发现刀切法相比于皮尔森相关系数更为客观，其考虑到了每一个特征对总体分布的贡献。但是当特征数较大时，需要计算(特征数)次皮尔森相关系数，计算量较大。  
此外，皮尔森相关系数假设了所有的数据服从高斯分布，对于实际上不符合高斯分布的数据，皮尔森公式的效果比较差。  
改进这个问题的方法是 斯皮尔曼等级相关系数(Spearsman's rank order correlation efficient)。它被定义成等级变量之间的皮尔森相关系数:对于样本容量为$n$的样本，$n$个原始数据被转换成等级数据，相关系数$ρ$为原始数据依据其在总体数据中平均的降序位置。斯皮尔曼等级相关系数虽然避免了皮尔森相关系数中对于数据分布的假设，但是使用等级来替代原有的数据使得大量的数据特征被丢失。  

### 标准化的数据
前文中提到，如果对基因表达数据进行标准化处理，那么欧氏距离的度量效果会好得多。数据的标准化定义为：  
$$O'_{id}=\frac{O_{id}-μ_{O_i}}{σ_{O_i}}$$
通过数学公式可以证明如下式子成立：  
$$Pearson(O_i,O_j)=Pearson(O'_i,O'_j)$$
$$Euclidean(O'_i,O'_j)=\sqrt{2p}(\sqrt{1-Pearson(O'_i,O'_j)})$$
上述式子可以说明，在标准化的前提下，皮尔森相关系数和欧氏距离的效果是相同的。  

## 基于基因的聚类
### K均值算法
K均值算法(K-means)是一种最基本的基于划分的(partition-based)聚类方法。在运行K-means之前需要设置类别数$K$，K-means会最终将数据分为$K$类。  
对于如图所示的数据集，使用K均值算法将其分成两类数据。
K均值算法的第一步是在数据集中随机生成两点，称为聚类中心/质心（Cluster Centroid）。（要分为多少类，就要生成多少个聚类中心）
K均值算法是一个迭代算法，每一次迭代过程分为两部分：
- 簇分配  
  K均值算法会遍历每一个数据，计算该点与每一个聚类中心的距离，该点会被归属到最近的质心。  
- 移动聚类中心  
  移动聚类中心到当前集群的平均位置  
之后K均值算法会重复上述两步，直到聚类中心不再移动，此时可以认为聚类已经实现。  
K均值算法的优化函数为：  
$$E=∑_{i=1}^K∑_{O∈C_i}|O-μ_i|^2$$
其中$O$为$C_i$簇中的一个数据点，$μ_i$为$C_i$簇的质心。  
因此K均值算法尝试最小化距离的各数据点到簇的质心的距离的平方和。  

#### 评价
K均值算法快而简单，往往经过很少的迭代就能够达到收敛。  
K均值算法的缺点是，对于基因表达数据，往往$K$是不能通过先验知识可以确定的，因此需要使用不同的$K$多次执行K均值算法以找到最合适的$K$。这种做法对数据量大和潜在类别数多的数据集来说往往是不切实际的。  
再者，K均值算法强制为每一个基因都会找到一个簇，即使这个基因可能并不适合每一个簇。对于信噪比非常低的基因表达数据，这些噪声也会被聚类到某些簇中，因此K均值算法对噪声非常敏感。  
某些改进后的K均值算法通过设置某些全局参数来对K均值算法的运行过程进行限制，但是这些全局参数的设置仍然需要多次尝试以找到合适的设置。  

### 自组织映射神经网络
自组织映射神经网络(SOM,Self-organizing)是一种只有两层的无监督的人工神经网络。通过学习输入空间中的数据，生成一个低维、离散的映射(Map)，从某种程度上也可看成一种降维算法。不同于一般神经网络基于损失函数的反向传播来训练，它运用竞争学习(competitive learning)策略，依靠神经元之间互相竞争逐步优化网络。且使用近邻关系函数(neighborhood function)来维持输入空间的拓扑结构。  
SOM的结构如下：  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220812094845.png width=50%>  

SOM只有两层，第一层为输入层，第二层为输出层，也称为竞争层(computational layer)。  
输入层神经元的数量是由输入向量的维度决定的，一个神经元对应一个特征。  
神经网络的每个神经元都与一个参考向量相关，每个数据点都被映射到具有 "最接近 "参考向量的神经元上。在运行该算法的过程中，每个数据对象作为一个训练样本，引导参考向量向输入向量空间的密集区域移动，从而使这些参考向量被训练成适合输入数据集的分布。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/v2-f52b70ceb4be67a91ebd807236a27fbf_b.gif width=50%>  

训练完成后，所有的数据点都被映射到竞争层的神经元，在竞争层观察到竞争层的神经元移动到输入层数据密集的区域。  

#### 评价
自组织映射神经网络最突出的特点是在输出层可以非常直观地观察到数据分布的特性；并且相比于K均值算法的抗噪性更好。但是自组织神经网络需要设定类别和初始竞争层的结构（如下图所示），这些参数的设置是一门学问。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220812101143.png width=50%>  
如果数据集中有大量的与目标无关紧要的数据，在算法运行结束后，每一个簇中充斥着大量的这样的无关紧要的数据。   

### 分层聚类
K均值算法和自组织映射神经网络都是基于划分的聚类方法，另一种聚类方法是分层聚类(hierarchical clustering)。与将数据直接划分为类别不同的是，分层聚类的聚类结果是相互交织叠加的类别，这些类别具有层级之分，层级可以通过聚类树状图(bendgram)进行表示，如下图所示：
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220814124704.png width=50%>  

聚类树状图表示了类别之间的从属关系，可以允许根据需要的聚类层级来让用户自由选择聚类的程度。  
分层聚类可以分为自上而下(divisive)和自下而上(agglomerative)两种算法类别。  
自下而上的分层聚类最初将每一个数据点视为单独的一类，接下来的每一次迭代都根据度量将两个距离差最小的数据点为一类，最终所有数据全部聚为一类时，算法收敛。自下而上的分层聚类算法的度量通常是相似度(proximity)。常见的自下而上的分层聚类比如单链接聚类(single link)、完全链接聚类(complete link)、最小方差聚类等算法(minimum-variance)。  
自上而下的分层最初将整个数据集中的所有数据视为一类，然后使用启发式算法或者基本图论对类别逐一划分，最终每一个数据点都为单独的一类时，算法收敛。  

<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/v2-799328533a95593af257a7a1649f521e_720w.gif width=50%>   


分层聚类的优点是为数据提供了一种直观的表达形式，而且允许算法的使用者根据结果自行决定聚类的深度。分层聚类的缺点是鲁棒性不高：对数据集的些许扰动最终可能会导致整个聚类树状图出现较大的变动。其次，算法的计算复杂度比较高。最后，由于分层聚类的贪心特性，每一次迭代的结果都基于前一次迭代，如果某一次迭代的聚类效果并不好，那么将会影响到之后的所有迭代。  


### 图论聚类
对于一个数据集$X$，可以建立起一个相似度矩阵$P$，使得矩阵$P$中的每一个元素为对应的两个数据点的相似度：$P[i,j]=proximity(O_i,O_j)$. 在图论聚类(graph thoertical clustering)中，根据相似度矩阵建立一个相似度图(proximity graph)$𝒢$，这个图中的每个定点代表一个数据点。两个数据点的连线上根据这两个数据点的相似度被赋予一定权重。如此就将一个聚类问题转化为了图论中的最小割问题(minimum cut)或者最大团(maximum cliques)问题。  

{% note info %}  
最小割问题：  
在图论中，去掉其中所有边能使一张网络流图不再连通（即分成两个子图）的边集称为图的割，一张图上最小的割称为最小割。图中所有的割中，边权值和最小的割为最小割。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220814133519.png width=30%>  
可以理解，在相似度图$𝒢$中的最小割代表使得相似度最小的几个连线被断开，形成两个新的簇。  
{% endnote %}

{% note info %}  
最大团问题：  
对于一个无向图$G=(V,E)$，$V$是点集，$E$是边集。取$V$的一个子集$U$，若对于$U$中任意两个点$u$和$v$，有边$(u,v)∈E$，那么称$U$是$G$的一个完全子图。如果该完全子图$U$不包含在更大的完全子图中，那么称$U$是一个团。$G$的最大团指的是顶点数最多的一个团。  
<img src = https://cdn.jsdelivr.net/gh/l61012345/Pic/img/20220814132739.png width=20%>  
{% endnote %}

本论文中提到的图论聚类的两种算法为CLICK和CAST。  

#### CLICK算法
CLICK算法(CLuster Identification via Connectivity Kernels)试图将相似度图中具有较大权值连接的顶点聚为一簇。CLICK算法基于一个假设：标准化后的权值服从正态分布。CLICK算法中相似图的某条连接的权重$w_{ij}$可以理解为顶点$i$和顶点$j$被分到同一个簇中的概率。CLICK的迭代过程需要不断地找到整个相似度图$𝒢$以及其最小割后的子图的最小割。  
之后，CLICK算法有额外的两步对聚类结果进行修饰(refine)：  
- “采用”步骤(adoption step)用于处理迭代过程之后剩下的单个顶点(singletions)，并更新当前的簇。  
- “合并”步骤(merge step)用于合并两个相似度超过某个提前设定好的阈值的簇。  
  
CLICK算法的同质性(homogeneity，同一个簇中的个体相似度高)和异质性/分离性(heterogeneity/seperation，不同簇中的个体相似度低，差异大)都比较好。  
但是CLICK几乎不能保证迭代过程不走错路，最终导致生成高度不平衡的划分(highly-unbalanced partitions)：比如一个将数据集分为数据点数量差异非常悬殊的划分。此外，如果最终的簇与簇之间可能存在交叠的话，CLICK算法也不能处理好这样的数据。对于基因表达数据而言，可以通过经验判断其最终簇和簇之间可能会有交叠，因此CLICK算法可能无法正确处理这样的基因表达数据。  

#### CAST算法
CAST算法认为对数据集正确的簇分类可以表示为一个团图(clique graph)$ℋ$，这个团图中的每一个团都对应了一个簇。团图的相似度图可以根据以概率$α$翻转每条连接得到。对于数据的聚类可以认为是通过对相似度图进行一定的翻转，最终得到一个确定的团图。  
CAST算法用亲和度(affinity)来衡量每一个数据点对某个簇的从属关系，并通过一个设定的亲和度阈值(affinity threshold)$t$来决定某个数据点是否属于这个簇。在算法的迭代过程中，CAST会不断删除这个簇中亲和度小的数据点，并添加亲和度高的数据点。亲和度阈值实际上是当前这个簇中所有数据点配对的平均相似度，因此无需设置。  
CAST算法的优点是不需要用户提前对簇的数量等参数进行设置。但是对于大量数据，寻找亲和度阈值过程的计算量和复杂度都比较高。  

### 基于模型的聚类
基于模型的聚类方法 (Model-based Clustering Methods)的基本其基本思想是:整个数据集的分布一定由有限个已知的概率分布模型叠加而成；其中每一个簇对应了一个已知的概率分布模型。那么整个整个数据集的概率模型可以表示为:  
$$L_{mix}(Θ,Γ)=∑_{i=1}^kγ_r^if_i(x_r|θ_i)$$
其中，$n$是数据点的数量；$k$是簇（即概率模型）的数量；$x_r$是一个数据点；$γ_r^i$表示$x_r$属于簇$C_i$的概率，称为隐含参数；$f_i(x_r|θ_i)$是引入$x_r$后簇$C_i$概率密度函数，其中包含一些未知参数$θ_i$，称为模型参数。  

基于模型的聚类方法需要找到使得$L_{mix}(Θ,Γ)$最大的所有$Θ=\{θ_i|1≤i≤k\}$和$Γ=\{γ^i_r|1≤i≤k,1≤r≤n\}$。  
通常，可以通过EM算法/期望最大化算法(Expectation-Maximum)找到合适的$Γ$和$Θ$。EM算法的运行过程分为两步：期望和最大化。  
- 首先对每一个簇的概率模型中参数$Θ$（比如均值和方差）进行的随机初始化。
- 期望过程(E过程)：计算每个数据点$x_r$更属于哪一个簇的概率分布，计算每个数据点对最佳簇的$γ_r^i$，得到$Γ$。  
- 最大化过程(M过程)：利用现在每个簇中的数据点分别对每个簇的概率密度参数$θ_i$进行极大似然估计，得到$Θ$.  

然后重新计算每个数据点$x_r$更属于哪一个簇的概率分布，如此重复E过程和M过程，直到$Γ$和$Θ$不再发生变化。  

基因表达数据可能会出现单个基因与两个不同的簇有高度相关性的情况。基于模型的聚类的概率性质适合处理这种情况。但是基于模型的聚类假设了每一个簇一定服从某个特定的概率分布模型，事实上这一假设未必符合实际。  

### 基于密度的分层聚类/DHC
基于密度的分层聚类又称为DHC算法(Density-based Hierarchical Clustering)，其基本思想是将一个簇视为一个多维的稠密区域，这个稠密区域中的数据点之间相互“吸引”(attract)，区域核心部分的数据点非常接近，因此具有高密度。  


### 基于基因聚类的困难性