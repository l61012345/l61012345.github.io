---
title: 8.4. 使用支持向量机
date: 2021/05/11
categories: 
- 机器学习基础课程——吴恩达
- 08. 支持向量机
---
# 使用支持向量机
本节将考虑在实际中应用SVM算法的一些问题。  

## 调用函数库实现
求解$θ$的过程很繁琐，因此在实际中通常采用调用现有函数库（比如liblinear,libsvm）的方式实现SVM，但仍然需要给这些函数补充参数：  
1. 选择参数C。
2. 选择内核参数。
如果不同特征之间的取值差异非常大，需要对特征变量做归一化。  

## 其他的核函数
目前学到的两种核函数：  
线性内核：即不使用内核参数，比如： $y=1,if θ^Tx≥0$。通常在有大量的特征，但是只有少量的训练样本的情况下，为了避免过拟合而采用线性拟合的方式。  
高斯核： 通常对大样本且决策边界不规则的情况适用。  

有些函数库需要用户自己写一个核函数。因此事实上除了这两种核函数之外，用户可以创建自己的核函数。但是只有满足默塞尔定理的函数才能被作为核函数。  
其他的核函数：
1. 多项式核函数：$k(x,l)=(x^Tl+b)^n$,b和n都是实数参数。    
2. 字符串核函数
3. 直方相交核函数
4. 卡方核函数

## 多类别分类
在$k$分类下，需要构建$k$个SVM函数，每一个函数需要将一个类别从其他的类别中区分开来。  
在大部分函数库中，多分类函数已经被预置，因此只需要调用即可。  

## 逻辑回归与SVM
已经知道SVM其实是对于逻辑回归的修改，那么在何种情况下使用它们？  
设$n$为特征数，$m$为训练样本数：  
**如果$n$远大于$m$（比如文本分类），使用逻辑回归或者是线性核的SVM。**   
**如果$n$的值相对于$m$较小且$m$的值比较适中，使用高斯核的SVM。**  
**如果$n$远小于$m$,应当首先手动增加一些特征，再使用逻辑回归或者是线性核的SVM。**  
> 线性核的SVM的本质就是线性的逻辑回归，因此两者的预测结果会非常相似，但是在具体的环境中两者的性能会有差异。  

上述的情况都可以使用神经网络进行训练，但是训练的速度可能会非常地慢。  
