---
title: 4.3. 线性回归的正则化
date: 2021/08/21
categories: 
- 机器学习基础课程——吴恩达
- 04. 正则化
---
# 线性回归的正则化
## 正则化的梯度下降算法
在线性回归中，我们使用修改后的梯度下降算法：  
Repeat {   
$$θ_0:=θ_0-\alpha\frac{1}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(j)} \tag{1}$$
> $θ_0$  不需要正则化  

$$θ_j:=θ_j-\alpha[\frac{1}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}θ_j] \tag{2}$$
$$j=1,2,3,...,n$$
}  
事实上： $\frac{1}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(j)}+\frac{λ}{m}θ_j=\frac{∂J(θ)}{∂θ_j}$  
$\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}=\frac{∂J(θ)}{∂θ_0}$  

如果将（2）中的$\theta_j$统一，那么就可以得到（3）：  
$$θ_j:=θ_j（1-α\frac{λ}{m}）-\frac{α}{m}\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)} \tag{3}$$

由于$1-α\frac{λ}{m}<1$,且只比1小一点点，也就是说，梯度下降算法每次更新的时候$θ_j$在一开始都会比原来小一点点，再进行原来的梯度下降更新  
  
## 正规方程
在之前的讲义中，探讨过设计两个矩阵：  
$X=\begin{bmatrix} (x^{(1)})^T \\ ...\\ (x^{(m)})^T \end{bmatrix}$ 代表有m个数据的数据集 和 $y=\begin{bmatrix} y^{(1)} \\ ...\\ y^{(m)} \end{bmatrix}$ 代表训练集当中的所有的标签  
通过：
$$θ=(X^TX)^{-1}X^Ty$$
（相当于对$J(θ)$中的每一个θ求偏导数，并且使其等于0）  
可以求出最适合的θ  
现在改变在正规方程中加入一项：
$$θ=(X^TX+λ
\begin{bmatrix}
0 & 0 & 0 & ...&0 \\   0 & 1 & 0& ...&0 \\ 0 & 0 & 1& ...&0 \\ ... & ... & ...& ...&... \\ 0 & 0 & 0& ...&1
\end{bmatrix})^{-1}X^Ty$$
来达到同样的效果  
>$\begin{bmatrix}
0 & 0 & 0 & ...&0 \\   0 & 1 & 0& ...&0 \\ 0 & 0 & 1& ...&0 \\ ... & ... & ...& ...&... \\ 0 & 0 & 0& ...&1
\end{bmatrix}$是一个(n+1)的方阵  

如果矩阵X不可逆$（m<=n）$,那么$(X^TX)^{-1}$也同样不可逆,但是经过数学证明，无论如何$(X^TX+λ
\begin{bmatrix}
0 & 0 & 0 & ...&0 \\   0 & 1 & 0& ...&0 \\ 0 & 0 & 1& ...&0 \\ ... & ... & ...& ...&...\\ 0 & 0 & 0& ...&1
\end{bmatrix})^{-1}$ 都是可逆的。