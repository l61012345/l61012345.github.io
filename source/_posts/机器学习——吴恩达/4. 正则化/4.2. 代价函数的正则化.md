---
title: 4.2. 代价函数的正则化
date: 2021/08/21
categories: 
- 机器学习基础课程——吴恩达
- 04. 正则化
---
# 代价函数的正则化  
对于代价函数：
$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2$$
增加两个惩罚项$1000\theta^2_3$和$1000\theta^2_4$，代价函数变为：  
$$min_{θ} \frac{1}{2m} \Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+1000\theta^2_3+1000\theta^2_4$$
如果要最小化这个函数，那么$\theta_3$与$\theta_4$就要尽可能的接近0，那么最后拟合的结果（假设函数）：$\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4$，仍然是一个类似的二次函数.  
正则化的基本思想是**如果所有的参数足够小，那么假设模型就更简单。**  
>事实上，如果参数足够小，得到的函数就会越平滑，越简单，越不容易出现过拟合的问题  

在实际上，对于大量的特征和大量的参数，比如$x_1..x_{100}$和$\theta_0...\theta_{100}$，我们无法确定哪些参数是高阶项的参数，这个时候采用的方法就是对代价函数进行修改，使得所有的参数都尽可能的小。  
修改后的代价函数方程：  
$$ J_{\theta}=\frac{1}{2m}[\Sigma_{i=1}^{m}(h_θ(x^{(i)})-y^{(i)})^2+λ\Sigma_{j=1}^{m}\theta_j^2]$$
其中$λ\Sigma_{j=1}^{m}\theta_j^2$称为**正则化项**，它的目的是为了**缩小每一项的参数**。
>$\theta_0$是否正则化对结果影响不大  
λ的作用是对“+”号的前后（前：更好的拟合训练集，后：假设函数足够简单）两项进行取舍平衡，称为正则化系数  

如果λ被设置的太大，那么所有参数的惩罚力度被加大，这些参数最后的结构都将全部接近于0，那么最后的假设函数将会变成$h_\theta(x)=θ_0$,最终导致欠拟合。  